{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0. Import data & data clean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52800, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AC</th>\n",
       "      <th>PMID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Terms</th>\n",
       "      <th>Text_combined</th>\n",
       "      <th>batch_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P06169</td>\n",
       "      <td>2185016</td>\n",
       "      <td>Autoregulation may control the expression of y...</td>\n",
       "      <td>Recently we deleted the pyruvate decarboxylase...</td>\n",
       "      <td>autoregulation</td>\n",
       "      <td>Autoregulation may control the expression of y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P0AEM5</td>\n",
       "      <td>12704152</td>\n",
       "      <td>Complete genome sequence and comparative genom...</td>\n",
       "      <td>We determined the complete genome sequence of ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Complete genome sequence and comparative genom...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B8FZE0</td>\n",
       "      <td>22316246</td>\n",
       "      <td>Genome sequence of Desulfitobacterium hafniens...</td>\n",
       "      <td>The genome of the Gram-positive, metal-reducin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Genome sequence of Desulfitobacterium hafniens...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P14656</td>\n",
       "      <td>12060286</td>\n",
       "      <td>Overlapping expression of cytosolic glutamine ...</td>\n",
       "      <td>In order to estimate whether cytosolic glutami...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Overlapping expression of cytosolic glutamine ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q7XXS4</td>\n",
       "      <td>27052628</td>\n",
       "      <td>Both overexpression and suppression of an Oryz...</td>\n",
       "      <td>Tight and accurate regulation of immunity and ...</td>\n",
       "      <td>autoactivation</td>\n",
       "      <td>Both overexpression and suppression of an Oryz...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AC      PMID                                              Title  \\\n",
       "0  P06169   2185016  Autoregulation may control the expression of y...   \n",
       "1  P0AEM5  12704152  Complete genome sequence and comparative genom...   \n",
       "2  B8FZE0  22316246  Genome sequence of Desulfitobacterium hafniens...   \n",
       "3  P14656  12060286  Overlapping expression of cytosolic glutamine ...   \n",
       "4  Q7XXS4  27052628  Both overexpression and suppression of an Oryz...   \n",
       "\n",
       "                                            Abstract           Terms  \\\n",
       "0  Recently we deleted the pyruvate decarboxylase...  autoregulation   \n",
       "1  We determined the complete genome sequence of ...             NaN   \n",
       "2  The genome of the Gram-positive, metal-reducin...             NaN   \n",
       "3  In order to estimate whether cytosolic glutami...             NaN   \n",
       "4  Tight and accurate regulation of immunity and ...  autoactivation   \n",
       "\n",
       "                                       Text_combined  batch_number  \n",
       "0  Autoregulation may control the expression of y...             1  \n",
       "1  Complete genome sequence and comparative genom...             1  \n",
       "2  Genome sequence of Desulfitobacterium hafniens...             1  \n",
       "3  Overlapping expression of cytosolic glutamine ...             1  \n",
       "4  Both overexpression and suppression of an Oryz...             1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/processed/shuffled_10_data.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fiatlux/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# clean text\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = \" \".join([word.strip() for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "df['Text_Cleaned'] = df['Text_combined'].apply(clean_text)\n",
    "\n",
    "# fill nan with 'non-autoregulatory'\n",
    "df['Terms'] = df['Terms'].fillna('non-autoregulatory')\n",
    "\n",
    "# keep only selected columns\n",
    "columns_to_keep = ['batch_number','Text_Cleaned','Terms']\n",
    "df_cleaned = df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52800, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_number</th>\n",
       "      <th>Text_Cleaned</th>\n",
       "      <th>Terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>autoregulation may control expression yeast py...</td>\n",
       "      <td>autoregulation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>complete genome sequence comparative genomics ...</td>\n",
       "      <td>non-autoregulatory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>genome sequence desulfitobacterium hafniense d...</td>\n",
       "      <td>non-autoregulatory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>overlapping expression cytosolic glutamine syn...</td>\n",
       "      <td>non-autoregulatory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>overexpression suppression oryza sativa nblrrl...</td>\n",
       "      <td>autoactivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_number                                       Text_Cleaned  \\\n",
       "0             1  autoregulation may control expression yeast py...   \n",
       "1             1  complete genome sequence comparative genomics ...   \n",
       "2             1  genome sequence desulfitobacterium hafniense d...   \n",
       "3             1  overlapping expression cytosolic glutamine syn...   \n",
       "4             1  overexpression suppression oryza sativa nblrrl...   \n",
       "\n",
       "                Terms  \n",
       "0      autoregulation  \n",
       "1  non-autoregulatory  \n",
       "2  non-autoregulatory  \n",
       "3  non-autoregulatory  \n",
       "4      autoactivation  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_cleaned.shape)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1h/csb8qjzj1sv0jtrd6ccllm8c0000gn/T/ipykernel_6266/626586514.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['Terms_List'] = df_cleaned['Terms'].apply(\n",
      "/var/folders/1h/csb8qjzj1sv0jtrd6ccllm8c0000gn/T/ipykernel_6266/626586514.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['Terms_List'] = df_cleaned['Terms_List'].apply(lambda x: list(set(x)))\n"
     ]
    }
   ],
   "source": [
    "# convert terms to list\n",
    "df_cleaned['Terms_List'] = df_cleaned['Terms'].apply(\n",
    "    lambda x: [term.strip() for term in x.split(',')]\n",
    ")\n",
    "df_cleaned['Terms_List'] = df_cleaned['Terms_List'].apply(lambda x: list(set(x)))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(df_cleaned['Terms_List'])\n",
    "label_columns = mlb.classes_\n",
    "\n",
    "labels_df = pd.DataFrame(labels, columns=label_columns)\n",
    "existing_columns = [col for col in label_columns if col in df_cleaned.columns]\n",
    "df_cleaned = df_cleaned.drop(columns=existing_columns, errors='ignore')\n",
    "df_cleaned = pd.concat([df_cleaned, labels_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52800, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_number</th>\n",
       "      <th>Text_Cleaned</th>\n",
       "      <th>Terms</th>\n",
       "      <th>Terms_List</th>\n",
       "      <th>autoactivation</th>\n",
       "      <th>autocatalysis</th>\n",
       "      <th>autocatalytic</th>\n",
       "      <th>autofeedback</th>\n",
       "      <th>autoinducer</th>\n",
       "      <th>autoinduction</th>\n",
       "      <th>autoinhibition</th>\n",
       "      <th>autoinhibitory</th>\n",
       "      <th>autokinase</th>\n",
       "      <th>autolysis</th>\n",
       "      <th>autophosphorylation</th>\n",
       "      <th>autoregulation</th>\n",
       "      <th>autoregulatory</th>\n",
       "      <th>autoubiquitination</th>\n",
       "      <th>non-autoregulatory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>autoregulation may control expression yeast py...</td>\n",
       "      <td>autoregulation</td>\n",
       "      <td>[autoregulation]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>complete genome sequence comparative genomics ...</td>\n",
       "      <td>non-autoregulatory</td>\n",
       "      <td>[non-autoregulatory]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>genome sequence desulfitobacterium hafniense d...</td>\n",
       "      <td>non-autoregulatory</td>\n",
       "      <td>[non-autoregulatory]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>overlapping expression cytosolic glutamine syn...</td>\n",
       "      <td>non-autoregulatory</td>\n",
       "      <td>[non-autoregulatory]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>overexpression suppression oryza sativa nblrrl...</td>\n",
       "      <td>autoactivation</td>\n",
       "      <td>[autoactivation]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_number                                       Text_Cleaned  \\\n",
       "0             1  autoregulation may control expression yeast py...   \n",
       "1             1  complete genome sequence comparative genomics ...   \n",
       "2             1  genome sequence desulfitobacterium hafniense d...   \n",
       "3             1  overlapping expression cytosolic glutamine syn...   \n",
       "4             1  overexpression suppression oryza sativa nblrrl...   \n",
       "\n",
       "                Terms            Terms_List  autoactivation  autocatalysis  \\\n",
       "0      autoregulation      [autoregulation]               0              0   \n",
       "1  non-autoregulatory  [non-autoregulatory]               0              0   \n",
       "2  non-autoregulatory  [non-autoregulatory]               0              0   \n",
       "3  non-autoregulatory  [non-autoregulatory]               0              0   \n",
       "4      autoactivation      [autoactivation]               1              0   \n",
       "\n",
       "   autocatalytic  autofeedback  autoinducer  autoinduction  autoinhibition  \\\n",
       "0              0             0            0              0               0   \n",
       "1              0             0            0              0               0   \n",
       "2              0             0            0              0               0   \n",
       "3              0             0            0              0               0   \n",
       "4              0             0            0              0               0   \n",
       "\n",
       "   autoinhibitory  autokinase  autolysis  autophosphorylation  autoregulation  \\\n",
       "0               0           0          0                    0               1   \n",
       "1               0           0          0                    0               0   \n",
       "2               0           0          0                    0               0   \n",
       "3               0           0          0                    0               0   \n",
       "4               0           0          0                    0               0   \n",
       "\n",
       "   autoregulatory  autoubiquitination  non-autoregulatory  \n",
       "0               0                   0                   0  \n",
       "1               0                   0                   1  \n",
       "2               0                   0                   1  \n",
       "3               0                   0                   1  \n",
       "4               0                   0                   0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_cleaned.shape)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_number           5280\n",
      "non-autoregulatory     3520\n",
      "autophosphorylation     838\n",
      "autocatalytic           176\n",
      "autoregulation          154\n",
      "autoubiquitination      145\n",
      "autoinhibition          133\n",
      "autoregulatory           81\n",
      "autoinducer              73\n",
      "autolysis                70\n",
      "autoinhibitory           60\n",
      "autoactivation           22\n",
      "autocatalysis            15\n",
      "autofeedback             13\n",
      "autoinduction            11\n",
      "autokinase                8\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check label distribution\n",
    "test_df = df_cleaned[df_cleaned['batch_number'] == 1]\n",
    "numeric_columns = test_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "label_counts = test_df[numeric_columns].sum(axis=0)\n",
    "label_columns = [col for col in df_cleaned.columns[3:] if col != \"Terms_List\"]\n",
    "print(label_counts.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Define Functions for the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: mps\n"
     ]
    }
   ],
   "source": [
    "# Device Configuration\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Device being used: {device}\")\n",
    "\n",
    "# Load tokenizer and base model\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext')\n",
    "bert_model = AutoModel.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data splitting\n",
    "def split_single_batch_data(batch_number, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data from a single batch into train and test sets using stratified sampling.\n",
    "    Prints Train/Test sizes and Non-auto/Auto counts.\n",
    "    \"\"\"\n",
    "    batch_df = df_cleaned[df_cleaned['batch_number'] == batch_number].copy()\n",
    "    X = batch_df['Text_Cleaned']\n",
    "    y = labels_df.loc[batch_df.index].values  # Ensure indexing alignment\n",
    "    \n",
    "    # calculate label distribution\n",
    "    non_auto_count = len(batch_df[batch_df['Terms'] == 'non-autoregulatory'])\n",
    "    auto_count = len(batch_df[batch_df['Terms'] != 'non-autoregulatory'])\n",
    "    \n",
    "    # split data\n",
    "    msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    for train_idx, test_idx in msss.split(X, y):\n",
    "        X_train = X.iloc[train_idx]\n",
    "        X_test = X.iloc[test_idx]\n",
    "        y_train = y[train_idx]\n",
    "        y_test = y[test_idx]\n",
    "    \n",
    "    print(f\"Batch {batch_number} | Train: {len(X_train)}, Test: {len(X_test)} | Non-auto: {non_auto_count}, Auto: {auto_count}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate class weights\n",
    "def get_data_and_weights(batch_number):\n",
    "    \"\"\"\n",
    "    Get data and calculate class weights for a specific batch.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = split_single_batch_data(batch_number)\n",
    "    \n",
    "    # Calculate class weights\n",
    "    pos_weights = []\n",
    "    for i in range(y_train.shape[1]):\n",
    "        neg_count = len(y_train) - np.sum(y_train[:, i])\n",
    "        pos_count = np.sum(y_train[:, i])\n",
    "        pos_weights.append(neg_count / pos_count if pos_count > 0 else 1.0)\n",
    "    \n",
    "    pos_weights = torch.FloatTensor(pos_weights).to(device)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, pos_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset class\n",
    "class PubMedDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.FloatTensor(label)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader\n",
    "def create_dataset_and_loader(X_train, y_train, X_test, y_test, batch_size):\n",
    "    train_dataset = PubMedDataset(X_train, y_train, tokenizer)\n",
    "    test_dataset = PubMedDataset(X_test, y_test, tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "class PubMedBERTClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, dropout=0.1):\n",
    "        super(PubMedBERTClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def train_epoch(model, data_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set thresholds for each label\n",
    "def set_thresholds(pos_weights):\n",
    "    \"\"\"\n",
    "    Set thresholds based on normalized pos_weights, scaled to [0.2, 0.8].\n",
    "    \"\"\"\n",
    "    thresholds = []\n",
    "\n",
    "    if len(pos_weights) != len(label_columns):\n",
    "        raise ValueError(f\"Length mismatch: pos_weights ({len(pos_weights)}) vs label_columns ({len(label_columns)})\")\n",
    "\n",
    "    # Calculate min and max weights for normalization\n",
    "    min_weight = pos_weights.min().item()\n",
    "    max_weight = pos_weights.max().item()\n",
    "    weight_range = max_weight - min_weight\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if weight_range == 0:\n",
    "        weight_range = 1\n",
    "\n",
    "    # Calculate thresholds\n",
    "    for weight in pos_weights:\n",
    "        # Normalize to [0, 1]\n",
    "        normalized_weight = (weight.item() - min_weight) / weight_range\n",
    "\n",
    "        # Map to [0.2, 0.8]\n",
    "        threshold = 0.8 - (0.6 * normalized_weight)\n",
    "\n",
    "        # Clamp to [0.2, 0.8]\n",
    "        threshold = max(0.2, min(threshold, 0.8))\n",
    "\n",
    "        thresholds.append(threshold)\n",
    "\n",
    "    # Print thresholds with two decimal places\n",
    "    formatted_thresholds = [f\"{t:.2f}\" for t in thresholds]\n",
    "    print(f\"\\nDynamic Thresholds: {formatted_thresholds}\")\n",
    "    \n",
    "    return thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, data_loader, criterion, thresholds):\n",
    "    \"\"\"\n",
    "    Evaluate the model with focused metrics output.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probabilities = torch.sigmoid(outputs).cpu().numpy()\n",
    "            \n",
    "            # Apply thresholds per label\n",
    "            predictions = np.array([\n",
    "                (probabilities[:, i] >= thresholds[i]).astype(int) for i in range(len(thresholds))\n",
    "            ]).T\n",
    "\n",
    "            all_predictions.extend(predictions)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # samples metrics\n",
    "    samples_precision = precision_score(all_labels, all_predictions, average='samples', zero_division=0)\n",
    "    samples_recall = recall_score(all_labels, all_predictions, average='samples', zero_division=0)\n",
    "    samples_f1 = f1_score(all_labels, all_predictions, average='samples', zero_division=0)\n",
    "\n",
    "    # f1 metrics\n",
    "    micro_f1 = f1_score(all_labels, all_predictions, average='micro', zero_division=0)\n",
    "    macro_f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "    weighted_f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "\n",
    "    # average loss\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    metrics = {\n",
    "        'loss': avg_loss,\n",
    "        'micro_f1': micro_f1,\n",
    "        'macro_f1': macro_f1,\n",
    "        'weighted_f1': weighted_f1,\n",
    "        'samples_f1': samples_f1,\n",
    "        'samples_precision': samples_precision,\n",
    "        'samples_recall': samples_recall\n",
    "    }\n",
    "    \n",
    "    # output results\n",
    "    print(f\"Loss: {avg_loss:.4f} | Micro F1: {micro_f1:.4f} | Macro F1: {macro_f1:.4f} | Weighted F1: {weighted_f1:.4f} | Samples F1: {samples_f1:.4f}\")\n",
    "    print(f\"Samples Precision: {samples_precision:.4f} | Samples Recall: {samples_recall:.4f}\")\n",
    "    \n",
    "    # return metrics for further analysis\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(batch_number, n_epochs, learning_rate, batch_size):\n",
    "    \"\"\"\n",
    "    Training loop for a single batch with dynamic threshold settings.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test, pos_weights = get_data_and_weights(batch_number)\n",
    "    print(f\"\\nProcessing Batch {batch_number} ...\")\n",
    "\n",
    "    # Calculate dynamic thresholds\n",
    "    thresholds = set_thresholds(pos_weights)\n",
    "    \n",
    "    train_loader, test_loader = create_dataset_and_loader(X_train, y_train, X_test, y_test, batch_size)\n",
    "\n",
    "    model = PubMedBERTClassifier(n_classes=y_train.shape[1]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_samples_f1 = 0.0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs} | Batch {batch_number}\")\n",
    "\n",
    "        train_epoch(model, train_loader, optimizer, criterion)\n",
    "        metrics = evaluate(model, test_loader, criterion, thresholds)\n",
    "\n",
    "        current_samples_f1 = metrics['samples_f1']\n",
    "\n",
    "        # Save best model and thresholds\n",
    "        if current_samples_f1 > best_samples_f1:\n",
    "            best_samples_f1 = current_samples_f1\n",
    "            model_path = f\"../src/model/best_model_batch_{batch_number}.pt\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"  New best model saved to {model_path}\")\n",
    "\n",
    "            # Save thresholds\n",
    "            thresholds_path = f\"../src/model/best_thresholds_batch_{batch_number}.json\"\n",
    "            with open(thresholds_path, \"w\") as f:\n",
    "                json.dump(thresholds, f)\n",
    "            print(f\"  Thresholds saved to {thresholds_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 | Train: 4223, Test: 1057 | Non-auto: 3520, Auto: 1760\n",
      "\n",
      "Processing Batch 1 ...\n",
      "\n",
      "Dynamic Thresholds: ['0.60', '0.50', '0.78', '0.44', '0.74', '0.40', '0.77', '0.73', '0.20', '0.74', '0.80', '0.77', '0.75', '0.77', '0.80']\n",
      "Epoch 1/7 | Batch 1\n",
      "Loss: 0.7703 | Micro F1: 0.1101 | Macro F1: 0.1762 | Weighted F1: 0.0782 | Samples F1: 0.0491\n",
      "Samples Precision: 0.0374 | Samples Recall: 0.1268\n",
      "  New best model saved to ../src/model/best_model_batch_1.pt\n",
      "  Thresholds saved to ../src/model/best_thresholds_batch_1.json\n",
      "Epoch 2/7 | Batch 1\n",
      "Loss: 0.5501 | Micro F1: 0.5500 | Macro F1: 0.3549 | Weighted F1: 0.7401 | Samples F1: 0.6198\n",
      "Samples Precision: 0.6008 | Samples Recall: 0.7129\n",
      "  New best model saved to ../src/model/best_model_batch_1.pt\n",
      "  Thresholds saved to ../src/model/best_thresholds_batch_1.json\n",
      "Epoch 3/7 | Batch 1\n",
      "Loss: 0.3836 | Micro F1: 0.5981 | Macro F1: 0.4176 | Weighted F1: 0.8026 | Samples F1: 0.7025\n",
      "Samples Precision: 0.6776 | Samples Recall: 0.8136\n",
      "  New best model saved to ../src/model/best_model_batch_1.pt\n",
      "  Thresholds saved to ../src/model/best_thresholds_batch_1.json\n",
      "Epoch 4/7 | Batch 1\n",
      "Loss: 0.3589 | Micro F1: 0.7387 | Macro F1: 0.5126 | Weighted F1: 0.8478 | Samples F1: 0.7756\n",
      "Samples Precision: 0.7528 | Samples Recall: 0.8510\n",
      "  New best model saved to ../src/model/best_model_batch_1.pt\n",
      "  Thresholds saved to ../src/model/best_thresholds_batch_1.json\n",
      "Epoch 5/7 | Batch 1\n",
      "Loss: 0.3400 | Micro F1: 0.7860 | Macro F1: 0.5249 | Weighted F1: 0.8771 | Samples F1: 0.8296\n",
      "Samples Precision: 0.8069 | Samples Recall: 0.8945\n",
      "  New best model saved to ../src/model/best_model_batch_1.pt\n",
      "  Thresholds saved to ../src/model/best_thresholds_batch_1.json\n",
      "Epoch 6/7 | Batch 1\n",
      "Loss: 0.3285 | Micro F1: 0.8080 | Macro F1: 0.5321 | Weighted F1: 0.8743 | Samples F1: 0.8387\n",
      "Samples Precision: 0.8170 | Samples Recall: 0.8917\n",
      "  New best model saved to ../src/model/best_model_batch_1.pt\n",
      "  Thresholds saved to ../src/model/best_thresholds_batch_1.json\n",
      "Epoch 7/7 | Batch 1\n",
      "Loss: 0.3447 | Micro F1: 0.8573 | Macro F1: 0.5921 | Weighted F1: 0.9067 | Samples F1: 0.8808\n",
      "Samples Precision: 0.8608 | Samples Recall: 0.9290\n",
      "  New best model saved to ../src/model/best_model_batch_1.pt\n",
      "  Thresholds saved to ../src/model/best_thresholds_batch_1.json\n",
      "Batch 2 | Train: 4221, Test: 1059 | Non-auto: 3520, Auto: 1760\n",
      "\n",
      "Processing Batch 2 ...\n",
      "\n",
      "Dynamic Thresholds: ['0.60', '0.50', '0.78', '0.44', '0.74', '0.40', '0.77', '0.73', '0.20', '0.74', '0.80', '0.77', '0.75', '0.77', '0.80']\n",
      "Epoch 1/7 | Batch 2\n",
      "Loss: 0.1910 | Micro F1: 0.7826 | Macro F1: 0.5566 | Weighted F1: 0.8653 | Samples F1: 0.8058\n",
      "Samples Precision: 0.7822 | Samples Recall: 0.8706\n",
      "  New best model saved to ../src/model/best_model_batch_2.pt\n",
      "  Thresholds saved to ../src/model/best_thresholds_batch_2.json\n",
      "Epoch 2/7 | Batch 2\n",
      "Loss: 0.1973 | Micro F1: 0.7358 | Macro F1: 0.5257 | Weighted F1: 0.8460 | Samples F1: 0.7754\n",
      "Samples Precision: 0.7517 | Samples Recall: 0.8442\n",
      "Epoch 3/7 | Batch 2\n",
      "Loss: 0.1878 | Micro F1: 0.7484 | Macro F1: 0.5052 | Weighted F1: 0.8426 | Samples F1: 0.8003\n",
      "Samples Precision: 0.7830 | Samples Recall: 0.8565\n",
      "Epoch 4/7 | Batch 2\n",
      "Loss: 0.1140 | Micro F1: 0.8747 | Macro F1: 0.6668 | Weighted F1: 0.9138 | Samples F1: 0.8920\n",
      "Samples Precision: 0.8766 | Samples Recall: 0.9263\n",
      "  New best model saved to ../src/model/best_model_batch_2.pt\n",
      "  Thresholds saved to ../src/model/best_thresholds_batch_2.json\n",
      "Epoch 5/7 | Batch 2\n",
      "Loss: 0.0986 | Micro F1: 0.8856 | Macro F1: 0.6752 | Weighted F1: 0.9189 | Samples F1: 0.8987\n",
      "Samples Precision: 0.8852 | Samples Recall: 0.9292\n",
      "  New best model saved to ../src/model/best_model_batch_2.pt\n",
      "  Thresholds saved to ../src/model/best_thresholds_batch_2.json\n",
      "Epoch 6/7 | Batch 2\n",
      "Loss: 0.1085 | Micro F1: 0.9047 | Macro F1: 0.6748 | Weighted F1: 0.9306 | Samples F1: 0.9215\n",
      "Samples Precision: 0.9105 | Samples Recall: 0.9471\n",
      "  New best model saved to ../src/model/best_model_batch_2.pt\n",
      "  Thresholds saved to ../src/model/best_thresholds_batch_2.json\n",
      "Epoch 7/7 | Batch 2\n",
      "Loss: 0.1098 | Micro F1: 0.8979 | Macro F1: 0.7059 | Weighted F1: 0.9180 | Samples F1: 0.9057\n",
      "Samples Precision: 0.8959 | Samples Recall: 0.9273\n",
      "Batch 3 | Train: 4218, Test: 1062 | Non-auto: 3520, Auto: 1760\n",
      "\n",
      "Processing Batch 3 ...\n",
      "\n",
      "Dynamic Thresholds: ['0.60', '0.50', '0.78', '0.44', '0.74', '0.40', '0.77', '0.73', '0.20', '0.74', '0.80', '0.77', '0.75', '0.77', '0.80']\n",
      "Epoch 1/7 | Batch 3\n",
      "Loss: 0.1544 | Micro F1: 0.8622 | Macro F1: 0.6590 | Weighted F1: 0.9255 | Samples F1: 0.8933\n",
      "Samples Precision: 0.8749 | Samples Recall: 0.9411\n",
      "  New best model saved to ../src/model/best_model_batch_3.pt\n",
      "  Thresholds saved to ../src/model/best_thresholds_batch_3.json\n",
      "Epoch 2/7 | Batch 3\n"
     ]
    }
   ],
   "source": [
    "# run model for all batches\n",
    "n_epochs = 7\n",
    "learning_rate = 2e-5\n",
    "batch_size = 16\n",
    "batch_range = 5\n",
    "\n",
    "for batch_num in range(1, batch_range):\n",
    "    train_model(batch_number=batch_num, n_epochs=n_epochs, learning_rate=learning_rate, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Batch | Micro-F1 | Macro-F1 | Weighted-F1 | Samples-F1 | Sample-Precision | Sample-Recall |\n",
    "|-------|----------|----------|-------------|------------|------------------|---------------|\n",
    "|   1   |  0.8749  |  0.6532  |    0.9073   |   0.8897   |      0.8766      |     0.9216    |\n",
    "|   2   |  0.9380  |  0.7721  |    0.9456   |   0.9455   |      0.9406      |     0.9570    |\n",
    "|   3   |  0.9327  |  0.7529  |    0.9492   |   0.9409   |      0.9336      |     0.9578    |\n",
    "|   4   |  0.9618  |  0.8444  |    0.9694   |   0.9665   |      0.9631      |     0.9751    |\n",
    "|   5   |  0.9692  |  0.9292  |    0.9702   |   0.9699   |      0.9707      |     0.9700    |\n",
    "|   6   |  0.9767  |  0.9363  |    0.9773   |   0.9790   |      0.9788      |     0.9810    |\n",
    "|   7   |  0.9818  |  0.9781  |    0.9821   |   0.9820   |      0.9826      |     0.9826    |\n",
    "|   8   |  0.9814  |  0.9073  |    0.9825   |   0.9837   |      0.9836      |     0.9845    |\n",
    "|   9   |  0.9870  |  0.9695  |    0.9873   |   0.9883   |      0.9886      |     0.9897    |\n",
    "|  10   |  0.9879  |  0.9558  |    0.9889   |   0.9886   |      0.9883      |     0.9897    |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoregulatory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
