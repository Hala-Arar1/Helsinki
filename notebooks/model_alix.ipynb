{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0. Import data & data clean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53130, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AC</th>\n",
       "      <th>PMID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Terms</th>\n",
       "      <th>Text_combined</th>\n",
       "      <th>batch_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q8NML3</td>\n",
       "      <td>17183211</td>\n",
       "      <td>RamA, the transcriptional regulator of acetate...</td>\n",
       "      <td>The RamA protein represents a LuxR-type transc...</td>\n",
       "      <td>autoregulation</td>\n",
       "      <td>RamA, the transcriptional regulator of acetate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q9SCZ4</td>\n",
       "      <td>17673660</td>\n",
       "      <td>The FERONIA receptor-like kinase mediates male...</td>\n",
       "      <td>In flowering plants, signaling between the mal...</td>\n",
       "      <td>autophosphorylation</td>\n",
       "      <td>The FERONIA receptor-like kinase mediates male...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q81WX1</td>\n",
       "      <td>12721629</td>\n",
       "      <td>The genome sequence of Bacillus anthracis Ames...</td>\n",
       "      <td>Bacillus anthracis is an endospore-forming bac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The genome sequence of Bacillus anthracis Ames...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P14410</td>\n",
       "      <td>8521865</td>\n",
       "      <td>Phosphorylation of the N-terminal intracellula...</td>\n",
       "      <td>This paper reports the phosphorylation of the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Phosphorylation of the N-terminal intracellula...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P36898</td>\n",
       "      <td>14523231</td>\n",
       "      <td>Mutations in bone morphogenetic protein recept...</td>\n",
       "      <td>Brachydactyly (BD) type A2 is an autosomal dom...</td>\n",
       "      <td>autophosphorylation</td>\n",
       "      <td>Mutations in bone morphogenetic protein recept...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AC      PMID                                              Title  \\\n",
       "0  Q8NML3  17183211  RamA, the transcriptional regulator of acetate...   \n",
       "1  Q9SCZ4  17673660  The FERONIA receptor-like kinase mediates male...   \n",
       "2  Q81WX1  12721629  The genome sequence of Bacillus anthracis Ames...   \n",
       "3  P14410   8521865  Phosphorylation of the N-terminal intracellula...   \n",
       "4  P36898  14523231  Mutations in bone morphogenetic protein recept...   \n",
       "\n",
       "                                            Abstract                Terms  \\\n",
       "0  The RamA protein represents a LuxR-type transc...       autoregulation   \n",
       "1  In flowering plants, signaling between the mal...  autophosphorylation   \n",
       "2  Bacillus anthracis is an endospore-forming bac...                  NaN   \n",
       "3  This paper reports the phosphorylation of the ...                  NaN   \n",
       "4  Brachydactyly (BD) type A2 is an autosomal dom...  autophosphorylation   \n",
       "\n",
       "                                       Text_combined  batch_number  \n",
       "0  RamA, the transcriptional regulator of acetate...             1  \n",
       "1  The FERONIA receptor-like kinase mediates male...             1  \n",
       "2  The genome sequence of Bacillus anthracis Ames...             1  \n",
       "3  Phosphorylation of the N-terminal intracellula...             1  \n",
       "4  Mutations in bone morphogenetic protein recept...             1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/processed/shuffled_10_data.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fiatlux/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# clean text\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = \" \".join([word.strip() for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "df['Text_Cleaned'] = df['Text_combined'].apply(clean_text)\n",
    "\n",
    "# fill nan with 'non-autoregulatory'\n",
    "df['Terms'] = df['Terms'].fillna('non-autoregulatory')\n",
    "\n",
    "# keep only selected columns\n",
    "columns_to_keep = ['batch_number','Text_Cleaned','Terms']\n",
    "df_cleaned = df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53130, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_number</th>\n",
       "      <th>Text_Cleaned</th>\n",
       "      <th>Terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>rama transcriptional regulator acetate metabol...</td>\n",
       "      <td>autoregulation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>feronia receptorlike kinase mediates malefemal...</td>\n",
       "      <td>autophosphorylation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>genome sequence bacillus anthracis ames compar...</td>\n",
       "      <td>non-autoregulatory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>phosphorylation nterminal intracellular tail s...</td>\n",
       "      <td>non-autoregulatory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mutations bone morphogenetic protein receptor ...</td>\n",
       "      <td>autophosphorylation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_number                                       Text_Cleaned  \\\n",
       "0             1  rama transcriptional regulator acetate metabol...   \n",
       "1             1  feronia receptorlike kinase mediates malefemal...   \n",
       "2             1  genome sequence bacillus anthracis ames compar...   \n",
       "3             1  phosphorylation nterminal intracellular tail s...   \n",
       "4             1  mutations bone morphogenetic protein receptor ...   \n",
       "\n",
       "                 Terms  \n",
       "0       autoregulation  \n",
       "1  autophosphorylation  \n",
       "2   non-autoregulatory  \n",
       "3   non-autoregulatory  \n",
       "4  autophosphorylation  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_cleaned.shape)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1h/csb8qjzj1sv0jtrd6ccllm8c0000gn/T/ipykernel_93881/626586514.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['Terms_List'] = df_cleaned['Terms'].apply(\n",
      "/var/folders/1h/csb8qjzj1sv0jtrd6ccllm8c0000gn/T/ipykernel_93881/626586514.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['Terms_List'] = df_cleaned['Terms_List'].apply(lambda x: list(set(x)))\n"
     ]
    }
   ],
   "source": [
    "# convert terms to list\n",
    "df_cleaned['Terms_List'] = df_cleaned['Terms'].apply(\n",
    "    lambda x: [term.strip() for term in x.split(',')]\n",
    ")\n",
    "df_cleaned['Terms_List'] = df_cleaned['Terms_List'].apply(lambda x: list(set(x)))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(df_cleaned['Terms_List'])\n",
    "label_columns = mlb.classes_\n",
    "\n",
    "labels_df = pd.DataFrame(labels, columns=label_columns)\n",
    "existing_columns = [col for col in label_columns if col in df_cleaned.columns]\n",
    "df_cleaned = df_cleaned.drop(columns=existing_columns, errors='ignore')\n",
    "df_cleaned = pd.concat([df_cleaned, labels_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53130, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_number</th>\n",
       "      <th>Text_Cleaned</th>\n",
       "      <th>Terms</th>\n",
       "      <th>Terms_List</th>\n",
       "      <th>autoactivation</th>\n",
       "      <th>autocatalysis</th>\n",
       "      <th>autocatalytic</th>\n",
       "      <th>autofeedback</th>\n",
       "      <th>autoinducer</th>\n",
       "      <th>autoinduction</th>\n",
       "      <th>autoinhibition</th>\n",
       "      <th>autoinhibitory</th>\n",
       "      <th>autokinase</th>\n",
       "      <th>autolysis</th>\n",
       "      <th>autophosphorylation</th>\n",
       "      <th>autoregulation</th>\n",
       "      <th>autoregulatory</th>\n",
       "      <th>autoubiquitination</th>\n",
       "      <th>non-autoregulatory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>rama transcriptional regulator acetate metabol...</td>\n",
       "      <td>autoregulation</td>\n",
       "      <td>[autoregulation]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>feronia receptorlike kinase mediates malefemal...</td>\n",
       "      <td>autophosphorylation</td>\n",
       "      <td>[autophosphorylation]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>genome sequence bacillus anthracis ames compar...</td>\n",
       "      <td>non-autoregulatory</td>\n",
       "      <td>[non-autoregulatory]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>phosphorylation nterminal intracellular tail s...</td>\n",
       "      <td>non-autoregulatory</td>\n",
       "      <td>[non-autoregulatory]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mutations bone morphogenetic protein receptor ...</td>\n",
       "      <td>autophosphorylation</td>\n",
       "      <td>[autophosphorylation]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_number                                       Text_Cleaned  \\\n",
       "0             1  rama transcriptional regulator acetate metabol...   \n",
       "1             1  feronia receptorlike kinase mediates malefemal...   \n",
       "2             1  genome sequence bacillus anthracis ames compar...   \n",
       "3             1  phosphorylation nterminal intracellular tail s...   \n",
       "4             1  mutations bone morphogenetic protein receptor ...   \n",
       "\n",
       "                 Terms             Terms_List  autoactivation  autocatalysis  \\\n",
       "0       autoregulation       [autoregulation]               0              0   \n",
       "1  autophosphorylation  [autophosphorylation]               0              0   \n",
       "2   non-autoregulatory   [non-autoregulatory]               0              0   \n",
       "3   non-autoregulatory   [non-autoregulatory]               0              0   \n",
       "4  autophosphorylation  [autophosphorylation]               0              0   \n",
       "\n",
       "   autocatalytic  autofeedback  autoinducer  autoinduction  autoinhibition  \\\n",
       "0              0             0            0              0               0   \n",
       "1              0             0            0              0               0   \n",
       "2              0             0            0              0               0   \n",
       "3              0             0            0              0               0   \n",
       "4              0             0            0              0               0   \n",
       "\n",
       "   autoinhibitory  autokinase  autolysis  autophosphorylation  autoregulation  \\\n",
       "0               0           0          0                    0               1   \n",
       "1               0           0          0                    1               0   \n",
       "2               0           0          0                    0               0   \n",
       "3               0           0          0                    0               0   \n",
       "4               0           0          0                    1               0   \n",
       "\n",
       "   autoregulatory  autoubiquitination  non-autoregulatory  \n",
       "0               0                   0                   0  \n",
       "1               0                   0                   0  \n",
       "2               0                   0                   1  \n",
       "3               0                   0                   1  \n",
       "4               0                   0                   0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_cleaned.shape)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_number           5313\n",
      "non-autoregulatory     3542\n",
      "autophosphorylation     844\n",
      "autocatalytic           176\n",
      "autoregulation          154\n",
      "autoubiquitination      145\n",
      "autoinhibition          135\n",
      "autoregulatory           84\n",
      "autoinducer              73\n",
      "autolysis                70\n",
      "autoinhibitory           60\n",
      "autoactivation           22\n",
      "autocatalysis            15\n",
      "autofeedback             13\n",
      "autoinduction            11\n",
      "autokinase                8\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check label distribution\n",
    "test_df = df_cleaned[df_cleaned['batch_number'] == 1]\n",
    "numeric_columns = test_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "label_counts = test_df[numeric_columns].sum(axis=0)\n",
    "label_columns = [col for col in df_cleaned.columns[3:] if col != \"Terms_List\"]\n",
    "print(label_counts.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Define Functions for the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: mps\n"
     ]
    }
   ],
   "source": [
    "# Device Configuration\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Device being used: {device}\")\n",
    "\n",
    "# Load tokenizer and base model\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext')\n",
    "bert_model = AutoModel.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data splitting\n",
    "def split_single_batch_data(batch_number, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data from a single batch into train and test sets using stratified sampling.\n",
    "    Prints Train/Test sizes and Non-auto/Auto counts.\n",
    "    \"\"\"\n",
    "    batch_df = df_cleaned[df_cleaned['batch_number'] == batch_number].copy()\n",
    "    X = batch_df['Text_Cleaned']\n",
    "    y = labels_df.loc[batch_df.index].values  # Ensure indexing alignment\n",
    "    \n",
    "    # calculate label distribution\n",
    "    non_auto_count = len(batch_df[batch_df['Terms'] == 'non-autoregulatory'])\n",
    "    auto_count = len(batch_df[batch_df['Terms'] != 'non-autoregulatory'])\n",
    "    \n",
    "    # split data\n",
    "    msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    for train_idx, test_idx in msss.split(X, y):\n",
    "        X_train = X.iloc[train_idx]\n",
    "        X_test = X.iloc[test_idx]\n",
    "        y_train = y[train_idx]\n",
    "        y_test = y[test_idx]\n",
    "    \n",
    "    print(f\"Batch {batch_number} | Train: {len(X_train)}, Test: {len(X_test)} | Non-auto: {non_auto_count}, Auto: {auto_count}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate class weights\n",
    "def get_data_and_weights(batch_number):\n",
    "    \"\"\"\n",
    "    Get data and calculate class weights for a specific batch.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = split_single_batch_data(batch_number)\n",
    "    \n",
    "    # Calculate class weights\n",
    "    pos_weights = []\n",
    "    for i in range(y_train.shape[1]):\n",
    "        neg_count = len(y_train) - np.sum(y_train[:, i])\n",
    "        pos_count = np.sum(y_train[:, i])\n",
    "        pos_weights.append(neg_count / pos_count if pos_count > 0 else 1.0)\n",
    "    \n",
    "    pos_weights = torch.FloatTensor(pos_weights).to(device)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, pos_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset class\n",
    "class PubMedDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.FloatTensor(label)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader\n",
    "def create_dataset_and_loader(X_train, y_train, X_test, y_test, batch_size):\n",
    "    train_dataset = PubMedDataset(X_train, y_train, tokenizer)\n",
    "    test_dataset = PubMedDataset(X_test, y_test, tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "class PubMedBERTClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, dropout=0.1):\n",
    "        super(PubMedBERTClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def train_epoch(model, data_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set thresholds for each label\n",
    "def set_thresholds(pos_weights):\n",
    "    \"\"\"\n",
    "    Set thresholds based on normalized pos_weights, scaled to [0.2, 0.8].\n",
    "    \"\"\"\n",
    "    thresholds = []\n",
    "\n",
    "    if len(pos_weights) != len(label_columns):\n",
    "        raise ValueError(f\"Length mismatch: pos_weights ({len(pos_weights)}) vs label_columns ({len(label_columns)})\")\n",
    "\n",
    "    # Calculate min and max weights for normalization\n",
    "    min_weight = pos_weights.min().item()\n",
    "    max_weight = pos_weights.max().item()\n",
    "    weight_range = max_weight - min_weight\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if weight_range == 0:\n",
    "        weight_range = 1\n",
    "\n",
    "    # Calculate thresholds\n",
    "    for weight in pos_weights:\n",
    "        # Normalize to [0, 1]\n",
    "        normalized_weight = (weight.item() - min_weight) / weight_range\n",
    "\n",
    "        # Map to [0.2, 0.8]\n",
    "        threshold = 0.8 - (0.6 * normalized_weight)\n",
    "\n",
    "        # Clamp to [0.2, 0.8]\n",
    "        threshold = max(0.2, min(threshold, 0.8))\n",
    "\n",
    "        thresholds.append(threshold)\n",
    "\n",
    "    # Print thresholds with two decimal places\n",
    "    formatted_thresholds = [f\"{t:.2f}\" for t in thresholds]\n",
    "    print(f\"\\nDynamic Thresholds: {formatted_thresholds}\")\n",
    "    \n",
    "    return thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, data_loader, criterion, thresholds):\n",
    "    \"\"\"\n",
    "    Evaluate the model with focused metrics output.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probabilities = torch.sigmoid(outputs).cpu().numpy()\n",
    "            \n",
    "            # Apply thresholds per label\n",
    "            predictions = np.array([\n",
    "                (probabilities[:, i] >= thresholds[i]).astype(int) for i in range(len(thresholds))\n",
    "            ]).T\n",
    "\n",
    "            all_predictions.extend(predictions)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # samples metrics\n",
    "    samples_precision = precision_score(all_labels, all_predictions, average='samples', zero_division=0)\n",
    "    samples_recall = recall_score(all_labels, all_predictions, average='samples', zero_division=0)\n",
    "    samples_f1 = f1_score(all_labels, all_predictions, average='samples', zero_division=0)\n",
    "\n",
    "    # f1 metrics\n",
    "    micro_f1 = f1_score(all_labels, all_predictions, average='micro', zero_division=0)\n",
    "    macro_f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "    weighted_f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "\n",
    "    # average loss\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    metrics = {\n",
    "        'loss': avg_loss,\n",
    "        'micro_f1': micro_f1,\n",
    "        'macro_f1': macro_f1,\n",
    "        'weighted_f1': weighted_f1,\n",
    "        'samples_f1': samples_f1,\n",
    "        'samples_precision': samples_precision,\n",
    "        'samples_recall': samples_recall\n",
    "    }\n",
    "    \n",
    "    # output results\n",
    "    print(f\"Loss: {avg_loss:.4f} | Micro F1: {micro_f1:.4f} | Macro F1: {macro_f1:.4f} | Weighted F1: {weighted_f1:.4f} | Samples F1: {samples_f1:.4f}\")\n",
    "    print(f\"Samples Precision: {samples_precision:.4f} | Samples Recall: {samples_recall:.4f}\")\n",
    "    \n",
    "    # return metrics for further analysis\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(batch_number, n_epochs, learning_rate, batch_size):\n",
    "    \"\"\"\n",
    "    Training loop for a single batch with dynamic threshold settings.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test, pos_weights = get_data_and_weights(batch_number)\n",
    "    print(f\"\\nProcessing Batch {batch_number} ...\")\n",
    "\n",
    "    # Calculate dynamic thresholds\n",
    "    thresholds = set_thresholds(pos_weights)\n",
    "    \n",
    "    train_loader, test_loader = create_dataset_and_loader(X_train, y_train, X_test, y_test, batch_size)\n",
    "\n",
    "    model = PubMedBERTClassifier(n_classes=y_train.shape[1]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_samples_f1 = 0.0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs} | Batch {batch_number}\")\n",
    "\n",
    "        train_epoch(model, train_loader, optimizer, criterion)\n",
    "        metrics = evaluate(model, test_loader, criterion, thresholds)\n",
    "\n",
    "        current_samples_f1 = metrics['samples_f1']\n",
    "\n",
    "        # Save best model and thresholds\n",
    "        if current_samples_f1 > best_samples_f1:\n",
    "            best_samples_f1 = current_samples_f1\n",
    "            model_path = f\"../src/model/best_model_batch_{batch_number}.pt\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"  New best model saved to {model_path}\")\n",
    "\n",
    "            # Save thresholds\n",
    "            thresholds_path = f\"../src/model/best_thresholds_batch_{batch_number}.json\"\n",
    "            with open(thresholds_path, \"w\") as f:\n",
    "                json.dump(thresholds, f)\n",
    "            print(f\"  Thresholds saved to {thresholds_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 | Train: 4248, Test: 1065 | Non-auto: 3542, Auto: 1771\n",
      "\n",
      "Processing Batch 1 ...\n",
      "\n",
      "Dynamic Thresholds: ['0.60', '0.50', '0.78', '0.44', '0.74', '0.40', '0.77', '0.73', '0.20', '0.74', '0.80', '0.77', '0.75', '0.77', '0.80']\n",
      "Epoch 1/7 | Batch 1\n",
      "Loss: 0.9384 | Micro F1: 0.0566 | Macro F1: 0.0763 | Weighted F1: 0.0310 | Samples F1: 0.0305\n",
      "Samples Precision: 0.0175 | Samples Recall: 0.1310\n",
      "  New best model saved to ../src/model/best_model_batch_1.pt\n",
      "  Thresholds saved to ../src/model/best_thresholds_batch_1.json\n",
      "Epoch 2/7 | Batch 1\n",
      "Loss: 0.3468 | Micro F1: 0.5675 | Macro F1: 0.4142 | Weighted F1: 0.7887 | Samples F1: 0.6616\n",
      "Samples Precision: 0.6359 | Samples Recall: 0.7798\n",
      "  New best model saved to ../src/model/best_model_batch_1.pt\n",
      "  Thresholds saved to ../src/model/best_thresholds_batch_1.json\n",
      "Epoch 3/7 | Batch 1\n",
      "Loss: 0.2170 | Micro F1: 0.7553 | Macro F1: 0.5094 | Weighted F1: 0.8637 | Samples F1: 0.8033\n",
      "Samples Precision: 0.7799 | Samples Recall: 0.8826\n",
      "  New best model saved to ../src/model/best_model_batch_1.pt\n",
      "  Thresholds saved to ../src/model/best_thresholds_batch_1.json\n",
      "Epoch 4/7 | Batch 1\n",
      "Loss: 0.1895 | Micro F1: 0.7552 | Macro F1: 0.5626 | Weighted F1: 0.8454 | Samples F1: 0.7709\n",
      "Samples Precision: 0.7405 | Samples Recall: 0.8493\n",
      "Epoch 5/7 | Batch 1\n",
      "Loss: 0.1686 | Micro F1: 0.7946 | Macro F1: 0.5874 | Weighted F1: 0.8690 | Samples F1: 0.8167\n",
      "Samples Precision: 0.7921 | Samples Recall: 0.8793\n",
      "  New best model saved to ../src/model/best_model_batch_1.pt\n",
      "  Thresholds saved to ../src/model/best_thresholds_batch_1.json\n",
      "Epoch 6/7 | Batch 1\n",
      "Loss: 0.1596 | Micro F1: 0.8747 | Macro F1: 0.6488 | Weighted F1: 0.9111 | Samples F1: 0.8867\n",
      "Samples Precision: 0.8725 | Samples Recall: 0.9225\n",
      "  New best model saved to ../src/model/best_model_batch_1.pt\n",
      "  Thresholds saved to ../src/model/best_thresholds_batch_1.json\n",
      "Epoch 7/7 | Batch 1\n",
      "Loss: 0.1485 | Micro F1: 0.8749 | Macro F1: 0.6532 | Weighted F1: 0.9073 | Samples F1: 0.8897\n",
      "Samples Precision: 0.8766 | Samples Recall: 0.9216\n",
      "  New best model saved to ../src/model/best_model_batch_1.pt\n",
      "  Thresholds saved to ../src/model/best_thresholds_batch_1.json\n",
      "Batch 2 | Train: 4252, Test: 1061 | Non-auto: 3542, Auto: 1771\n",
      "\n",
      "Processing Batch 2 ...\n",
      "\n",
      "Dynamic Thresholds: ['0.60', '0.50', '0.78', '0.44', '0.74', '0.40', '0.77', '0.73', '0.20', '0.74', '0.80', '0.77', '0.75', '0.77', '0.80']\n",
      "Epoch 1/7 | Batch 2\n"
     ]
    }
   ],
   "source": [
    "# run model for all batches\n",
    "n_epochs = 7\n",
    "learning_rate = 2e-5\n",
    "batch_size = 16\n",
    "\n",
    "for batch_num in range(1, 11):\n",
    "    train_model(batch_number=batch_num, n_epochs=n_epochs, learning_rate=learning_rate, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoregulatory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
