{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2870573,"sourceType":"datasetVersion","datasetId":1758005},{"sourceId":11840415,"sourceType":"datasetVersion","datasetId":7439220}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install iterative-stratification\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:04:50.132684Z","iopub.execute_input":"2025-05-16T20:04:50.132966Z","iopub.status.idle":"2025-05-16T20:04:54.641985Z","shell.execute_reply.started":"2025-05-16T20:04:50.132945Z","shell.execute_reply":"2025-05-16T20:04:54.641138Z"}},"outputs":[{"name":"stdout","text":"Collecting iterative-stratification\n  Downloading iterative_stratification-0.1.9-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from iterative-stratification) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from iterative-stratification) (1.13.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from iterative-stratification) (1.2.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->iterative-stratification) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->iterative-stratification) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->iterative-stratification) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->iterative-stratification) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->iterative-stratification) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->iterative-stratification) (2.4.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->iterative-stratification) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->iterative-stratification) (3.5.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->iterative-stratification) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->iterative-stratification) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->iterative-stratification) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->iterative-stratification) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->iterative-stratification) (2024.2.0)\nDownloading iterative_stratification-0.1.9-py3-none-any.whl (8.5 kB)\nInstalling collected packages: iterative-stratification\nSuccessfully installed iterative-stratification-0.1.9\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport json\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n\nimport torch\nfrom torch import nn\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:04:54.643171Z","iopub.execute_input":"2025-05-16T20:04:54.643506Z","iopub.status.idle":"2025-05-16T20:05:02.392649Z","shell.execute_reply.started":"2025-05-16T20:04:54.643459Z","shell.execute_reply":"2025-05-16T20:05:02.391759Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"**0. Import data & data clean**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/shuffled-2/shuffled_10_data.csv\")\nprint(df.shape)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:05:02.394139Z","iopub.execute_input":"2025-05-16T20:05:02.394602Z","iopub.status.idle":"2025-05-16T20:05:05.369194Z","shell.execute_reply.started":"2025-05-16T20:05:02.394578Z","shell.execute_reply":"2025-05-16T20:05:05.368435Z"}},"outputs":[{"name":"stdout","text":"(52800, 7)\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"       AC      PMID                                              Title  \\\n0  P06169   2185016  Autoregulation may control the expression of y...   \n1  P0AEM5  12704152  Complete genome sequence and comparative genom...   \n2  B8FZE0  22316246  Genome sequence of Desulfitobacterium hafniens...   \n3  P14656  12060286  Overlapping expression of cytosolic glutamine ...   \n4  Q7XXS4  27052628  Both overexpression and suppression of an Oryz...   \n\n                                            Abstract           Terms  \\\n0  Recently we deleted the pyruvate decarboxylase...  autoregulation   \n1  We determined the complete genome sequence of ...             NaN   \n2  The genome of the Gram-positive, metal-reducin...             NaN   \n3  In order to estimate whether cytosolic glutami...             NaN   \n4  Tight and accurate regulation of immunity and ...  autoactivation   \n\n                                       Text_combined  batch_number  \n0  Autoregulation may control the expression of y...             1  \n1  Complete genome sequence and comparative genom...             1  \n2  Genome sequence of Desulfitobacterium hafniens...             1  \n3  Overlapping expression of cytosolic glutamine ...             1  \n4  Both overexpression and suppression of an Oryz...             1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AC</th>\n      <th>PMID</th>\n      <th>Title</th>\n      <th>Abstract</th>\n      <th>Terms</th>\n      <th>Text_combined</th>\n      <th>batch_number</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>P06169</td>\n      <td>2185016</td>\n      <td>Autoregulation may control the expression of y...</td>\n      <td>Recently we deleted the pyruvate decarboxylase...</td>\n      <td>autoregulation</td>\n      <td>Autoregulation may control the expression of y...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>P0AEM5</td>\n      <td>12704152</td>\n      <td>Complete genome sequence and comparative genom...</td>\n      <td>We determined the complete genome sequence of ...</td>\n      <td>NaN</td>\n      <td>Complete genome sequence and comparative genom...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>B8FZE0</td>\n      <td>22316246</td>\n      <td>Genome sequence of Desulfitobacterium hafniens...</td>\n      <td>The genome of the Gram-positive, metal-reducin...</td>\n      <td>NaN</td>\n      <td>Genome sequence of Desulfitobacterium hafniens...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>P14656</td>\n      <td>12060286</td>\n      <td>Overlapping expression of cytosolic glutamine ...</td>\n      <td>In order to estimate whether cytosolic glutami...</td>\n      <td>NaN</td>\n      <td>Overlapping expression of cytosolic glutamine ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Q7XXS4</td>\n      <td>27052628</td>\n      <td>Both overexpression and suppression of an Oryz...</td>\n      <td>Tight and accurate regulation of immunity and ...</td>\n      <td>autoactivation</td>\n      <td>Both overexpression and suppression of an Oryz...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# clean text\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = re.sub(r'\\d+', '', text)\n    text = \" \".join([word.strip() for word in text.split() if word not in stop_words])\n    return text\n\ndf['Text_Cleaned'] = df['Text_combined'].apply(clean_text)\n\n# fill nan with 'non-autoregulatory'\ndf['Terms'] = df['Terms'].fillna('non-autoregulatory')\n\n# keep only selected columns\ncolumns_to_keep = ['batch_number','Text_Cleaned','Terms']\ndf_cleaned = df[columns_to_keep]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:05:05.370723Z","iopub.execute_input":"2025-05-16T20:05:05.371029Z","iopub.status.idle":"2025-05-16T20:05:09.989631Z","shell.execute_reply.started":"2025-05-16T20:05:05.371006Z","shell.execute_reply":"2025-05-16T20:05:09.988674Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(df_cleaned.shape)\ndf_cleaned.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:05:09.990629Z","iopub.execute_input":"2025-05-16T20:05:09.990869Z","iopub.status.idle":"2025-05-16T20:05:09.999448Z","shell.execute_reply.started":"2025-05-16T20:05:09.990850Z","shell.execute_reply":"2025-05-16T20:05:09.998626Z"}},"outputs":[{"name":"stdout","text":"(52800, 3)\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   batch_number                                       Text_Cleaned  \\\n0             1  autoregulation may control expression yeast py...   \n1             1  complete genome sequence comparative genomics ...   \n2             1  genome sequence desulfitobacterium hafniense d...   \n3             1  overlapping expression cytosolic glutamine syn...   \n4             1  overexpression suppression oryza sativa nblrrl...   \n\n                Terms  \n0      autoregulation  \n1  non-autoregulatory  \n2  non-autoregulatory  \n3  non-autoregulatory  \n4      autoactivation  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>batch_number</th>\n      <th>Text_Cleaned</th>\n      <th>Terms</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>autoregulation may control expression yeast py...</td>\n      <td>autoregulation</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>complete genome sequence comparative genomics ...</td>\n      <td>non-autoregulatory</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>genome sequence desulfitobacterium hafniense d...</td>\n      <td>non-autoregulatory</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>overlapping expression cytosolic glutamine syn...</td>\n      <td>non-autoregulatory</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>overexpression suppression oryza sativa nblrrl...</td>\n      <td>autoactivation</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# convert terms to list\ndf_cleaned['Terms_List'] = df_cleaned['Terms'].apply(\n    lambda x: [term.strip() for term in x.split(',')]\n)\ndf_cleaned['Terms_List'] = df_cleaned['Terms_List'].apply(lambda x: list(set(x)))\n\nmlb = MultiLabelBinarizer()\nlabels = mlb.fit_transform(df_cleaned['Terms_List'])\nlabel_columns = mlb.classes_\n\nlabels_df = pd.DataFrame(labels, columns=label_columns)\nexisting_columns = [col for col in label_columns if col in df_cleaned.columns]\ndf_cleaned = df_cleaned.drop(columns=existing_columns, errors='ignore')\ndf_cleaned = pd.concat([df_cleaned, labels_df], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:05:10.000231Z","iopub.execute_input":"2025-05-16T20:05:10.000490Z","iopub.status.idle":"2025-05-16T20:05:10.301326Z","shell.execute_reply.started":"2025-05-16T20:05:10.000470Z","shell.execute_reply":"2025-05-16T20:05:10.300683Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-7-ad1ee87dd14c>:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_cleaned['Terms_List'] = df_cleaned['Terms'].apply(\n<ipython-input-7-ad1ee87dd14c>:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_cleaned['Terms_List'] = df_cleaned['Terms_List'].apply(lambda x: list(set(x)))\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(df_cleaned.shape)\ndf_cleaned.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:05:10.302056Z","iopub.execute_input":"2025-05-16T20:05:10.302260Z","iopub.status.idle":"2025-05-16T20:05:10.316171Z","shell.execute_reply.started":"2025-05-16T20:05:10.302242Z","shell.execute_reply":"2025-05-16T20:05:10.315422Z"}},"outputs":[{"name":"stdout","text":"(52800, 19)\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   batch_number                                       Text_Cleaned  \\\n0             1  autoregulation may control expression yeast py...   \n1             1  complete genome sequence comparative genomics ...   \n2             1  genome sequence desulfitobacterium hafniense d...   \n3             1  overlapping expression cytosolic glutamine syn...   \n4             1  overexpression suppression oryza sativa nblrrl...   \n\n                Terms            Terms_List  autoactivation  autocatalysis  \\\n0      autoregulation      [autoregulation]               0              0   \n1  non-autoregulatory  [non-autoregulatory]               0              0   \n2  non-autoregulatory  [non-autoregulatory]               0              0   \n3  non-autoregulatory  [non-autoregulatory]               0              0   \n4      autoactivation      [autoactivation]               1              0   \n\n   autocatalytic  autofeedback  autoinducer  autoinduction  autoinhibition  \\\n0              0             0            0              0               0   \n1              0             0            0              0               0   \n2              0             0            0              0               0   \n3              0             0            0              0               0   \n4              0             0            0              0               0   \n\n   autoinhibitory  autokinase  autolysis  autophosphorylation  autoregulation  \\\n0               0           0          0                    0               1   \n1               0           0          0                    0               0   \n2               0           0          0                    0               0   \n3               0           0          0                    0               0   \n4               0           0          0                    0               0   \n\n   autoregulatory  autoubiquitination  non-autoregulatory  \n0               0                   0                   0  \n1               0                   0                   1  \n2               0                   0                   1  \n3               0                   0                   1  \n4               0                   0                   0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>batch_number</th>\n      <th>Text_Cleaned</th>\n      <th>Terms</th>\n      <th>Terms_List</th>\n      <th>autoactivation</th>\n      <th>autocatalysis</th>\n      <th>autocatalytic</th>\n      <th>autofeedback</th>\n      <th>autoinducer</th>\n      <th>autoinduction</th>\n      <th>autoinhibition</th>\n      <th>autoinhibitory</th>\n      <th>autokinase</th>\n      <th>autolysis</th>\n      <th>autophosphorylation</th>\n      <th>autoregulation</th>\n      <th>autoregulatory</th>\n      <th>autoubiquitination</th>\n      <th>non-autoregulatory</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>autoregulation may control expression yeast py...</td>\n      <td>autoregulation</td>\n      <td>[autoregulation]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>complete genome sequence comparative genomics ...</td>\n      <td>non-autoregulatory</td>\n      <td>[non-autoregulatory]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>genome sequence desulfitobacterium hafniense d...</td>\n      <td>non-autoregulatory</td>\n      <td>[non-autoregulatory]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>overlapping expression cytosolic glutamine syn...</td>\n      <td>non-autoregulatory</td>\n      <td>[non-autoregulatory]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>overexpression suppression oryza sativa nblrrl...</td>\n      <td>autoactivation</td>\n      <td>[autoactivation]</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# check label distribution\ntest_df = df_cleaned[df_cleaned['batch_number'] == 1]\nnumeric_columns = test_df.select_dtypes(include=['int64', 'float64']).columns\nlabel_counts = test_df[numeric_columns].sum(axis=0)\nlabel_columns = [col for col in df_cleaned.columns[3:] if col != \"Terms_List\"]\nprint(label_counts.sort_values(ascending=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:05:10.318564Z","iopub.execute_input":"2025-05-16T20:05:10.318781Z","iopub.status.idle":"2025-05-16T20:05:10.335700Z","shell.execute_reply.started":"2025-05-16T20:05:10.318762Z","shell.execute_reply":"2025-05-16T20:05:10.334843Z"}},"outputs":[{"name":"stdout","text":"batch_number           5280\nnon-autoregulatory     3520\nautophosphorylation     838\nautocatalytic           176\nautoregulation          154\nautoubiquitination      145\nautoinhibition          133\nautoregulatory           81\nautoinducer              73\nautolysis                70\nautoinhibitory           60\nautoactivation           22\nautocatalysis            15\nautofeedback             13\nautoinduction            11\nautokinase                8\ndtype: int64\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"**1. Define Functions for the Model**","metadata":{}},{"cell_type":"code","source":"# Device Configuration\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"Device being used: {device}\")\n\n# Load tokenizer and base model\ntokenizer = AutoTokenizer.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext')\nbert_model = AutoModel.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:05:10.336925Z","iopub.execute_input":"2025-05-16T20:05:10.337134Z","iopub.status.idle":"2025-05-16T20:05:30.841552Z","shell.execute_reply.started":"2025-05-16T20:05:10.337100Z","shell.execute_reply":"2025-05-16T20:05:30.840656Z"}},"outputs":[{"name":"stdout","text":"Device being used: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83f08a0b4faf446a9644970102967d83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51142b21e0e04ebc86970e6b111f5250"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e63381cb452e4c0aa7b5f2e7af0d18fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11376f738ba54612941ce370daeafb94"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# data splitting\ndef split_single_batch_data(batch_number, test_size=0.2, random_state=42):\n    \"\"\"\n    Split data from a single batch into train and test sets using stratified sampling.\n    Prints Train/Test sizes and Non-auto/Auto counts.\n    \"\"\"\n    batch_df = df_cleaned[df_cleaned['batch_number'] == batch_number].copy()\n    X = batch_df['Text_Cleaned']\n    y = labels_df.loc[batch_df.index].values  # Ensure indexing alignment\n    \n    # calculate label distribution\n    non_auto_count = len(batch_df[batch_df['Terms'] == 'non-autoregulatory'])\n    auto_count = len(batch_df[batch_df['Terms'] != 'non-autoregulatory'])\n    \n    # split data\n    msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n    \n    for train_idx, test_idx in msss.split(X, y):\n        X_train = X.iloc[train_idx]\n        X_test = X.iloc[test_idx]\n        y_train = y[train_idx]\n        y_test = y[test_idx]\n    \n    print(f\"Batch {batch_number} | Train: {len(X_train)}, Test: {len(X_test)} | Non-auto: {non_auto_count}, Auto: {auto_count}\")\n    \n    return X_train, X_test, y_train, y_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:05:30.842548Z","iopub.execute_input":"2025-05-16T20:05:30.843025Z","iopub.status.idle":"2025-05-16T20:05:30.850030Z","shell.execute_reply.started":"2025-05-16T20:05:30.843002Z","shell.execute_reply":"2025-05-16T20:05:30.848904Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# calculate class weights\ndef get_data_and_weights(batch_number):\n    \"\"\"\n    Get data and calculate class weights for a specific batch.\n    \"\"\"\n    X_train, X_test, y_train, y_test = split_single_batch_data(batch_number)\n    \n    # Calculate class weights\n    pos_weights = []\n    for i in range(y_train.shape[1]):\n        neg_count = len(y_train) - np.sum(y_train[:, i])\n        pos_count = np.sum(y_train[:, i])\n        pos_weights.append(neg_count / pos_count if pos_count > 0 else 1.0)\n    \n    pos_weights = torch.FloatTensor(pos_weights).to(device)\n    \n    return X_train, X_test, y_train, y_test, pos_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:05:30.850709Z","iopub.execute_input":"2025-05-16T20:05:30.850974Z","iopub.status.idle":"2025-05-16T20:05:30.882450Z","shell.execute_reply.started":"2025-05-16T20:05:30.850953Z","shell.execute_reply":"2025-05-16T20:05:30.881606Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# create dataset class\nclass PubMedDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts.iloc[idx])\n        label = self.labels[idx]\n        \n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.FloatTensor(label)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:05:30.883464Z","iopub.execute_input":"2025-05-16T20:05:30.883770Z","iopub.status.idle":"2025-05-16T20:05:30.899629Z","shell.execute_reply.started":"2025-05-16T20:05:30.883743Z","shell.execute_reply":"2025-05-16T20:05:30.898884Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# create data loader\ndef create_dataset_and_loader(X_train, y_train, X_test, y_test, batch_size):\n    train_dataset = PubMedDataset(X_train, y_train, tokenizer)\n    test_dataset = PubMedDataset(X_test, y_test, tokenizer)\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n    \n    return train_loader, test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:05:30.900546Z","iopub.execute_input":"2025-05-16T20:05:30.900813Z","iopub.status.idle":"2025-05-16T20:05:30.918863Z","shell.execute_reply.started":"2025-05-16T20:05:30.900794Z","shell.execute_reply":"2025-05-16T20:05:30.918017Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# create model\nclass LabelWiseAttentionClassifier(nn.Module):\n    def __init__(self, base_model, num_labels, dropout=0.1):\n        super().__init__()\n        self.bert = base_model\n        self.num_labels = num_labels\n        self.dropout = nn.Dropout(dropout)\n        self.attention = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(self.bert.config.hidden_size, 1),\n                nn.Softmax(dim=1)\n            ) for _ in range(num_labels)\n        ])\n        self.classifiers = nn.ModuleList([\n            nn.Linear(self.bert.config.hidden_size, 1) for _ in range(num_labels)\n        ])\n\n    def forward(self, input_ids, attention_mask):\n        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        hidden_states = output.last_hidden_state  # shape: (batch_size, seq_len, hidden_dim)\n         \n        \n        logits = []\n        for i in range(self.num_labels):\n            attn_weights = self.attention[i](hidden_states)  # shape: (batch_size, seq_len, 1)\n            \n            context_vector = torch.sum(attn_weights * hidden_states, dim=1)  # (batch_size, hidden_dim)\n            \n            context_vector = self.dropout(context_vector)\n            logit = self.classifiers[i](context_vector)  # (batch_size, 1)\n     \n            logits.append(logit)\n\n        logits = torch.cat(logits, dim=1)  # shape: (batch_size, num_labels)\n      \n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:05:30.919589Z","iopub.execute_input":"2025-05-16T20:05:30.919808Z","iopub.status.idle":"2025-05-16T20:05:30.928768Z","shell.execute_reply.started":"2025-05-16T20:05:30.919789Z","shell.execute_reply":"2025-05-16T20:05:30.927984Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# training function\ndef train_epoch(model, data_loader, optimizer, criterion):\n    model.train()\n    total_loss = 0\n    \n    for batch in data_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(data_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:05:30.929586Z","iopub.execute_input":"2025-05-16T20:05:30.929828Z","iopub.status.idle":"2025-05-16T20:05:30.948725Z","shell.execute_reply.started":"2025-05-16T20:05:30.929809Z","shell.execute_reply":"2025-05-16T20:05:30.948099Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# set thresholds for each label\ndef set_thresholds(pos_weights):\n    \"\"\"\n    Set thresholds based on normalized pos_weights, scaled to [0.2, 0.8].\n    \"\"\"\n    thresholds = []\n\n    if len(pos_weights) != len(label_columns):\n        raise ValueError(f\"Length mismatch: pos_weights ({len(pos_weights)}) vs label_columns ({len(label_columns)})\")\n\n    # Calculate min and max weights for normalization\n    min_weight = pos_weights.min().item()\n    max_weight = pos_weights.max().item()\n    weight_range = max_weight - min_weight\n\n    # Avoid division by zero\n    if weight_range == 0:\n        weight_range = 1\n\n    # Calculate thresholds\n    for weight in pos_weights:\n        # Normalize to [0, 1]\n        normalized_weight = (weight.item() - min_weight) / weight_range\n\n        # Map to [0.2, 0.8]\n        threshold = 0.8 - (0.6 * normalized_weight)\n\n        # Clamp to [0.2, 0.8]\n        threshold = max(0.2, min(threshold, 0.8))\n\n        thresholds.append(threshold)\n\n    # Print thresholds with two decimal places\n    formatted_thresholds = [f\"{t:.2f}\" for t in thresholds]\n    print(f\"\\nDynamic Thresholds: {formatted_thresholds}\")\n    \n    return thresholds\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:05:30.949536Z","iopub.execute_input":"2025-05-16T20:05:30.949766Z","iopub.status.idle":"2025-05-16T20:05:30.962690Z","shell.execute_reply.started":"2025-05-16T20:05:30.949736Z","shell.execute_reply":"2025-05-16T20:05:30.961879Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Evaluation function\ndef evaluate(model, data_loader, criterion, thresholds):\n    \"\"\"\n    Evaluate the model with focused metrics output.\n    \"\"\"\n    model.eval()\n    total_loss = 0\n    all_predictions = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            \n            probabilities = torch.sigmoid(outputs).cpu().numpy()\n            \n            # Apply thresholds per label\n            predictions = np.array([\n                (probabilities[:, i] >= thresholds[i]).astype(int) for i in range(len(thresholds))\n            ]).T\n\n            all_predictions.extend(predictions)\n            all_labels.extend(labels.cpu().numpy())\n    \n    all_predictions = np.array(all_predictions)\n    all_labels = np.array(all_labels)\n\n    # samples metrics\n    samples_precision = precision_score(all_labels, all_predictions, average='samples', zero_division=0)\n    samples_recall = recall_score(all_labels, all_predictions, average='samples', zero_division=0)\n    samples_f1 = f1_score(all_labels, all_predictions, average='samples', zero_division=0)\n\n    # f1 metrics\n    micro_f1 = f1_score(all_labels, all_predictions, average='micro', zero_division=0)\n    macro_f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n    weighted_f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n\n    # average loss\n    avg_loss = total_loss / len(data_loader)\n\n    metrics = {\n        'loss': avg_loss,\n        'micro_f1': micro_f1,\n        'macro_f1': macro_f1,\n        'weighted_f1': weighted_f1,\n        'samples_f1': samples_f1,\n        'samples_precision': samples_precision,\n        'samples_recall': samples_recall\n    }\n    \n    # output results\n    print(f\"Loss: {avg_loss:.4f} | Micro F1: {micro_f1:.4f} | Macro F1: {macro_f1:.4f} | Weighted F1: {weighted_f1:.4f} | Samples F1: {samples_f1:.4f}\")\n    print(f\"Samples Precision: {samples_precision:.4f} | Samples Recall: {samples_recall:.4f}\")\n    \n    # return metrics for further analysis\n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:05:30.963458Z","iopub.execute_input":"2025-05-16T20:05:30.963655Z","iopub.status.idle":"2025-05-16T20:05:30.988321Z","shell.execute_reply.started":"2025-05-16T20:05:30.963638Z","shell.execute_reply":"2025-05-16T20:05:30.987542Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"**2. Train model**","metadata":{}},{"cell_type":"code","source":"def train_model(batch_number, n_epochs, learning_rate, batch_size):\n    \"\"\"\n    Training loop for a single batch with dynamic threshold settings.\n    \"\"\"\n    X_train, X_test, y_train, y_test, pos_weights = get_data_and_weights(batch_number)\n    print(f\"\\nProcessing Batch {batch_number} ...\")\n\n    # Calculate dynamic thresholds\n    thresholds = set_thresholds(pos_weights)\n    \n    train_loader, test_loader = create_dataset_and_loader(X_train, y_train, X_test, y_test, batch_size)\n\n    model = LabelWiseAttentionClassifier(base_model=bert_model, num_labels=y_train.shape[1]).to(device)\n    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n    best_samples_f1 = 0.0\n\n    for epoch in range(n_epochs):\n        print(f\"Epoch {epoch + 1}/{n_epochs} | Batch {batch_number}\")\n\n        train_epoch(model, train_loader, optimizer, criterion)\n        metrics = evaluate(model, test_loader, criterion, thresholds)\n\n        current_samples_f1 = metrics['samples_f1']\n\n        # Save best model and thresholds\n        if current_samples_f1 > best_samples_f1:\n            best_samples_f1 = current_samples_f1\n\n    # ✅ Create writable directory\n            model_dir = \"models\"\n            os.makedirs(model_dir, exist_ok=True)\n\n    # ✅ Save model\n            model_path = f\"{model_dir}/best_model_batch_{batch_number}.pt\"\n            torch.save(model.state_dict(), model_path)\n            print(f\"✅ Saved model to {model_path}\")\n\n    # ✅ Save thresholds\n            thresholds_path = f\"{model_dir}/best_thresholds_batch_{batch_number}.json\"\n            with open(thresholds_path, \"w\") as f:\n                json.dump(thresholds, f)\n            print(f\"✅ Saved thresholds to {thresholds_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:05:30.989167Z","iopub.execute_input":"2025-05-16T20:05:30.989483Z","iopub.status.idle":"2025-05-16T20:05:31.006188Z","shell.execute_reply.started":"2025-05-16T20:05:30.989452Z","shell.execute_reply":"2025-05-16T20:05:31.005409Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":" # Save best model and thresholds\n #       if current_samples_f1 > best_samples_f1:\n#            best_samples_f1 = current_samples_f1\n#            model_path = f\"../src/model/best_model_batch_{batch_number}.pt\"\n#            torch.save(model.state_dict(), model_path)\n#            print(f\"  New best model saved to {model_path}\")\n\n            # Save thresholds\n#            thresholds_path = f\"../src/model/best_thresholds_batch_{batch_number}.json\"\n#            with open(thresholds_path, \"w\") as f:\n#                json.dump(thresholds, f)\n#            print(f\"  Thresholds saved to {thresholds_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:05:31.006959Z","iopub.execute_input":"2025-05-16T20:05:31.007267Z","iopub.status.idle":"2025-05-16T20:05:31.025212Z","shell.execute_reply.started":"2025-05-16T20:05:31.007239Z","shell.execute_reply":"2025-05-16T20:05:31.024420Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# run model for all batches\nn_epochs = 7\nlearning_rate = 2e-5\nbatch_size = 16\n\nfor batch_num in range(1, 11):\n    train_model(batch_number=batch_num, n_epochs=n_epochs, learning_rate=learning_rate, batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:05:31.025982Z","iopub.execute_input":"2025-05-16T20:05:31.026271Z","execution_failed":"2025-05-17T07:24:09.181Z"}},"outputs":[{"name":"stdout","text":"Batch 1 | Train: 4223, Test: 1057 | Non-auto: 3520, Auto: 1760\n\nProcessing Batch 1 ...\n\nDynamic Thresholds: ['0.60', '0.50', '0.78', '0.44', '0.74', '0.40', '0.77', '0.73', '0.20', '0.74', '0.80', '0.77', '0.75', '0.77', '0.80']\nEpoch 1/7 | Batch 1\nLoss: 0.4938 | Micro F1: 0.4131 | Macro F1: 0.4254 | Weighted F1: 0.6888 | Samples F1: 0.4714\nSamples Precision: 0.4297 | Samples Recall: 0.6334\n✅ Saved model to models/best_model_batch_1.pt\n✅ Saved thresholds to models/best_thresholds_batch_1.json\nEpoch 2/7 | Batch 1\nLoss: 0.3116 | Micro F1: 0.6479 | Macro F1: 0.5074 | Weighted F1: 0.7479 | Samples F1: 0.6182\nSamples Precision: 0.5910 | Samples Recall: 0.7020\n✅ Saved model to models/best_model_batch_1.pt\n✅ Saved thresholds to models/best_thresholds_batch_1.json\nEpoch 3/7 | Batch 1\nLoss: 0.3427 | Micro F1: 0.7046 | Macro F1: 0.5476 | Weighted F1: 0.8496 | Samples F1: 0.7500\nSamples Precision: 0.7192 | Samples Recall: 0.8425\n✅ Saved model to models/best_model_batch_1.pt\n✅ Saved thresholds to models/best_thresholds_batch_1.json\nEpoch 4/7 | Batch 1\nLoss: 0.2705 | Micro F1: 0.7762 | Macro F1: 0.5754 | Weighted F1: 0.8742 | Samples F1: 0.7985\nSamples Precision: 0.7695 | Samples Recall: 0.8761\n✅ Saved model to models/best_model_batch_1.pt\n✅ Saved thresholds to models/best_thresholds_batch_1.json\nEpoch 5/7 | Batch 1\nLoss: 0.2684 | Micro F1: 0.8565 | Macro F1: 0.6148 | Weighted F1: 0.9021 | Samples F1: 0.8683\nSamples Precision: 0.8458 | Samples Recall: 0.9167\n✅ Saved model to models/best_model_batch_1.pt\n✅ Saved thresholds to models/best_thresholds_batch_1.json\nEpoch 6/7 | Batch 1\nLoss: 0.3196 | Micro F1: 0.8197 | Macro F1: 0.5720 | Weighted F1: 0.8978 | Samples F1: 0.8531\nSamples Precision: 0.8297 | Samples Recall: 0.9139\nEpoch 7/7 | Batch 1\nLoss: 0.2894 | Micro F1: 0.8951 | Macro F1: 0.6405 | Weighted F1: 0.9283 | Samples F1: 0.9073\nSamples Precision: 0.8951 | Samples Recall: 0.9347\n✅ Saved model to models/best_model_batch_1.pt\n✅ Saved thresholds to models/best_thresholds_batch_1.json\nBatch 2 | Train: 4221, Test: 1059 | Non-auto: 3520, Auto: 1760\n\nProcessing Batch 2 ...\n\nDynamic Thresholds: ['0.60', '0.50', '0.78', '0.44', '0.74', '0.40', '0.77', '0.73', '0.20', '0.74', '0.80', '0.77', '0.75', '0.77', '0.80']\nEpoch 1/7 | Batch 2\nLoss: 0.1941 | Micro F1: 0.6969 | Macro F1: 0.5618 | Weighted F1: 0.8766 | Samples F1: 0.7453\nSamples Precision: 0.6940 | Samples Recall: 0.8758\n✅ Saved model to models/best_model_batch_2.pt\n✅ Saved thresholds to models/best_thresholds_batch_2.json\nEpoch 2/7 | Batch 2\nLoss: 0.1287 | Micro F1: 0.8913 | Macro F1: 0.6697 | Weighted F1: 0.9230 | Samples F1: 0.8999\nSamples Precision: 0.8889 | Samples Recall: 0.9263\n✅ Saved model to models/best_model_batch_2.pt\n✅ Saved thresholds to models/best_thresholds_batch_2.json\nEpoch 3/7 | Batch 2\nLoss: 0.1031 | Micro F1: 0.9113 | Macro F1: 0.7363 | Weighted F1: 0.9300 | Samples F1: 0.9177\nSamples Precision: 0.9096 | Samples Recall: 0.9381\n✅ Saved model to models/best_model_batch_2.pt\n✅ Saved thresholds to models/best_thresholds_batch_2.json\nEpoch 4/7 | Batch 2\nLoss: 0.0930 | Micro F1: 0.9154 | Macro F1: 0.7137 | Weighted F1: 0.9311 | Samples F1: 0.9198\nSamples Precision: 0.9138 | Samples Recall: 0.9363\n✅ Saved model to models/best_model_batch_2.pt\n✅ Saved thresholds to models/best_thresholds_batch_2.json\nEpoch 5/7 | Batch 2\nLoss: 0.0961 | Micro F1: 0.9309 | Macro F1: 0.7631 | Weighted F1: 0.9415 | Samples F1: 0.9336\nSamples Precision: 0.9299 | Samples Recall: 0.9452\n✅ Saved model to models/best_model_batch_2.pt\n✅ Saved thresholds to models/best_thresholds_batch_2.json\nEpoch 6/7 | Batch 2\nLoss: 0.0630 | Micro F1: 0.9381 | Macro F1: 0.8078 | Weighted F1: 0.9454 | Samples F1: 0.9386\nSamples Precision: 0.9360 | Samples Recall: 0.9476\n✅ Saved model to models/best_model_batch_2.pt\n✅ Saved thresholds to models/best_thresholds_batch_2.json\nEpoch 7/7 | Batch 2\nLoss: 0.0613 | Micro F1: 0.9351 | Macro F1: 0.8265 | Weighted F1: 0.9417 | Samples F1: 0.9330\nSamples Precision: 0.9301 | Samples Recall: 0.9419\nBatch 3 | Train: 4218, Test: 1062 | Non-auto: 3520, Auto: 1760\n\nProcessing Batch 3 ...\n\nDynamic Thresholds: ['0.60', '0.50', '0.78', '0.44', '0.74', '0.40', '0.77', '0.73', '0.20', '0.74', '0.80', '0.77', '0.75', '0.77', '0.80']\nEpoch 1/7 | Batch 3\nLoss: 0.1749 | Micro F1: 0.7753 | Macro F1: 0.6147 | Weighted F1: 0.9039 | Samples F1: 0.8314\nSamples Precision: 0.8074 | Samples Recall: 0.9035\n✅ Saved model to models/best_model_batch_3.pt\n✅ Saved thresholds to models/best_thresholds_batch_3.json\nEpoch 2/7 | Batch 3\nLoss: 0.1043 | Micro F1: 0.8759 | Macro F1: 0.7155 | Weighted F1: 0.8905 | Samples F1: 0.8671\nSamples Precision: 0.8635 | Samples Recall: 0.8790\n✅ Saved model to models/best_model_batch_3.pt\n✅ Saved thresholds to models/best_thresholds_batch_3.json\nEpoch 3/7 | Batch 3\nLoss: 0.0928 | Micro F1: 0.9063 | Macro F1: 0.7217 | Weighted F1: 0.9398 | Samples F1: 0.9255\nSamples Precision: 0.9195 | Samples Recall: 0.9459\n✅ Saved model to models/best_model_batch_3.pt\n✅ Saved thresholds to models/best_thresholds_batch_3.json\nEpoch 4/7 | Batch 3\nLoss: 0.0649 | Micro F1: 0.9305 | Macro F1: 0.8032 | Weighted F1: 0.9430 | Samples F1: 0.9323\nSamples Precision: 0.9273 | Samples Recall: 0.9440\n✅ Saved model to models/best_model_batch_3.pt\n✅ Saved thresholds to models/best_thresholds_batch_3.json\nEpoch 5/7 | Batch 3\nLoss: 0.0596 | Micro F1: 0.9392 | Macro F1: 0.8038 | Weighted F1: 0.9510 | Samples F1: 0.9456\nSamples Precision: 0.9397 | Samples Recall: 0.9590\n✅ Saved model to models/best_model_batch_3.pt\n✅ Saved thresholds to models/best_thresholds_batch_3.json\nEpoch 6/7 | Batch 3\nLoss: 0.0459 | Micro F1: 0.9676 | Macro F1: 0.9731 | Weighted F1: 0.9681 | Samples F1: 0.9672\nSamples Precision: 0.9674 | Samples Recall: 0.9685\n✅ Saved model to models/best_model_batch_3.pt\n✅ Saved thresholds to models/best_thresholds_batch_3.json\nEpoch 7/7 | Batch 3\nLoss: 0.0570 | Micro F1: 0.9452 | Macro F1: 0.8854 | Weighted F1: 0.9478 | Samples F1: 0.9447\nSamples Precision: 0.9433 | Samples Recall: 0.9487\nBatch 4 | Train: 4225, Test: 1055 | Non-auto: 3520, Auto: 1760\n\nProcessing Batch 4 ...\n\nDynamic Thresholds: ['0.60', '0.50', '0.78', '0.44', '0.74', '0.40', '0.77', '0.73', '0.20', '0.74', '0.80', '0.77', '0.75', '0.77', '0.80']\nEpoch 1/7 | Batch 4\nLoss: 0.1213 | Micro F1: 0.9368 | Macro F1: 0.7457 | Weighted F1: 0.9549 | Samples F1: 0.9486\nSamples Precision: 0.9473 | Samples Recall: 0.9564\n✅ Saved model to models/best_model_batch_4.pt\n✅ Saved thresholds to models/best_thresholds_batch_4.json\nEpoch 2/7 | Batch 4\nLoss: 0.0838 | Micro F1: 0.9425 | Macro F1: 0.7703 | Weighted F1: 0.9562 | Samples F1: 0.9501\nSamples Precision: 0.9488 | Samples Recall: 0.9564\n✅ Saved model to models/best_model_batch_4.pt\n✅ Saved thresholds to models/best_thresholds_batch_4.json\nEpoch 3/7 | Batch 4\nLoss: 0.0666 | Micro F1: 0.9537 | Macro F1: 0.8303 | Weighted F1: 0.9610 | Samples F1: 0.9566\nSamples Precision: 0.9561 | Samples Recall: 0.9607\n✅ Saved model to models/best_model_batch_4.pt\n✅ Saved thresholds to models/best_thresholds_batch_4.json\nEpoch 4/7 | Batch 4\nLoss: 0.0529 | Micro F1: 0.9606 | Macro F1: 0.8189 | Weighted F1: 0.9685 | Samples F1: 0.9704\nSamples Precision: 0.9695 | Samples Recall: 0.9749\n✅ Saved model to models/best_model_batch_4.pt\n✅ Saved thresholds to models/best_thresholds_batch_4.json\nEpoch 5/7 | Batch 4\nLoss: 0.0550 | Micro F1: 0.9470 | Macro F1: 0.8150 | Weighted F1: 0.9547 | Samples F1: 0.9543\nSamples Precision: 0.9532 | Samples Recall: 0.9588\nEpoch 6/7 | Batch 4\nLoss: 0.0565 | Micro F1: 0.9537 | Macro F1: 0.8358 | Weighted F1: 0.9587 | Samples F1: 0.9555\nSamples Precision: 0.9540 | Samples Recall: 0.9602\nEpoch 7/7 | Batch 4\nLoss: 0.0678 | Micro F1: 0.9505 | Macro F1: 0.8624 | Weighted F1: 0.9548 | Samples F1: 0.9534\nSamples Precision: 0.9523 | Samples Recall: 0.9578\nBatch 5 | Train: 4222, Test: 1058 | Non-auto: 3520, Auto: 1760\n\nProcessing Batch 5 ...\n\nDynamic Thresholds: ['0.60', '0.50', '0.78', '0.44', '0.74', '0.40', '0.77', '0.73', '0.20', '0.74', '0.80', '0.77', '0.75', '0.77', '0.80']\nEpoch 1/7 | Batch 5\nLoss: 0.1203 | Micro F1: 0.9342 | Macro F1: 0.7482 | Weighted F1: 0.9687 | Samples F1: 0.9542\nSamples Precision: 0.9465 | Samples Recall: 0.9750\n✅ Saved model to models/best_model_batch_5.pt\n✅ Saved thresholds to models/best_thresholds_batch_5.json\nEpoch 2/7 | Batch 5\nLoss: 0.0769 | Micro F1: 0.9558 | Macro F1: 0.7900 | Weighted F1: 0.9713 | Samples F1: 0.9647\nSamples Precision: 0.9613 | Samples Recall: 0.9750\n✅ Saved model to models/best_model_batch_5.pt\n✅ Saved thresholds to models/best_thresholds_batch_5.json\nEpoch 3/7 | Batch 5\nLoss: 0.0713 | Micro F1: 0.9513 | Macro F1: 0.8075 | Weighted F1: 0.9616 | Samples F1: 0.9571\nSamples Precision: 0.9546 | Samples Recall: 0.9646\nEpoch 4/7 | Batch 5\nLoss: 0.0482 | Micro F1: 0.9799 | Macro F1: 0.9141 | Weighted F1: 0.9819 | Samples F1: 0.9808\nSamples Precision: 0.9801 | Samples Recall: 0.9835\n✅ Saved model to models/best_model_batch_5.pt\n✅ Saved thresholds to models/best_thresholds_batch_5.json\nEpoch 5/7 | Batch 5\nLoss: 0.0630 | Micro F1: 0.9464 | Macro F1: 0.8507 | Weighted F1: 0.9517 | Samples F1: 0.9496\nSamples Precision: 0.9479 | Samples Recall: 0.9551\nEpoch 6/7 | Batch 5\nLoss: 0.0519 | Micro F1: 0.9640 | Macro F1: 0.9146 | Weighted F1: 0.9660 | Samples F1: 0.9649\nSamples Precision: 0.9639 | Samples Recall: 0.9683\nEpoch 7/7 | Batch 5\nLoss: 0.0349 | Micro F1: 0.9692 | Macro F1: 0.8947 | Weighted F1: 0.9726 | Samples F1: 0.9721\nSamples Precision: 0.9711 | Samples Recall: 0.9754\nBatch 6 | Train: 4225, Test: 1055 | Non-auto: 3520, Auto: 1760\n\nProcessing Batch 6 ...\n\nDynamic Thresholds: ['0.60', '0.50', '0.78', '0.44', '0.74', '0.40', '0.77', '0.73', '0.20', '0.74', '0.80', '0.77', '0.75', '0.77', '0.80']\nEpoch 1/7 | Batch 6\nLoss: 0.1207 | Micro F1: 0.9209 | Macro F1: 0.7511 | Weighted F1: 0.9649 | Samples F1: 0.9434\nSamples Precision: 0.9314 | Samples Recall: 0.9709\n✅ Saved model to models/best_model_batch_6.pt\n✅ Saved thresholds to models/best_thresholds_batch_6.json\nEpoch 2/7 | Batch 6\nLoss: 0.0817 | Micro F1: 0.9455 | Macro F1: 0.7970 | Weighted F1: 0.9553 | Samples F1: 0.9520\nSamples Precision: 0.9511 | Samples Recall: 0.9572\n✅ Saved model to models/best_model_batch_6.pt\n✅ Saved thresholds to models/best_thresholds_batch_6.json\nEpoch 3/7 | Batch 6\nLoss: 0.0719 | Micro F1: 0.9527 | Macro F1: 0.7857 | Weighted F1: 0.9637 | Samples F1: 0.9620\nSamples Precision: 0.9607 | Samples Recall: 0.9684\n✅ Saved model to models/best_model_batch_6.pt\n✅ Saved thresholds to models/best_thresholds_batch_6.json\nEpoch 4/7 | Batch 6\nLoss: 0.0547 | Micro F1: 0.9607 | Macro F1: 0.8778 | Weighted F1: 0.9659 | Samples F1: 0.9640\nSamples Precision: 0.9634 | Samples Recall: 0.9671\n✅ Saved model to models/best_model_batch_6.pt\n✅ Saved thresholds to models/best_thresholds_batch_6.json\nEpoch 5/7 | Batch 6\nLoss: 0.0701 | Micro F1: 0.9715 | Macro F1: 0.8686 | Weighted F1: 0.9760 | Samples F1: 0.9751\nSamples Precision: 0.9747 | Samples Recall: 0.9776\n✅ Saved model to models/best_model_batch_6.pt\n✅ Saved thresholds to models/best_thresholds_batch_6.json\nEpoch 6/7 | Batch 6\nLoss: 0.0501 | Micro F1: 0.9812 | Macro F1: 0.9364 | Weighted F1: 0.9821 | Samples F1: 0.9836\nSamples Precision: 0.9845 | Samples Recall: 0.9837\n✅ Saved model to models/best_model_batch_6.pt\n✅ Saved thresholds to models/best_thresholds_batch_6.json\nEpoch 7/7 | Batch 6\nLoss: 0.0745 | Micro F1: 0.9709 | Macro F1: 0.8831 | Weighted F1: 0.9733 | Samples F1: 0.9720\nSamples Precision: 0.9721 | Samples Recall: 0.9736\nBatch 7 | Train: 4230, Test: 1050 | Non-auto: 3520, Auto: 1760\n\nProcessing Batch 7 ...\n\nDynamic Thresholds: ['0.60', '0.50', '0.78', '0.44', '0.74', '0.40', '0.77', '0.73', '0.20', '0.74', '0.80', '0.77', '0.75', '0.77', '0.80']\nEpoch 1/7 | Batch 7\nLoss: 0.1107 | Micro F1: 0.9350 | Macro F1: 0.7995 | Weighted F1: 0.9658 | Samples F1: 0.9496\nSamples Precision: 0.9415 | Samples Recall: 0.9690\n✅ Saved model to models/best_model_batch_7.pt\n✅ Saved thresholds to models/best_thresholds_batch_7.json\nEpoch 2/7 | Batch 7\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import torch\nimport json\n\n# Load the model\nmodel_path = \"models/best_model_batch_1.pt\"\nmodel = LabelWiseAttentionClassifier(model_name=\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\", num_labels=15)\nmodel.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\nmodel.eval().to(device)\n\n# Load thresholds\nwith open(\"models/best_thresholds_batch_1.json\") as f:\n    thresholds = json.load(f)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n\n# Use your real label column names\nlabel_columns = ['non-autoregulatory', 'autophosphorylation', 'autocatalytic', 'autoregulation', 'autoubiquitination',\n                 'autoinhibition', 'autoregulatory', 'autoinducer', 'autolysis', 'autoinhibitory',\n                 'autoactivation', 'autocatalysis', 'autofeedback', 'autoinduction', 'autokinase']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_abstract(text, model, tokenizer, thresholds, label_columns):\n    model.eval()\n    encoding = tokenizer(\n        text,\n        add_special_tokens=True,\n        truncation=True,\n        max_length=512,\n        padding='max_length',\n        return_tensors='pt'\n    )\n\n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n\n    with torch.no_grad():\n        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n        probs = torch.sigmoid(logits).cpu().numpy()[0]\n        preds = (probs >= np.array(thresholds)).astype(int)\n\n    predicted_labels = [label_columns[i] for i, p in enumerate(preds) if p == 1]\n    return predicted_labels, probs\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_text = \"We found that the autoregulatory mechanism of this kinase is governed by autophosphorylation of a specific serine site.\"\nlabels, scores = predict_abstract(sample_text, model, tokenizer, thresholds, label_columns)\n\nprint(\"Predicted Labels:\", labels)\nprint(\"All Probabilities:\", scores)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}