{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f46fcac3",
   "metadata": {},
   "source": [
    "# Bio Bert Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19186769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import os, random, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "617f1134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49a7a706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Device setup\n",
    "device = (\n",
    "    torch.device(\"mps\")\n",
    "    if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cuda\")\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2812e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary directories for saving models and results\n",
    "import os\n",
    "os.makedirs(\"model/batch_1\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485aa656",
   "metadata": {},
   "source": [
    "## 1. Read in Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ef715f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53130, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AC</th>\n",
       "      <th>PMID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Terms</th>\n",
       "      <th>Text_combined</th>\n",
       "      <th>batch_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q8NML3</td>\n",
       "      <td>17183211</td>\n",
       "      <td>RamA, the transcriptional regulator of acetate...</td>\n",
       "      <td>The RamA protein represents a LuxR-type transc...</td>\n",
       "      <td>autoregulation</td>\n",
       "      <td>RamA, the transcriptional regulator of acetate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q9SCZ4</td>\n",
       "      <td>17673660</td>\n",
       "      <td>The FERONIA receptor-like kinase mediates male...</td>\n",
       "      <td>In flowering plants, signaling between the mal...</td>\n",
       "      <td>autophosphorylation</td>\n",
       "      <td>The FERONIA receptor-like kinase mediates male...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q81WX1</td>\n",
       "      <td>12721629</td>\n",
       "      <td>The genome sequence of Bacillus anthracis Ames...</td>\n",
       "      <td>Bacillus anthracis is an endospore-forming bac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The genome sequence of Bacillus anthracis Ames...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P14410</td>\n",
       "      <td>8521865</td>\n",
       "      <td>Phosphorylation of the N-terminal intracellula...</td>\n",
       "      <td>This paper reports the phosphorylation of the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Phosphorylation of the N-terminal intracellula...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P36898</td>\n",
       "      <td>14523231</td>\n",
       "      <td>Mutations in bone morphogenetic protein recept...</td>\n",
       "      <td>Brachydactyly (BD) type A2 is an autosomal dom...</td>\n",
       "      <td>autophosphorylation</td>\n",
       "      <td>Mutations in bone morphogenetic protein recept...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AC      PMID                                              Title  \\\n",
       "0  Q8NML3  17183211  RamA, the transcriptional regulator of acetate...   \n",
       "1  Q9SCZ4  17673660  The FERONIA receptor-like kinase mediates male...   \n",
       "2  Q81WX1  12721629  The genome sequence of Bacillus anthracis Ames...   \n",
       "3  P14410   8521865  Phosphorylation of the N-terminal intracellula...   \n",
       "4  P36898  14523231  Mutations in bone morphogenetic protein recept...   \n",
       "\n",
       "                                            Abstract                Terms  \\\n",
       "0  The RamA protein represents a LuxR-type transc...       autoregulation   \n",
       "1  In flowering plants, signaling between the mal...  autophosphorylation   \n",
       "2  Bacillus anthracis is an endospore-forming bac...                  NaN   \n",
       "3  This paper reports the phosphorylation of the ...                  NaN   \n",
       "4  Brachydactyly (BD) type A2 is an autosomal dom...  autophosphorylation   \n",
       "\n",
       "                                       Text_combined  batch_number  \n",
       "0  RamA, the transcriptional regulator of acetate...             1  \n",
       "1  The FERONIA receptor-like kinase mediates male...             1  \n",
       "2  The genome sequence of Bacillus anthracis Ames...             1  \n",
       "3  Phosphorylation of the N-terminal intracellula...             1  \n",
       "4  Mutations in bone morphogenetic protein recept...             1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../data/processed/shuffled_10_data.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea4f48e",
   "metadata": {},
   "source": [
    "## 2. Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c11d8441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Clean text\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    # Handle NaN values\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    # Keep hyphens as they may be important in biomedical terms\n",
    "    text = re.sub(r'[^\\w\\s-]', '', text)\n",
    "    # Keep numbers that might be part of important terms\n",
    "    # text = re.sub(r'\\d+', '', text)\n",
    "    text = \" \".join([word.strip() for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "df['Text_Cleaned'] = df['Text_combined'].apply(clean_text)\n",
    "\n",
    "# Fill nan with 'non-autoregulatory'\n",
    "df['Terms'] = df['Terms'].fillna('non-autoregulatory')\n",
    "\n",
    "# Keep only selected columns\n",
    "columns_to_keep = ['batch_number', 'Text_Cleaned', 'Terms']\n",
    "df_cleaned = df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ac85503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53130, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_number</th>\n",
       "      <th>Text_Cleaned</th>\n",
       "      <th>Terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>rama transcriptional regulator acetate metabol...</td>\n",
       "      <td>autoregulation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>feronia receptor-like kinase mediates male-fem...</td>\n",
       "      <td>autophosphorylation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>genome sequence bacillus anthracis ames compar...</td>\n",
       "      <td>non-autoregulatory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>phosphorylation n-terminal intracellular tail ...</td>\n",
       "      <td>non-autoregulatory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mutations bone morphogenetic protein receptor ...</td>\n",
       "      <td>autophosphorylation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_number                                       Text_Cleaned  \\\n",
       "0             1  rama transcriptional regulator acetate metabol...   \n",
       "1             1  feronia receptor-like kinase mediates male-fem...   \n",
       "2             1  genome sequence bacillus anthracis ames compar...   \n",
       "3             1  phosphorylation n-terminal intracellular tail ...   \n",
       "4             1  mutations bone morphogenetic protein receptor ...   \n",
       "\n",
       "                 Terms  \n",
       "0       autoregulation  \n",
       "1  autophosphorylation  \n",
       "2   non-autoregulatory  \n",
       "3   non-autoregulatory  \n",
       "4  autophosphorylation  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_cleaned.shape)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0e3083",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7e07168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n9/9tsj9gw15ll9ycvpvm3m3pq80000gn/T/ipykernel_6298/649158024.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['Terms_List'] = df_cleaned['Terms'].apply(\n",
      "/var/folders/n9/9tsj9gw15ll9ycvpvm3m3pq80000gn/T/ipykernel_6298/649158024.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['Terms_List'] = df_cleaned['Terms_List'].apply(lambda x: list(set(x)))\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Prepare labels\n",
    "# Convert terms to list\n",
    "df_cleaned['Terms_List'] = df_cleaned['Terms'].apply(\n",
    "    lambda x: [term.strip() for term in x.split(',')]\n",
    ")\n",
    "df_cleaned['Terms_List'] = df_cleaned['Terms_List'].apply(lambda x: list(set(x)))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(df_cleaned['Terms_List'])\n",
    "label_columns = mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fa499d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_df = pd.DataFrame(labels, columns=label_columns)\n",
    "existing_columns = [col for col in label_columns if col in df_cleaned.columns]\n",
    "df_cleaned = df_cleaned.drop(columns=existing_columns, errors='ignore')\n",
    "df_cleaned = pd.concat([df_cleaned, labels_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de3ced07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53130, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_number</th>\n",
       "      <th>Text_Cleaned</th>\n",
       "      <th>Terms</th>\n",
       "      <th>Terms_List</th>\n",
       "      <th>autoactivation</th>\n",
       "      <th>autocatalysis</th>\n",
       "      <th>autocatalytic</th>\n",
       "      <th>autofeedback</th>\n",
       "      <th>autoinducer</th>\n",
       "      <th>autoinduction</th>\n",
       "      <th>autoinhibition</th>\n",
       "      <th>autoinhibitory</th>\n",
       "      <th>autokinase</th>\n",
       "      <th>autolysis</th>\n",
       "      <th>autophosphorylation</th>\n",
       "      <th>autoregulation</th>\n",
       "      <th>autoregulatory</th>\n",
       "      <th>autoubiquitination</th>\n",
       "      <th>non-autoregulatory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>rama transcriptional regulator acetate metabol...</td>\n",
       "      <td>autoregulation</td>\n",
       "      <td>[autoregulation]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>feronia receptor-like kinase mediates male-fem...</td>\n",
       "      <td>autophosphorylation</td>\n",
       "      <td>[autophosphorylation]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>genome sequence bacillus anthracis ames compar...</td>\n",
       "      <td>non-autoregulatory</td>\n",
       "      <td>[non-autoregulatory]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>phosphorylation n-terminal intracellular tail ...</td>\n",
       "      <td>non-autoregulatory</td>\n",
       "      <td>[non-autoregulatory]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mutations bone morphogenetic protein receptor ...</td>\n",
       "      <td>autophosphorylation</td>\n",
       "      <td>[autophosphorylation]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_number                                       Text_Cleaned  \\\n",
       "0             1  rama transcriptional regulator acetate metabol...   \n",
       "1             1  feronia receptor-like kinase mediates male-fem...   \n",
       "2             1  genome sequence bacillus anthracis ames compar...   \n",
       "3             1  phosphorylation n-terminal intracellular tail ...   \n",
       "4             1  mutations bone morphogenetic protein receptor ...   \n",
       "\n",
       "                 Terms             Terms_List  autoactivation  autocatalysis  \\\n",
       "0       autoregulation       [autoregulation]               0              0   \n",
       "1  autophosphorylation  [autophosphorylation]               0              0   \n",
       "2   non-autoregulatory   [non-autoregulatory]               0              0   \n",
       "3   non-autoregulatory   [non-autoregulatory]               0              0   \n",
       "4  autophosphorylation  [autophosphorylation]               0              0   \n",
       "\n",
       "   autocatalytic  autofeedback  autoinducer  autoinduction  autoinhibition  \\\n",
       "0              0             0            0              0               0   \n",
       "1              0             0            0              0               0   \n",
       "2              0             0            0              0               0   \n",
       "3              0             0            0              0               0   \n",
       "4              0             0            0              0               0   \n",
       "\n",
       "   autoinhibitory  autokinase  autolysis  autophosphorylation  autoregulation  \\\n",
       "0               0           0          0                    0               1   \n",
       "1               0           0          0                    1               0   \n",
       "2               0           0          0                    0               0   \n",
       "3               0           0          0                    0               0   \n",
       "4               0           0          0                    1               0   \n",
       "\n",
       "   autoregulatory  autoubiquitination  non-autoregulatory  \n",
       "0               0                   0                   0  \n",
       "1               0                   0                   0  \n",
       "2               0                   0                   1  \n",
       "3               0                   0                   1  \n",
       "4               0                   0                   0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_cleaned.shape)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc5a6b4",
   "metadata": {},
   "source": [
    "## 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6db7b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Instantiate the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80ebbd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Splitting\n",
    "def split_single_batch_data(batch_number, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data from a single batch into train and test sets using stratified sampling.\n",
    "    \"\"\"\n",
    "    batch_df = df_cleaned[df_cleaned['batch_number'] == batch_number].copy()\n",
    "    \n",
    "    if len(batch_df) == 0:\n",
    "        print(f\"Warning: No data found for batch {batch_number}\")\n",
    "        return None, None, None, None\n",
    "        \n",
    "    X = batch_df['Text_Cleaned']\n",
    "    y = labels_df.loc[batch_df.index].values  # Ensure indexing alignment\n",
    "    \n",
    "    # Calculate label distribution\n",
    "    non_auto_count = len(batch_df[batch_df['Terms'] == 'non-autoregulatory'])\n",
    "    auto_count = len(batch_df[batch_df['Terms'] != 'non-autoregulatory'])\n",
    "    \n",
    "    try:\n",
    "        # Split data\n",
    "        msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "        \n",
    "        for train_idx, test_idx in msss.split(X, y):\n",
    "            X_train = X.iloc[train_idx]\n",
    "            X_test = X.iloc[test_idx]\n",
    "            y_train = y[train_idx]\n",
    "            y_test = y[test_idx]\n",
    "        \n",
    "        print(f\"Batch {batch_number} | Train: {len(X_train)}, Test: {len(X_test)} | Non-auto: {non_auto_count}, Auto: {auto_count}\")\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    except Exception as e:\n",
    "        print(f\"Error in splitting batch {batch_number}: {e}\")\n",
    "        # Fallback to regular train_test_split if stratified splitting fails\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "        print(f\"Fallback split for batch {batch_number} | Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee507758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "def get_data_and_weights(batch_number):\n",
    "    \"\"\"\n",
    "    Get data and calculate class weights for a specific batch.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = split_single_batch_data(batch_number)\n",
    "    \n",
    "    if X_train is None:\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    # Calculate class weights\n",
    "    pos_weights = []\n",
    "    for i in range(y_train.shape[1]):\n",
    "        neg_count = len(y_train) - np.sum(y_train[:, i])\n",
    "        pos_count = np.sum(y_train[:, i])\n",
    "        # Handle the case where a class might have no positive examples\n",
    "        if pos_count == 0:\n",
    "            pos_weights.append(1.0)  # Default weight\n",
    "        else:\n",
    "            # Calculate weight and clip to prevent extreme values\n",
    "            weight = neg_count / pos_count\n",
    "            weight = min(max(weight, 0.1), 10.0)  # Clip between 0.1 and 10\n",
    "            pos_weights.append(weight)\n",
    "    \n",
    "    pos_weights = torch.FloatTensor(pos_weights).to(device)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, pos_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8d8eab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Dataset and DataLoader classes\n",
    "# Create dataset class\n",
    "class BioBERTDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.FloatTensor(label)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05fd9cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader\n",
    "def create_dataset_and_loader(X_train, y_train, X_test, y_test, batch_size):\n",
    "    train_dataset = BioBERTDataset(X_train, y_train, tokenizer)\n",
    "    test_dataset = BioBERTDataset(X_test, y_test, tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e29c9d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Model Architecture\n",
    "# Improved BioBERT Classifier with intermediate layers\n",
    "class ImprovedBioBERTClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, dropout1=0.1, dropout2=0.2):\n",
    "        super(ImprovedBioBERTClassifier, self).__init__()\n",
    "        # Create a fresh copy of the model\n",
    "        self.bert = AutoModel.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "        \n",
    "        # Add an intermediate layer\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.intermediate = nn.Linear(self.bert.config.hidden_size, 512)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        self.classifier = nn.Linear(512, n_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.xavier_normal_(self.intermediate.weight)\n",
    "        nn.init.zeros_(self.intermediate.bias)\n",
    "        nn.init.xavier_normal_(self.classifier.weight)\n",
    "        nn.init.zeros_(self.classifier.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        pooled_output = self.dropout1(pooled_output)\n",
    "        intermediate = self.intermediate(pooled_output)\n",
    "        intermediate = self.activation(intermediate)\n",
    "        intermediate = self.dropout2(intermediate)\n",
    "        logits = self.classifier(intermediate)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "331fc11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Training and Evaluation Functions\n",
    "# Training function with improvements\n",
    "def train_epoch(model, data_loader, optimizer, criterion, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        try:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Check for NaN loss\n",
    "            if torch.isnan(loss):\n",
    "                print(\"WARNING: NaN loss detected, skipping batch\")\n",
    "                continue\n",
    "                \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_batches += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch processing: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Protect against division by zero\n",
    "    if total_batches == 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    return total_loss / total_batches\n",
    "\n",
    "# Optimize thresholds for each label\n",
    "def optimize_thresholds(model, val_loader, n_labels):\n",
    "    model.eval()\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Optimizing thresholds\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels']\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "            \n",
    "            all_outputs.append(probs)\n",
    "            all_labels.append(labels.numpy())\n",
    "    \n",
    "    all_outputs = np.vstack(all_outputs)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "    optimal_thresholds = []\n",
    "    \n",
    "    for i in range(n_labels):\n",
    "        best_f1 = 0\n",
    "        best_threshold = 0.5\n",
    "        \n",
    "        for threshold in np.arange(0.3, 0.7, 0.05):\n",
    "            preds = (all_outputs[:, i] >= threshold).astype(int)\n",
    "            f1 = f1_score(all_labels[:, i], preds, zero_division=0)\n",
    "            \n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "        \n",
    "        optimal_thresholds.append(best_threshold)\n",
    "        \n",
    "    return optimal_thresholds\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, data_loader, criterion, thresholds):\n",
    "    \"\"\"\n",
    "    Evaluate the model with focused metrics output.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probabilities = torch.sigmoid(outputs).cpu().numpy()\n",
    "            \n",
    "            # Apply thresholds per label\n",
    "            predictions = np.array([\n",
    "                (probabilities[:, i] >= thresholds[i]).astype(int) for i in range(len(thresholds))\n",
    "            ]).T\n",
    "\n",
    "            all_predictions.extend(predictions)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Samples metrics\n",
    "    samples_precision = precision_score(all_labels, all_predictions, average='samples', zero_division=0)\n",
    "    samples_recall = recall_score(all_labels, all_predictions, average='samples', zero_division=0)\n",
    "    samples_f1 = f1_score(all_labels, all_predictions, average='samples', zero_division=0)\n",
    "\n",
    "    # F1 metrics\n",
    "    micro_f1 = f1_score(all_labels, all_predictions, average='micro', zero_division=0)\n",
    "    macro_f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "    weighted_f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "\n",
    "    # Average loss\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    metrics = {\n",
    "        'loss': avg_loss,\n",
    "        'micro_f1': micro_f1,\n",
    "        'macro_f1': macro_f1,\n",
    "        'weighted_f1': weighted_f1,\n",
    "        'samples_f1': samples_f1,\n",
    "        'samples_precision': samples_precision,\n",
    "        'samples_recall': samples_recall\n",
    "    }\n",
    "    \n",
    "    # Output results\n",
    "    print(f\"Loss: {avg_loss:.4f} | Micro F1: {micro_f1:.4f} | Macro F1: {macro_f1:.4f} | Weighted F1: {weighted_f1:.4f} | Samples F1: {samples_f1:.4f}\")\n",
    "    print(f\"Samples Precision: {samples_precision:.4f} | Samples Recall: {samples_recall:.4f}\")\n",
    "    \n",
    "    # Return metrics for further analysis\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aad9763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this near the top of your code to create necessary directories\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a479e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Main Training Function\n",
    "def train_model(batch_number, n_epochs, learning_rate, batch_size, use_optimal_thresholds=True):\n",
    "    \"\"\"\n",
    "    Training loop for a single batch with dynamic threshold settings.\n",
    "    \"\"\"\n",
    "    # Get data\n",
    "    data_result = get_data_and_weights(batch_number)\n",
    "    \n",
    "    if data_result[0] is None:\n",
    "        print(f\"Skipping batch {batch_number} due to data issues.\")\n",
    "        return\n",
    "        \n",
    "    X_train, X_test, y_train, y_test, pos_weights = data_result\n",
    "    print(f\"\\nProcessing Batch {batch_number} ...\")\n",
    "    \n",
    "    # Set initial thresholds\n",
    "    initial_thresholds = [0.5] * y_train.shape[1]\n",
    "    print(f\"\\nInitial Thresholds: {[f'{t:.2f}' for t in initial_thresholds]}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader, test_loader = create_dataset_and_loader(X_train, y_train, X_test, y_test, batch_size)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ImprovedBioBERTClassifier(n_classes=y_train.shape[1]).to(device)\n",
    "    \n",
    "    # Set up loss and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    # Set up scheduler with warmup\n",
    "    total_steps = len(train_loader) * n_epochs\n",
    "    warmup_steps = int(total_steps * 0.1)  # 10% warmup\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_samples_f1 = 0.0\n",
    "    \n",
    "    # Dictionary to track all metrics\n",
    "    all_metrics = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'micro_f1': [],\n",
    "        'macro_f1': [],\n",
    "        'weighted_f1': [],\n",
    "        'samples_f1': [],\n",
    "        'samples_precision': [],\n",
    "        'samples_recall': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs} | Batch {batch_number}\")\n",
    "        \n",
    "        # Train\n",
    "        epoch_loss = train_epoch(model, train_loader, optimizer, criterion, scheduler)\n",
    "        print(f\"Training Loss: {epoch_loss:.4f}\")\n",
    "        all_metrics['train_loss'].append(epoch_loss)\n",
    "        \n",
    "        # Optimize thresholds if requested (only after first epoch)\n",
    "        thresholds = initial_thresholds\n",
    "        if use_optimal_thresholds and epoch >= 1:\n",
    "            try:\n",
    "                print(\"Optimizing thresholds...\")\n",
    "                thresholds = optimize_thresholds(model, test_loader, len(initial_thresholds))\n",
    "                print(f\"Optimized Thresholds: {[f'{t:.2f}' for t in thresholds]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error optimizing thresholds: {e}\")\n",
    "                print(\"Using default thresholds\")\n",
    "                thresholds = initial_thresholds\n",
    "        \n",
    "        # Evaluate\n",
    "        metrics = evaluate(model, test_loader, criterion, thresholds)\n",
    "        \n",
    "        # Store metrics\n",
    "        for k, v in metrics.items():\n",
    "            if k in all_metrics:\n",
    "                all_metrics[k].append(v)\n",
    "        \n",
    "        current_samples_f1 = metrics['samples_f1']\n",
    "        \n",
    "        # Save best model and thresholds\n",
    "        if current_samples_f1 > best_samples_f1:\n",
    "            best_samples_f1 = current_samples_f1\n",
    "            \n",
    "            # Create model directory if it doesn't exist\n",
    "            model_dir = f\"model/batch_{batch_number}\"\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            \n",
    "            # Save model\n",
    "            model_path = f\"{model_dir}/best_model.pt\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"  New best model saved to {model_path}\")\n",
    "            \n",
    "            # Save thresholds\n",
    "            thresholds_path = f\"{model_dir}/best_thresholds.json\"\n",
    "            with open(thresholds_path, \"w\") as f:\n",
    "                json.dump(thresholds, f)\n",
    "            print(f\"  Thresholds saved to {thresholds_path}\")\n",
    "    \n",
    "    # Save metrics history\n",
    "    metrics_path = f\"results/metrics_batch_{batch_number}.json\"\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(all_metrics, f)\n",
    "    print(f\"Metrics saved to {metrics_path}\")\n",
    "    \n",
    "    # Save label columns\n",
    "    with open('label_columns.json', 'w') as f:\n",
    "        json.dump(list(label_columns), f)\n",
    "    \n",
    "    return model, best_samples_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7418e7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Inference Class\n",
    "class BioBERTInference:\n",
    "    def __init__(self, model_path, thresholds_path, label_columns_path):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Load label columns\n",
    "        with open(label_columns_path, 'r') as f:\n",
    "            self.label_columns = json.load(f)\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = ImprovedBioBERTClassifier(n_classes=len(self.label_columns))\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Load thresholds\n",
    "        with open(thresholds_path, 'r') as f:\n",
    "            self.thresholds = json.load(f)\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        return clean_text(text)\n",
    "    \n",
    "    def predict(self, text):\n",
    "        processed_text = self.preprocess(text)\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            processed_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].to(self.device)\n",
    "        attention_mask = encoding['attention_mask'].to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, attention_mask)\n",
    "            probabilities = torch.sigmoid(outputs).cpu().numpy()[0]\n",
    "        \n",
    "        # Apply thresholds and get predicted labels\n",
    "        predictions = {}\n",
    "        for i, label in enumerate(self.label_columns):\n",
    "            if probabilities[i] >= self.thresholds[i]:\n",
    "                predictions[label] = float(probabilities[i])\n",
    "        \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60c4c1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 | Train: 4248, Test: 1065 | Non-auto: 3542, Auto: 1771\n",
      "\n",
      "Processing Batch 1 ...\n",
      "\n",
      "Initial Thresholds: ['0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50']\n",
      "Epoch 1/7 | Batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 531/531 [16:02<00:00,  1.81s/it, loss=0.2179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 134/134 [01:27<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2737 | Micro F1: 0.7476 | Macro F1: 0.2364 | Weighted F1: 0.7354 | Samples F1: 0.7659\n",
      "Samples Precision: 0.7523 | Samples Recall: 0.7972\n",
      "  New best model saved to model/batch_1/best_model.pt\n",
      "  Thresholds saved to model/batch_1/best_thresholds.json\n",
      "Epoch 2/7 | Batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 531/531 [17:17<00:00,  1.95s/it, loss=0.1089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1809\n",
      "Optimizing thresholds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing thresholds: 100%|██████████| 134/134 [01:25<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Thresholds: ['0.35', '0.50', '0.50', '0.30', '0.50', '0.50', '0.65', '0.45', '0.50', '0.65', '0.65', '0.65', '0.65', '0.65', '0.35']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 134/134 [01:26<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0974 | Micro F1: 0.9067 | Macro F1: 0.6415 | Weighted F1: 0.9112 | Samples F1: 0.9169\n",
      "Samples Precision: 0.9036 | Samples Recall: 0.9441\n",
      "  New best model saved to model/batch_1/best_model.pt\n",
      "  Thresholds saved to model/batch_1/best_thresholds.json\n",
      "Epoch 3/7 | Batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 531/531 [17:31<00:00,  1.98s/it, loss=0.0249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1021\n",
      "Optimizing thresholds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing thresholds: 100%|██████████| 134/134 [01:31<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Thresholds: ['0.30', '0.50', '0.45', '0.30', '0.30', '0.35', '0.65', '0.65', '0.50', '0.55', '0.50', '0.65', '0.65', '0.65', '0.60']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 134/134 [01:31<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0835 | Micro F1: 0.9369 | Macro F1: 0.7883 | Weighted F1: 0.9362 | Samples F1: 0.9363\n",
      "Samples Precision: 0.9338 | Samples Recall: 0.9423\n",
      "  New best model saved to model/batch_1/best_model.pt\n",
      "  Thresholds saved to model/batch_1/best_thresholds.json\n",
      "Epoch 4/7 | Batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 531/531 [17:07<00:00,  1.93s/it, loss=0.1045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0797\n",
      "Optimizing thresholds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing thresholds: 100%|██████████| 134/134 [01:23<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Thresholds: ['0.30', '0.50', '0.45', '0.30', '0.30', '0.30', '0.65', '0.35', '0.30', '0.50', '0.65', '0.50', '0.35', '0.45', '0.55']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 134/134 [01:24<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0752 | Micro F1: 0.9382 | Macro F1: 0.8407 | Weighted F1: 0.9382 | Samples F1: 0.9391\n",
      "Samples Precision: 0.9383 | Samples Recall: 0.9432\n",
      "  New best model saved to model/batch_1/best_model.pt\n",
      "  Thresholds saved to model/batch_1/best_thresholds.json\n",
      "Epoch 5/7 | Batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 531/531 [17:25<00:00,  1.97s/it, loss=0.2058]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0700\n",
      "Optimizing thresholds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing thresholds: 100%|██████████| 134/134 [01:24<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Thresholds: ['0.30', '0.50', '0.60', '0.30', '0.30', '0.30', '0.50', '0.40', '0.30', '0.45', '0.30', '0.30', '0.30', '0.60', '0.30']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 134/134 [01:37<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0816 | Micro F1: 0.9456 | Macro F1: 0.8682 | Weighted F1: 0.9444 | Samples F1: 0.9484\n",
      "Samples Precision: 0.9441 | Samples Recall: 0.9582\n",
      "  New best model saved to model/batch_1/best_model.pt\n",
      "  Thresholds saved to model/batch_1/best_thresholds.json\n",
      "Epoch 6/7 | Batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 531/531 [17:18<00:00,  1.96s/it, loss=0.4400]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0576\n",
      "Optimizing thresholds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing thresholds: 100%|██████████| 134/134 [01:24<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Thresholds: ['0.30', '0.50', '0.35', '0.30', '0.30', '0.30', '0.45', '0.35', '0.30', '0.65', '0.65', '0.35', '0.30', '0.30', '0.40']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 134/134 [01:29<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0752 | Micro F1: 0.9435 | Macro F1: 0.8668 | Weighted F1: 0.9434 | Samples F1: 0.9460\n",
      "Samples Precision: 0.9440 | Samples Recall: 0.9516\n",
      "Epoch 7/7 | Batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 531/531 [16:40<00:00,  1.88s/it, loss=0.0121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0552\n",
      "Optimizing thresholds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing thresholds: 100%|██████████| 134/134 [01:23<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Thresholds: ['0.30', '0.50', '0.60', '0.30', '0.30', '0.30', '0.40', '0.40', '0.30', '0.45', '0.30', '0.35', '0.30', '0.30', '0.30']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 134/134 [01:26<00:00,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0740 | Micro F1: 0.9438 | Macro F1: 0.8707 | Weighted F1: 0.9437 | Samples F1: 0.9474\n",
      "Samples Precision: 0.9437 | Samples Recall: 0.9563\n",
      "Metrics saved to results/metrics_batch_1.json\n",
      "Batch 1 training complete. Best F1 score: 0.9484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Run Training\n",
    "# Change these parameters as needed\n",
    "n_epochs = 7\n",
    "learning_rate = 5e-6\n",
    "batch_size = 8\n",
    "\n",
    "# To train a specific batch\n",
    "batch_num = 1  # Change this to the batch you want to train\n",
    "model, f1_score = train_model(\n",
    "    batch_number=batch_num,\n",
    "    n_epochs=n_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,\n",
    "    use_optimal_thresholds=True\n",
    ")\n",
    "print(f\"Batch {batch_num} training complete. Best F1 score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e9f0f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model on obvious examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n9/9tsj9gw15ll9ycvpvm3m3pq80000gn/T/ipykernel_6298/1895935901.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model/batch_1/best_model.pt\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Thresholds loaded successfully!\n",
      "\n",
      "Testing model on less obvious examples...\n",
      "Model loaded successfully!\n",
      "Thresholds loaded successfully!\n",
      "\n",
      "Testing model on challenging examples...\n",
      "Model loaded successfully!\n",
      "Thresholds loaded successfully!\n",
      "\n",
      "Testing model on negative examples...\n",
      "Model loaded successfully!\n",
      "Thresholds loaded successfully!\n",
      "\n",
      "=== Obvious Examples ===\n",
      "\n",
      "Example 1: \"The receptor undergoes autophosphorylation upon ligand binding, which triggers a signaling cascade.\"\n",
      "  Predicted classes (above threshold):\n",
      "    - autophosphorylation: 0.7972\n",
      "    - autoregulation: 0.3231\n",
      "  Top 3 probabilities:\n",
      "    - autophosphorylation: 0.7972\n",
      "    - autoregulation: 0.3231\n",
      "    - autoinhibitory: 0.2873\n",
      "\n",
      "Example 2: \"Transcription factors exhibiting autoregulation can bind to their own promoters to control expression levels.\"\n",
      "  Predicted classes (above threshold):\n",
      "    - autoregulation: 0.8560\n",
      "    - autoregulatory: 0.3619\n",
      "  Top 3 probabilities:\n",
      "    - autoregulation: 0.8560\n",
      "    - autoregulatory: 0.3619\n",
      "    - non-autoregulatory: 0.2407\n",
      "\n",
      "Example 3: \"The protein kinase shows autoactivation through conformational changes in its catalytic domain.\"\n",
      "  Predicted classes (above threshold):\n",
      "    - autophosphorylation: 0.4994\n",
      "    - autoregulation: 0.4863\n",
      "  Top 3 probabilities:\n",
      "    - autophosphorylation: 0.4994\n",
      "    - autoregulation: 0.4863\n",
      "    - autoinhibition: 0.4073\n",
      "\n",
      "Example 4: \"Bacterial quorum sensing relies on autoinducers that accumulate in the environment.\"\n",
      "  Predicted classes (above threshold):\n",
      "    - autoinducer: 0.7519\n",
      "    - autoregulation: 0.3835\n",
      "    - autophosphorylation: 0.3411\n",
      "  Top 3 probabilities:\n",
      "    - autoinducer: 0.7519\n",
      "    - autoregulation: 0.3835\n",
      "    - autophosphorylation: 0.3411\n",
      "\n",
      "Example 5: \"Apoptosis involves proteases that undergo autocatalytic activation through proteolytic cleavage.\"\n",
      "  Predicted classes (above threshold):\n",
      "    - autocatalytic: 0.6773\n",
      "    - autophosphorylation: 0.4686\n",
      "  Top 3 probabilities:\n",
      "    - autocatalytic: 0.6773\n",
      "    - autophosphorylation: 0.4686\n",
      "    - autolysis: 0.3238\n",
      "\n",
      "\n",
      "=== Less Obvious Examples ===\n",
      "\n",
      "Example 1: \"The transcription factor binds to its own promoter region, creating a negative feedback loop.\"\n",
      "  Predicted classes (above threshold):\n",
      "    - autoregulation: 0.4508\n",
      "    - autoregulatory: 0.4445\n",
      "    - autophosphorylation: 0.3426\n",
      "  Top 3 probabilities:\n",
      "    - autoregulation: 0.4508\n",
      "    - autoregulatory: 0.4445\n",
      "    - autophosphorylation: 0.3426\n",
      "\n",
      "Example 2: \"Upon phosphorylation, the enzyme can activate additional copies of itself, creating a positive feedback circuit.\"\n",
      "  Predicted classes (above threshold):\n",
      "    - autophosphorylation: 0.4418\n",
      "    - autoregulation: 0.4318\n",
      "    - autoregulatory: 0.3495\n",
      "  Top 3 probabilities:\n",
      "    - autophosphorylation: 0.4418\n",
      "    - autoregulation: 0.4318\n",
      "    - autoregulatory: 0.3495\n",
      "\n",
      "Example 3: \"The receptor dimerizes and cross-phosphorylates residues in the intracellular domain.\"\n",
      "  Predicted classes (above threshold):\n",
      "    - autophosphorylation: 0.6613\n",
      "  Top 3 probabilities:\n",
      "    - autophosphorylation: 0.6613\n",
      "    - autoinhibition: 0.2414\n",
      "    - autoregulatory: 0.2216\n",
      "\n",
      "Example 4: \"The repressor protein inhibits its own gene expression when concentrations exceed a threshold.\"\n",
      "  Predicted classes (above threshold):\n",
      "    - autoregulation: 0.3961\n",
      "    - autoregulatory: 0.3957\n",
      "  Top 3 probabilities:\n",
      "    - autoregulation: 0.3961\n",
      "    - autoregulatory: 0.3957\n",
      "    - autoinducer: 0.2808\n",
      "\n",
      "Example 5: \"Bacterial cells produce signaling molecules that, when detected, stimulate further production of the same molecule.\"\n",
      "  Predicted classes (above threshold):\n",
      "    - autophosphorylation: 0.4666\n",
      "    - non-autoregulatory: 0.3016\n",
      "  Top 3 probabilities:\n",
      "    - autophosphorylation: 0.4666\n",
      "    - non-autoregulatory: 0.3016\n",
      "    - autoregulation: 0.2976\n",
      "\n",
      "\n",
      "=== Challenging Examples ===\n",
      "\n",
      "Example 1: \"The protein shows increased activity following binding to its interaction partner, which itself is regulated by the same signaling pathway.\"\n",
      "  Predicted classes (above threshold):\n",
      "    - autophosphorylation: 0.4267\n",
      "    - autoregulation: 0.4034\n",
      "    - non-autoregulatory: 0.3630\n",
      "  Top 3 probabilities:\n",
      "    - autophosphorylation: 0.4267\n",
      "    - autoregulation: 0.4034\n",
      "    - non-autoregulatory: 0.3630\n",
      "\n",
      "Example 2: \"Enzyme activity decreases following substrate binding, potentially through an allosteric mechanism involving the active site.\"\n",
      "  Predicted classes (above threshold):\n",
      "    - autophosphorylation: 0.3618\n",
      "    - autoregulation: 0.3274\n",
      "    - non-autoregulatory: 0.3017\n",
      "  Top 3 probabilities:\n",
      "    - autophosphorylation: 0.3618\n",
      "    - autoinhibition: 0.3434\n",
      "    - autoregulation: 0.3274\n",
      "\n",
      "Example 3: \"Regulatory T cells suppress immune responses through multiple feedback mechanisms involving cytokine signaling.\"\n",
      "  Predicted classes (above threshold):\n",
      "    - autoregulatory: 0.4614\n",
      "    - non-autoregulatory: 0.4424\n",
      "    - autophosphorylation: 0.3601\n",
      "    - autoregulation: 0.3520\n",
      "  Top 3 probabilities:\n",
      "    - autoregulatory: 0.4614\n",
      "    - non-autoregulatory: 0.4424\n",
      "    - autophosphorylation: 0.3601\n",
      "\n",
      "Example 4: \"The gene locus contains binding sites for both activating and repressing factors that are co-expressed with the gene itself.\"\n",
      "  Predicted classes (above threshold):\n",
      "    - autoregulation: 0.4018\n",
      "    - non-autoregulatory: 0.3575\n",
      "    - autophosphorylation: 0.3202\n",
      "  Top 3 probabilities:\n",
      "    - autoregulation: 0.4018\n",
      "    - non-autoregulatory: 0.3575\n",
      "    - autophosphorylation: 0.3202\n",
      "\n",
      "Example 5: \"Proteolytic processing of the prohormone yields bioactive peptides that modulate receptor sensitivity.\"\n",
      "  Predicted classes (above threshold):\n",
      "    - autophosphorylation: 0.4003\n",
      "    - autoregulation: 0.3293\n",
      "    - autoregulatory: 0.3088\n",
      "  Top 3 probabilities:\n",
      "    - autocatalytic: 0.4010\n",
      "    - autophosphorylation: 0.4003\n",
      "    - autoregulation: 0.3293\n",
      "\n",
      "\n",
      "=== Negative Examples ===\n",
      "\n",
      "Example 1: \"The housekeeping gene is constitutively expressed under normal cellular conditions.\"\n",
      "  Predicted classes (above threshold):\n",
      "    - autoregulation: 0.4902\n",
      "    - non-autoregulatory: 0.4694\n",
      "    - autophosphorylation: 0.3473\n",
      "  Top 3 probabilities:\n",
      "    - autoregulation: 0.4902\n",
      "    - non-autoregulatory: 0.4694\n",
      "    - autophosphorylation: 0.3473\n",
      "\n",
      "Example 2: \"Protein translation is initiated at the ribosome following mRNA binding.\"\n",
      "  Predicted classes (above threshold):\n",
      "    - autoregulation: 0.4048\n",
      "    - autophosphorylation: 0.3959\n",
      "    - non-autoregulatory: 0.3438\n",
      "  Top 3 probabilities:\n",
      "    - autoregulation: 0.4048\n",
      "    - autophosphorylation: 0.3959\n",
      "    - non-autoregulatory: 0.3438\n",
      "\n",
      "Example 3: \"Cell division requires the coordinated action of multiple cytoskeletal proteins.\"\n",
      "  Predicted classes (above threshold):\n",
      "    - autophosphorylation: 0.4657\n",
      "    - autoregulatory: 0.3814\n",
      "    - autoregulation: 0.3591\n",
      "  Top 3 probabilities:\n",
      "    - autophosphorylation: 0.4657\n",
      "    - autoregulatory: 0.3814\n",
      "    - autoregulation: 0.3591\n",
      "\n",
      "Example 4: \"Passive diffusion of ions occurs through the membrane channel following a concentration gradient.\"\n",
      "  Predicted classes (above threshold):\n",
      "    - autophosphorylation: 0.3872\n",
      "    - autoregulation: 0.3580\n",
      "    - autoregulatory: 0.3335\n",
      "  Top 3 probabilities:\n",
      "    - autophosphorylation: 0.3872\n",
      "    - autoregulation: 0.3580\n",
      "    - autoinhibition: 0.3398\n",
      "\n",
      "Example 5: \"The monoclonal antibody binds specifically to the epitope on the target antigen.\"\n",
      "  Predicted classes (above threshold):\n",
      "    - non-autoregulatory: 0.3173\n",
      "  Top 3 probabilities:\n",
      "    - non-autoregulatory: 0.3173\n",
      "    - autoregulatory: 0.2973\n",
      "    - autophosphorylation: 0.2786\n",
      "\n",
      "\n",
      "=== Performance Analysis ===\n",
      "\n",
      "Obvious Examples:\n",
      "  Detection rate: 5/5 (100.0%)\n",
      "  Average predicted classes: 2.20\n",
      "  Average confidence: 0.7164\n",
      "  Auto-related classes detected: 5/5 (100.0%)\n",
      "\n",
      "Less Obvious Examples:\n",
      "  Detection rate: 5/5 (100.0%)\n",
      "  Average predicted classes: 2.20\n",
      "  Average confidence: 0.4833\n",
      "  Auto-related classes detected: 5/5 (100.0%)\n",
      "\n",
      "Challenging Examples:\n",
      "  Detection rate: 5/5 (100.0%)\n",
      "  Average predicted classes: 3.20\n",
      "  Average confidence: 0.4105\n",
      "  Auto-related classes detected: 5/5 (100.0%)\n",
      "\n",
      "Negative Examples:\n",
      "  Detection rate: 5/5 (100.0%)\n",
      "  Average predicted classes: 2.60\n",
      "  Average confidence: 0.4130\n",
      "  False positives (auto-related): 4/5 (80.0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import torch.nn as nn\n",
    "\n",
    "# First, let's define a function to load the model and make predictions\n",
    "def load_model_and_predict(text_samples):\n",
    "    \"\"\"\n",
    "    Load the saved model and make predictions on text samples\n",
    "    \"\"\"\n",
    "    # Load label columns\n",
    "    try:\n",
    "        with open('label_columns.json', 'r') as f:\n",
    "            label_columns = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        # Fallback if file not found\n",
    "        label_columns = [\n",
    "            'autoactivation', 'autocatalysis', 'autocatalytic', 'autofeedback',\n",
    "            'autoinducer', 'autoinduction', 'autoinhibition', 'autoinhibitory',\n",
    "            'autokinase', 'autolysis', 'autophosphorylation', 'autoregulation',\n",
    "            'autoregulatory', 'autoubiquitination', 'non-autoregulatory'\n",
    "        ]\n",
    "    \n",
    "    # Define the model architecture (same as in training)\n",
    "    class ImprovedBioBERTClassifier(nn.Module):\n",
    "        def __init__(self, n_classes, dropout1=0.1, dropout2=0.2):\n",
    "            super(ImprovedBioBERTClassifier, self).__init__()\n",
    "            self.bert = AutoModel.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "            self.dropout1 = nn.Dropout(dropout1)\n",
    "            self.intermediate = nn.Linear(self.bert.config.hidden_size, 512)\n",
    "            self.activation = nn.ReLU()\n",
    "            self.dropout2 = nn.Dropout(dropout2)\n",
    "            self.classifier = nn.Linear(512, n_classes)\n",
    "        \n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "            pooled_output = self.dropout1(pooled_output)\n",
    "            intermediate = self.intermediate(pooled_output)\n",
    "            intermediate = self.activation(intermediate)\n",
    "            intermediate = self.dropout2(intermediate)\n",
    "            logits = self.classifier(intermediate)\n",
    "            return logits\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ImprovedBioBERTClassifier(n_classes=len(label_columns)).to(device)\n",
    "    \n",
    "    # Load model weights\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(\"model/batch_1/best_model.pt\", map_location=device))\n",
    "        print(\"Model loaded successfully!\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Model file not found. Make sure you have saved the model properly.\")\n",
    "        return None\n",
    "    \n",
    "    # Load thresholds\n",
    "    try:\n",
    "        with open(\"model/batch_1/best_thresholds.json\", 'r') as f:\n",
    "            thresholds = json.load(f)\n",
    "        print(\"Thresholds loaded successfully!\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: Thresholds file not found. Using default threshold of 0.5.\")\n",
    "        thresholds = [0.5] * len(label_columns)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess and predict for each text\n",
    "    results = []\n",
    "    \n",
    "    for text in text_samples:\n",
    "        # Tokenize\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        input_ids = encoding['input_ids'].to(device)\n",
    "        attention_mask = encoding['attention_mask'].to(device)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            probabilities = torch.sigmoid(outputs).cpu().numpy()[0]\n",
    "        \n",
    "        # Get predictions above threshold\n",
    "        predictions = {}\n",
    "        for i, label in enumerate(label_columns):\n",
    "            if probabilities[i] >= thresholds[i]:\n",
    "                predictions[label] = float(probabilities[i])\n",
    "        \n",
    "        # Get top 3 probabilities regardless of threshold\n",
    "        top_indices = np.argsort(probabilities)[::-1][:3]\n",
    "        top_3 = {label_columns[i]: float(probabilities[i]) for i in top_indices}\n",
    "        \n",
    "        # Append results\n",
    "        results.append({\n",
    "            'text': text,\n",
    "            'predictions': predictions,\n",
    "            'top_3': top_3,\n",
    "            'all_probs': {label_columns[i]: float(probabilities[i]) for i in range(len(label_columns))}\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Now, let's create diverse test cases\n",
    "\n",
    "# Category 1: Obvious examples with \"auto\" terms in them\n",
    "obvious_examples = [\n",
    "    \"The receptor undergoes autophosphorylation upon ligand binding, which triggers a signaling cascade.\",\n",
    "    \"Transcription factors exhibiting autoregulation can bind to their own promoters to control expression levels.\",\n",
    "    \"The protein kinase shows autoactivation through conformational changes in its catalytic domain.\",\n",
    "    \"Bacterial quorum sensing relies on autoinducers that accumulate in the environment.\",\n",
    "    \"Apoptosis involves proteases that undergo autocatalytic activation through proteolytic cleavage.\"\n",
    "]\n",
    "\n",
    "# Category 2: Less obvious examples (no \"auto\" terms)\n",
    "less_obvious_examples = [\n",
    "    \"The transcription factor binds to its own promoter region, creating a negative feedback loop.\",\n",
    "    \"Upon phosphorylation, the enzyme can activate additional copies of itself, creating a positive feedback circuit.\",\n",
    "    \"The receptor dimerizes and cross-phosphorylates residues in the intracellular domain.\",\n",
    "    \"The repressor protein inhibits its own gene expression when concentrations exceed a threshold.\",\n",
    "    \"Bacterial cells produce signaling molecules that, when detected, stimulate further production of the same molecule.\"\n",
    "]\n",
    "\n",
    "# Category 3: Challenging cases with ambiguous regulatory mechanisms\n",
    "challenging_examples = [\n",
    "    \"The protein shows increased activity following binding to its interaction partner, which itself is regulated by the same signaling pathway.\",\n",
    "    \"Enzyme activity decreases following substrate binding, potentially through an allosteric mechanism involving the active site.\",\n",
    "    \"Regulatory T cells suppress immune responses through multiple feedback mechanisms involving cytokine signaling.\",\n",
    "    \"The gene locus contains binding sites for both activating and repressing factors that are co-expressed with the gene itself.\",\n",
    "    \"Proteolytic processing of the prohormone yields bioactive peptides that modulate receptor sensitivity.\"\n",
    "]\n",
    "\n",
    "# Category 4: Negative cases (non-autoregulatory)\n",
    "negative_examples = [\n",
    "    \"The housekeeping gene is constitutively expressed under normal cellular conditions.\",\n",
    "    \"Protein translation is initiated at the ribosome following mRNA binding.\",\n",
    "    \"Cell division requires the coordinated action of multiple cytoskeletal proteins.\",\n",
    "    \"Passive diffusion of ions occurs through the membrane channel following a concentration gradient.\",\n",
    "    \"The monoclonal antibody binds specifically to the epitope on the target antigen.\"\n",
    "]\n",
    "\n",
    "# Run prediction on all test sets\n",
    "print(\"Testing model on obvious examples...\")\n",
    "obvious_results = load_model_and_predict(obvious_examples)\n",
    "\n",
    "print(\"\\nTesting model on less obvious examples...\")\n",
    "less_obvious_results = load_model_and_predict(less_obvious_examples)\n",
    "\n",
    "print(\"\\nTesting model on challenging examples...\")\n",
    "challenging_results = load_model_and_predict(challenging_examples)\n",
    "\n",
    "print(\"\\nTesting model on negative examples...\")\n",
    "negative_results = load_model_and_predict(negative_examples)\n",
    "\n",
    "# Display results in a clear format\n",
    "def display_test_results(results, category_name):\n",
    "    print(f\"\\n=== {category_name} ===\\n\")\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"Example {i+1}: \\\"{result['text']}\\\"\")\n",
    "        \n",
    "        if result['predictions']:\n",
    "            print(\"  Predicted classes (above threshold):\")\n",
    "            for label, prob in sorted(result['predictions'].items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"    - {label}: {prob:.4f}\")\n",
    "        else:\n",
    "            print(\"  No classes above threshold\")\n",
    "        \n",
    "        print(\"  Top 3 probabilities:\")\n",
    "        for label, prob in sorted(result['top_3'].items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"    - {label}: {prob:.4f}\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "# Display all results\n",
    "if obvious_results:\n",
    "    display_test_results(obvious_results, \"Obvious Examples\")\n",
    "    \n",
    "if less_obvious_results:\n",
    "    display_test_results(less_obvious_results, \"Less Obvious Examples\")\n",
    "    \n",
    "if challenging_results:\n",
    "    display_test_results(challenging_results, \"Challenging Examples\")\n",
    "    \n",
    "if negative_results:\n",
    "    display_test_results(negative_results, \"Negative Examples\")\n",
    "\n",
    "# Analyze overall performance\n",
    "def analyze_performance(results_sets):\n",
    "    categories = [\"Obvious\", \"Less Obvious\", \"Challenging\", \"Negative\"]\n",
    "    \n",
    "    print(\"\\n=== Performance Analysis ===\\n\")\n",
    "    \n",
    "    for category, results in zip(categories, results_sets):\n",
    "        if not results:\n",
    "            continue\n",
    "            \n",
    "        # Count how many examples had at least one prediction\n",
    "        predictions_count = sum(1 for r in results if r['predictions'])\n",
    "        \n",
    "        # Calculate average number of predicted classes\n",
    "        avg_predictions = sum(len(r['predictions']) for r in results) / len(results)\n",
    "        \n",
    "        # Calculate confidence (average probability of top prediction)\n",
    "        avg_confidence = sum(list(r['top_3'].values())[0] for r in results) / len(results)\n",
    "        \n",
    "        print(f\"{category} Examples:\")\n",
    "        print(f\"  Detection rate: {predictions_count}/{len(results)} ({predictions_count/len(results)*100:.1f}%)\")\n",
    "        print(f\"  Average predicted classes: {avg_predictions:.2f}\")\n",
    "        print(f\"  Average confidence: {avg_confidence:.4f}\")\n",
    "        \n",
    "        # For non-negative categories, check if autoregulatory classes are detected\n",
    "        if category != \"Negative\":\n",
    "            auto_detected = sum(1 for r in results if any('auto' in label.lower() for label in r['predictions']))\n",
    "            print(f\"  Auto-related classes detected: {auto_detected}/{len(results)} ({auto_detected/len(results)*100:.1f}%)\")\n",
    "        # For negative category, check for false positives\n",
    "        else:\n",
    "            auto_detected = sum(1 for r in results if any('auto' in label.lower() and label != 'non-autoregulatory' for label in r['predictions']))\n",
    "            print(f\"  False positives (auto-related): {auto_detected}/{len(results)} ({auto_detected/len(results)*100:.1f}%)\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "# Run performance analysis\n",
    "if all([obvious_results, less_obvious_results, challenging_results, negative_results]):\n",
    "    analyze_performance([obvious_results, less_obvious_results, challenging_results, negative_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d6595a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Model Architecture\n",
    "class EnhancedBioBERTClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, dropout1=0.1, dropout2=0.2):\n",
    "        super(EnhancedBioBERTClassifier, self).__init__()\n",
    "        # Load base BioBERT\n",
    "        self.bert = AutoModel.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "        \n",
    "        # Add relation attention mechanism\n",
    "        self.relation_query = nn.Parameter(torch.randn(768, 1))\n",
    "        self.relation_key = nn.Linear(768, 768)\n",
    "        self.relation_value = nn.Linear(768, 768)\n",
    "        \n",
    "        # Main classification path\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.intermediate = nn.Linear(768 * 2, 512)  # Doubled for concatenation\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        self.classifier = nn.Linear(512, n_classes)\n",
    "        \n",
    "        # Initialize weights properly\n",
    "        nn.init.xavier_normal_(self.intermediate.weight)\n",
    "        nn.init.zeros_(self.intermediate.bias)\n",
    "        nn.init.xavier_normal_(self.classifier.weight)\n",
    "        nn.init.zeros_(self.classifier.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BioBERT embeddings\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Get sequence outputs\n",
    "        sequence_output = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
    "        cls_token = sequence_output[:, 0, :]  # [batch_size, hidden_size]\n",
    "        \n",
    "        # Apply relation attention\n",
    "        attention_mask_expanded = attention_mask.unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
    "        \n",
    "        # Calculate relation-aware attention weights\n",
    "        relation_keys = self.relation_key(sequence_output)  # [batch_size, seq_len, hidden_size]\n",
    "        query = self.relation_query.unsqueeze(0).expand(input_ids.size(0), -1, -1)  # [batch_size, hidden_size, 1]\n",
    "        \n",
    "        # Get attention scores and mask padding tokens\n",
    "        attention_scores = torch.bmm(relation_keys, query)  # [batch_size, seq_len, 1]\n",
    "        attention_scores = attention_scores.masked_fill(attention_mask_expanded == 0, -10000.0)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)  # [batch_size, seq_len, 1]\n",
    "        \n",
    "        # Get relation-aware context vector\n",
    "        relation_values = self.relation_value(sequence_output)  # [batch_size, seq_len, hidden_size]\n",
    "        relation_context = torch.sum(attention_weights * relation_values, dim=1)  # [batch_size, hidden_size]\n",
    "        \n",
    "        # Combine CLS token with relation context\n",
    "        pooled_output = torch.cat([cls_token, relation_context], dim=1)  # [batch_size, hidden_size*2]\n",
    "        pooled_output = self.dropout1(pooled_output)\n",
    "        \n",
    "        # Classification\n",
    "        intermediate = self.intermediate(pooled_output)\n",
    "        intermediate = self.activation(intermediate)\n",
    "        intermediate = self.dropout2(intermediate)\n",
    "        logits = self.classifier(intermediate)\n",
    "        \n",
    "        return logits, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d447ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_enhanced_model(original_model_path):\n",
    "    \"\"\"\n",
    "    Load the existing model and convert it to the enhanced model\n",
    "    \"\"\"\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load label columns\n",
    "    try:\n",
    "        with open('label_columns.json', 'r') as f:\n",
    "            label_columns = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        # Fallback if file not found\n",
    "        label_columns = [\n",
    "            'autoactivation', 'autocatalysis', 'autocatalytic', 'autofeedback',\n",
    "            'autoinducer', 'autoinduction', 'autoinhibition', 'autoinhibitory',\n",
    "            'autokinase', 'autolysis', 'autophosphorylation', 'autoregulation',\n",
    "            'autoregulatory', 'autoubiquitination', 'non-autoregulatory'\n",
    "        ]\n",
    "    \n",
    "    # Define the original model architecture\n",
    "    class ImprovedBioBERTClassifier(nn.Module):\n",
    "        def __init__(self, n_classes, dropout1=0.1, dropout2=0.2):\n",
    "            super(ImprovedBioBERTClassifier, self).__init__()\n",
    "            self.bert = AutoModel.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "            self.dropout1 = nn.Dropout(dropout1)\n",
    "            self.intermediate = nn.Linear(self.bert.config.hidden_size, 512)\n",
    "            self.activation = nn.ReLU()\n",
    "            self.dropout2 = nn.Dropout(dropout2)\n",
    "            self.classifier = nn.Linear(512, n_classes)\n",
    "        \n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "            pooled_output = self.dropout1(pooled_output)\n",
    "            intermediate = self.intermediate(pooled_output)\n",
    "            intermediate = self.activation(intermediate)\n",
    "            intermediate = self.dropout2(intermediate)\n",
    "            logits = self.classifier(intermediate)\n",
    "            return logits\n",
    "    \n",
    "    # Load original model\n",
    "    original_model = ImprovedBioBERTClassifier(n_classes=len(label_columns)).to(device)\n",
    "    original_model.load_state_dict(torch.load(original_model_path, map_location=device))\n",
    "    print(\"Original model loaded successfully!\")\n",
    "    \n",
    "    # Create enhanced model\n",
    "    enhanced_model = EnhancedBioBERTClassifier(n_classes=len(label_columns)).to(device)\n",
    "    \n",
    "    # Transfer weights from original model to enhanced model\n",
    "    # 1. BERT weights\n",
    "    enhanced_model.bert.load_state_dict(original_model.bert.state_dict())\n",
    "    \n",
    "    print(\"Enhanced model created and initialized with original BERT weights!\")\n",
    "    return enhanced_model, device, label_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "579aa8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Fine-tuning function for adaptation to implicit relations\n",
    "def finetune_for_implicit_relations(model, batch_size=4, learning_rate=1e-5, epochs=3):\n",
    "    \"\"\"\n",
    "    Fine-tune the model to detect implicit relations using augmented examples\n",
    "    \"\"\"\n",
    "    # Set up tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "    \n",
    "    # Create examples that paraphrase autoregulatory mechanisms without using \"auto\" terms\n",
    "    implicit_examples = [\n",
    "        # Text describing autoregulation without \"auto\" terms\n",
    "        (\"The transcription factor binds to its own promoter region.\", \"autoregulation\"),\n",
    "        (\"The enzyme activates itself through conformational change.\", \"autoactivation\"),\n",
    "        (\"The protein phosphorylates itself on a tyrosine residue.\", \"autophosphorylation\"),\n",
    "        (\"The protease cleaves itself to generate the active form.\", \"autocatalysis\"),\n",
    "        (\"The cell produces molecules that signal itself to change behavior.\", \"autoinduction\"),\n",
    "        (\"The receptor signals to reduce its own expression level.\", \"autoinhibition\"),\n",
    "        # More challenging and diverse examples\n",
    "        (\"Upon binding ligand, the receptor undergoes a conformational change that enables phosphorylation of its cytoplasmic domain.\", \"autophosphorylation\"),\n",
    "        (\"The transcription factor negatively controls expression of its own gene.\", \"autoregulation\"),\n",
    "        (\"The kinase domain transfers phosphate groups to residues within the same protein.\", \"autophosphorylation\"),\n",
    "        (\"This bacterial system uses cell-to-cell signaling to coordinate population behavior.\", \"autoinduction\"),\n",
    "        (\"The peptide recognizes and binds specifically to the same protein it was derived from.\", \"autofeedback\"),\n",
    "        (\"The dimeric protein activates by cross-phosphorylation between the two identical subunits.\", \"autoactivation\")\n",
    "    ]\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    class ImplicitRelationDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, examples, tokenizer, max_length=512):\n",
    "            self.examples = examples\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "            \n",
    "            # Map labels to indices\n",
    "            self.label_map = {\n",
    "                'autoactivation': 0,\n",
    "                'autocatalysis': 1,\n",
    "                'autocatalytic': 2,\n",
    "                'autofeedback': 3,\n",
    "                'autoinducer': 4,\n",
    "                'autoinduction': 5,\n",
    "                'autoinhibition': 6,\n",
    "                'autoinhibitory': 7,\n",
    "                'autokinase': 8,\n",
    "                'autolysis': 9,\n",
    "                'autophosphorylation': 10,\n",
    "                'autoregulation': 11,\n",
    "                'autoregulatory': 12,\n",
    "                'autoubiquitination': 13,\n",
    "                'non-autoregulatory': 14\n",
    "            }\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.examples)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            text, label = self.examples[idx]\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Create one-hot encoded label\n",
    "            label_index = self.label_map[label]\n",
    "            label_tensor = torch.zeros(15)\n",
    "            label_tensor[label_index] = 1.0\n",
    "            \n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'labels': label_tensor\n",
    "            }\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = ImplicitRelationDataset(implicit_examples, tokenizer)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Set up optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            # Get batch data\n",
    "            input_ids = batch['input_ids'].to(model.bert.device)\n",
    "            attention_mask = batch['attention_mask'].to(model.bert.device)\n",
    "            labels = batch['labels'].to(model.bert.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            logits, attention_weights = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    print(\"Fine-tuning complete!\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d39abcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(model, text, tokenizer):\n",
    "    \"\"\"\n",
    "    Visualize which parts of the text the model attends to for relation detection\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Tokenize the text\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = encoding['input_ids'].to(model.bert.device)\n",
    "    attention_mask = encoding['attention_mask'].to(model.bert.device)\n",
    "    \n",
    "    # Get tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    \n",
    "    # Forward pass to get attention weights\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, attention_weights = model(input_ids, attention_mask)\n",
    "    \n",
    "    # Convert attention weights to numpy\n",
    "    attention_weights = attention_weights.squeeze().cpu().numpy()\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    sns.heatmap([attention_weights[1:len(tokens)-1]], \n",
    "                xticklabels=tokens[1:len(tokens)-1],\n",
    "                yticklabels=['Attention'],\n",
    "                cmap='viridis',\n",
    "                cbar_kws={'label': 'Attention Weight'})\n",
    "    plt.title(f'Relation Attention for: \"{text}\"')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find top 5 attended tokens\n",
    "    top_indices = attention_weights.argsort()[-5:][::-1]\n",
    "    top_tokens = [tokens[i+1] for i in top_indices if i+1 < len(tokens)-1]\n",
    "    print(f\"Top attended tokens: {', '.join(top_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4effb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_enhanced_model(model, texts, label_columns, thresholds=None):\n",
    "    \"\"\"\n",
    "    Make predictions with the enhanced model and highlight relation attention\n",
    "    \"\"\"\n",
    "    # Set default thresholds if not provided\n",
    "    if thresholds is None:\n",
    "        thresholds = [0.3] * len(label_columns)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    results = []\n",
    "    for text in texts:\n",
    "        # Tokenize\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        input_ids = encoding['input_ids'].to(model.bert.device)\n",
    "        attention_mask = encoding['attention_mask'].to(model.bert.device)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            logits, attention_weights = model(input_ids, attention_mask)\n",
    "            probabilities = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "        \n",
    "        # Get predictions above threshold\n",
    "        predictions = {}\n",
    "        for i, label in enumerate(label_columns):\n",
    "            if probabilities[i] >= thresholds[i]:\n",
    "                predictions[label] = float(probabilities[i])\n",
    "        \n",
    "        # Get top 3 probabilities regardless of threshold\n",
    "        top_indices = np.argsort(probabilities)[::-1][:3]\n",
    "        top_3 = {label_columns[i]: float(probabilities[i]) for i in top_indices}\n",
    "        \n",
    "        # Extract attention for relation understanding\n",
    "        attention = attention_weights.squeeze().cpu().numpy()\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "        attended_tokens = []\n",
    "        for i in range(1, min(len(tokens)-1, len(attention))):\n",
    "            if attention[i] > 0.05:  # Threshold for significant attention\n",
    "                attended_tokens.append((tokens[i], float(attention[i])))\n",
    "        \n",
    "        attended_tokens.sort(key=lambda x: x[1], reverse=True)\n",
    "        attended_tokens = attended_tokens[:5]  # Top 5 attended tokens\n",
    "        \n",
    "        # Append results\n",
    "        results.append({\n",
    "            'text': text,\n",
    "            'predictions': predictions,\n",
    "            'top_3': top_3,\n",
    "            'attended_tokens': attended_tokens,\n",
    "            'all_probs': {label_columns[i]: float(probabilities[i]) for i in range(len(label_columns))}\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b0724f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n9/9tsj9gw15ll9ycvpvm3m3pq80000gn/T/ipykernel_6298/758444873.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  original_model.load_state_dict(torch.load(original_model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model loaded successfully!\n",
      "Enhanced model created and initialized with original BERT weights!\n",
      "Epoch 1/3 - Loss: 0.7482\n",
      "Epoch 2/3 - Loss: 0.6287\n",
      "Epoch 3/3 - Loss: 0.5606\n",
      "Fine-tuning complete!\n",
      "Thresholds loaded successfully!\n",
      "Enhanced model saved!\n",
      "Example 1: \"The protein binds to its own regulatory region, creating a negative feedback loop.\"\n",
      "  Predicted autoregulatory classes:\n",
      "    - autoinduction: 0.5398\n",
      "    - non-autoregulatory: 0.4928\n",
      "    - autoregulation: 0.4501\n",
      "    - autofeedback: 0.3955\n",
      "    - autophosphorylation: 0.3788\n",
      "    - autokinase: 0.3711\n",
      "    - autoactivation: 0.3444\n",
      "    - autoinducer: 0.3203\n",
      "  Top 3 probabilities:\n",
      "    - autoinduction: 0.5398\n",
      "    - autoinhibition: 0.4943\n",
      "    - non-autoregulatory: 0.4928\n",
      "  Top attended tokens (relation clues):\n",
      "    - protein: 0.4730\n",
      "    - feedback: 0.3411\n",
      "    - to: 0.1461\n",
      "\n",
      "Example 2: \"The enzyme can activate other copies of itself, creating a cascade effect.\"\n",
      "  Predicted autoregulatory classes:\n",
      "    - autoinhibition: 0.5047\n",
      "    - autoinduction: 0.4666\n",
      "    - autoregulation: 0.4581\n",
      "    - non-autoregulatory: 0.3993\n",
      "    - autophosphorylation: 0.3855\n",
      "    - autofeedback: 0.3819\n",
      "    - autokinase: 0.3476\n",
      "    - autoinducer: 0.3274\n",
      "    - autoactivation: 0.3029\n",
      "  Top 3 probabilities:\n",
      "    - autoinhibition: 0.5047\n",
      "    - autoinduction: 0.4666\n",
      "    - autocatalysis: 0.4645\n",
      "  Top attended tokens (relation clues):\n",
      "    - .: 0.8246\n",
      "    - other: 0.1208\n",
      "\n",
      "Example 3: \"Upon binding ligand, the receptor undergoes a conformational change that enables phosphorylation of its cytoplasmic domain.\"\n",
      "  Predicted autoregulatory classes:\n",
      "    - autoinhibition: 0.5159\n",
      "    - non-autoregulatory: 0.4892\n",
      "    - autoinduction: 0.4612\n",
      "    - autoregulation: 0.4457\n",
      "    - autophosphorylation: 0.4118\n",
      "    - autokinase: 0.3506\n",
      "    - autoactivation: 0.3410\n",
      "    - autofeedback: 0.3405\n",
      "    - autoinducer: 0.3270\n",
      "  Top 3 probabilities:\n",
      "    - autoinhibition: 0.5159\n",
      "    - non-autoregulatory: 0.4892\n",
      "    - autoinduction: 0.4612\n",
      "  Top attended tokens (relation clues):\n",
      "    - ##igan: 0.4972\n",
      "    - binding: 0.2706\n",
      "    - ##d: 0.2291\n",
      "\n",
      "Example 4: \"The kinase domain transfers phosphate groups to residues within the same protein structure.\"\n",
      "  Predicted autoregulatory classes:\n",
      "    - autoinduction: 0.4705\n",
      "    - non-autoregulatory: 0.4342\n",
      "    - autoregulation: 0.3945\n",
      "    - autophosphorylation: 0.3743\n",
      "    - autofeedback: 0.3505\n",
      "    - autoinducer: 0.3242\n",
      "    - autokinase: 0.3214\n",
      "  Top 3 probabilities:\n",
      "    - autoinhibition: 0.4757\n",
      "    - autoinduction: 0.4705\n",
      "    - non-autoregulatory: 0.4342\n",
      "  Top attended tokens (relation clues):\n",
      "    - the: 0.3451\n",
      "    - domain: 0.3190\n",
      "    - .: 0.1116\n",
      "    - protein: 0.0908\n",
      "    - to: 0.0745\n",
      "\n",
      "Example 5: \"The transcription factor controls expression of its own gene, maintaining homeostasis.\"\n",
      "  Predicted autoregulatory classes:\n",
      "    - autoinhibition: 0.5666\n",
      "    - autoinduction: 0.5193\n",
      "    - autoregulation: 0.4502\n",
      "    - non-autoregulatory: 0.4494\n",
      "    - autokinase: 0.4042\n",
      "    - autofeedback: 0.3998\n",
      "    - autoactivation: 0.3462\n",
      "    - autoinducer: 0.3455\n",
      "    - autophosphorylation: 0.3106\n",
      "  Top 3 probabilities:\n",
      "    - autoinhibition: 0.5666\n",
      "    - autoinduction: 0.5193\n",
      "    - autocatalysis: 0.5000\n",
      "  Top attended tokens (relation clues):\n",
      "    - transcription: 0.8227\n",
      "    - factor: 0.1639\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top attended tokens: binds, loop, its, .\n"
     ]
    }
   ],
   "source": [
    "def enhance_and_test_model():\n",
    "    # Step 1: Convert existing model to enhanced model\n",
    "    enhanced_model, device, label_columns = convert_to_enhanced_model(\"model/batch_1/best_model.pt\")\n",
    "    \n",
    "    # Step 2: Fine-tune for implicit relations\n",
    "    enhanced_model = finetune_for_implicit_relations(enhanced_model, batch_size=4, learning_rate=1e-5, epochs=3)\n",
    "    \n",
    "    # Step 3: Load thresholds (or use defaults)\n",
    "    try:\n",
    "        with open(\"model/batch_1/best_thresholds.json\", 'r') as f:\n",
    "            thresholds = json.load(f)\n",
    "        print(\"Thresholds loaded successfully!\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: Thresholds file not found. Using default threshold of 0.3.\")\n",
    "        thresholds = [0.3] * len(label_columns)  # Lower thresholds for higher sensitivity\n",
    "    \n",
    "    # Step 4: Save the enhanced model\n",
    "    torch.save(enhanced_model.state_dict(), \"model/enhanced_biobert_model.pt\")\n",
    "    print(\"Enhanced model saved!\")\n",
    "    \n",
    "    # Step 5: Test on implicit examples\n",
    "    implicit_test_examples = [\n",
    "        \"The protein binds to its own regulatory region, creating a negative feedback loop.\",\n",
    "        \"The enzyme can activate other copies of itself, creating a cascade effect.\",\n",
    "        \"Upon binding ligand, the receptor undergoes a conformational change that enables phosphorylation of its cytoplasmic domain.\",\n",
    "        \"The kinase domain transfers phosphate groups to residues within the same protein structure.\",\n",
    "        \"The transcription factor controls expression of its own gene, maintaining homeostasis.\"\n",
    "    ]\n",
    "    \n",
    "    # Test and visualize\n",
    "    results = predict_with_enhanced_model(enhanced_model, implicit_test_examples, label_columns, thresholds)\n",
    "    \n",
    "    # Display results\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"Example {i+1}: \\\"{result['text']}\\\"\")\n",
    "        \n",
    "        if result['predictions']:\n",
    "            print(\"  Predicted autoregulatory classes:\")\n",
    "            for label, prob in sorted(result['predictions'].items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"    - {label}: {prob:.4f}\")\n",
    "        else:\n",
    "            print(\"  No autoregulatory classes detected\")\n",
    "        \n",
    "        print(\"  Top 3 probabilities:\")\n",
    "        for label, prob in sorted(result['top_3'].items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"    - {label}: {prob:.4f}\")\n",
    "        \n",
    "        print(\"  Top attended tokens (relation clues):\")\n",
    "        for token, weight in result['attended_tokens']:\n",
    "            print(f\"    - {token}: {weight:.4f}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Visualize attention for a sample\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "    visualize_attention(enhanced_model, implicit_test_examples[0], tokenizer)\n",
    "    \n",
    "    return enhanced_model, label_columns, thresholds\n",
    "\n",
    "# Run the enhancement and testing\n",
    "import json\n",
    "import numpy as np\n",
    "enhanced_model, label_columns, thresholds = enhance_and_test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a8e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EnhancedBioBERTInference:\n",
    "    def __init__(self, model_path, thresholds_path, label_columns_path):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Load label columns\n",
    "        with open(label_columns_path, 'r') as f:\n",
    "            self.label_columns = json.load(f)\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = EnhancedBioBERTClassifier(n_classes=len(self.label_columns))\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Load thresholds\n",
    "        with open(thresholds_path, 'r') as f:\n",
    "            self.thresholds = json.load(f)\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        return clean_text(text)\n",
    "    \n",
    "    def predict(self, text):\n",
    "        processed_text = self.preprocess(text)\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            processed_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].to(self.device)\n",
    "        attention_mask = encoding['attention_mask'].to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits, attention_weights = self.model(input_ids, attention_mask)\n",
    "            probabilities = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "        \n",
    "        # Apply thresholds and get predicted labels\n",
    "        predictions = {}\n",
    "        for i, label in enumerate(self.label_columns):\n",
    "            if probabilities[i] >= self.thresholds[i]:\n",
    "                predictions[label] = float(probabilities[i])\n",
    "        \n",
    "        # Extract attention for relation understanding\n",
    "        attention = attention_weights.squeeze().cpu().numpy()\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "        attended_tokens = []\n",
    "        for i in range(1, min(len(tokens)-1, len(attention))):\n",
    "            if attention[i] > 0.05:  # Threshold for significant attention\n",
    "                attended_tokens.append((tokens[i], float(attention[i])))\n",
    "        \n",
    "        attended_tokens.sort(key=lambda x: x[1], reverse=True)\n",
    "        attended_tokens = attended_tokens[:5]  # Top 5 attended tokens\n",
    "        \n",
    "        return {\n",
    "            'predictions': predictions,\n",
    "            'top_3': {self.label_columns[i]: float(probabilities[i]) \n",
    "                     for i in np.argsort(probabilities)[::-1][:3]},\n",
    "            'attended_tokens': attended_tokens\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f99f429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Comparison on Test Examples ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n9/9tsj9gw15ll9ycvpvm3m3pq80000gn/T/ipykernel_6298/857903350.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model loaded successfully!\n",
      "Enhanced model loaded successfully!\n",
      "\n",
      "Example 1: \"The receptor undergoes autophosphorylation upon ligand binding.\"\n",
      "  ORIGINAL MODEL:\n",
      "    Predicted classes:\n",
      "      - autophosphorylation: 0.9238\n",
      "  ENHANCED MODEL:\n",
      "    Predicted classes:\n",
      "      - non-autoregulatory: 0.4896\n",
      "      - autoinhibitory: 0.4719\n",
      "      - autoinduction: 0.4355\n",
      "      - autophosphorylation: 0.4261\n",
      "      - autoregulation: 0.4142\n",
      "      - autoinducer: 0.4129\n",
      "      - autofeedback: 0.3742\n",
      "      - autokinase: 0.3393\n",
      "      - autoregulatory: 0.3146\n",
      "    Relation cues (attended tokens):\n",
      "      - ##igan: 0.9826\n",
      "  COMPARISON:\n",
      "    Enhanced model detected 8 additional classes: autoinhibitory, autoregulatory, autoregulation, autoinduction, autokinase, non-autoregulatory, autofeedback, autoinducer\n",
      "    autophosphorylation: Original 0.9238 vs Enhanced 0.4261 (Diff: -0.4977)\n",
      "\n",
      "Example 2: \"The transcription factor binds to its own promoter, creating a feedback loop.\"\n",
      "  ORIGINAL MODEL:\n",
      "    Predicted classes:\n",
      "      - autoregulation: 0.3559\n",
      "      - non-autoregulatory: 0.3007\n",
      "  ENHANCED MODEL:\n",
      "    Predicted classes:\n",
      "      - autoinhibition: 0.5298\n",
      "      - autoinduction: 0.4982\n",
      "      - non-autoregulatory: 0.4797\n",
      "      - autofeedback: 0.4323\n",
      "      - autoregulation: 0.4011\n",
      "      - autophosphorylation: 0.3546\n",
      "      - autoactivation: 0.3440\n",
      "      - autokinase: 0.3271\n",
      "    Relation cues (attended tokens):\n",
      "      - transcription: 0.8650\n",
      "      - factor: 0.1305\n",
      "  COMPARISON:\n",
      "    Enhanced model detected 6 additional classes: autoinhibition, autoactivation, autophosphorylation, autoinduction, autokinase, autofeedback\n",
      "    non-autoregulatory: Original 0.3007 vs Enhanced 0.4797 (Diff: +0.1790)\n",
      "    autoregulation: Original 0.3559 vs Enhanced 0.4011 (Diff: +0.0453)\n",
      "\n",
      "Example 3: \"The kinase transfers phosphate groups to residues within the same protein.\"\n",
      "  ORIGINAL MODEL:\n",
      "    Predicted classes:\n",
      "      - autophosphorylation: 0.5746\n",
      "  ENHANCED MODEL:\n",
      "    Predicted classes:\n",
      "      - autocatalysis: 0.5373\n",
      "      - autoinduction: 0.4967\n",
      "      - autoinhibitory: 0.4793\n",
      "      - autophosphorylation: 0.4524\n",
      "      - autoregulation: 0.3939\n",
      "      - non-autoregulatory: 0.3886\n",
      "      - autoinducer: 0.3561\n",
      "      - autoactivation: 0.3177\n",
      "      - autofeedback: 0.3117\n",
      "    Relation cues (attended tokens):\n",
      "      - protein: 0.2876\n",
      "  COMPARISON:\n",
      "    Enhanced model detected 8 additional classes: autoactivation, autoinhibitory, autoregulation, autoinduction, non-autoregulatory, autofeedback, autoinducer, autocatalysis\n",
      "    autophosphorylation: Original 0.5746 vs Enhanced 0.4524 (Diff: -0.1223)\n",
      "\n",
      "Example 4: \"The enzyme can activate other copies of itself, creating a cascade effect.\"\n",
      "  ORIGINAL MODEL:\n",
      "    Predicted classes:\n",
      "      - autophosphorylation: 0.4547\n",
      "  ENHANCED MODEL:\n",
      "    Predicted classes:\n",
      "      - autoinhibition: 0.5125\n",
      "      - autoinhibitory: 0.4898\n",
      "      - autoinduction: 0.4405\n",
      "      - non-autoregulatory: 0.4195\n",
      "      - autofeedback: 0.3917\n",
      "      - autoregulation: 0.3481\n",
      "      - autoinducer: 0.3150\n",
      "      - autophosphorylation: 0.3063\n",
      "    Relation cues (attended tokens):\n",
      "      - effect: 0.7093\n",
      "  COMPARISON:\n",
      "    Enhanced model detected 7 additional classes: autoinhibition, autoinhibitory, autoregulation, autoinduction, non-autoregulatory, autofeedback, autoinducer\n",
      "    autophosphorylation: Original 0.4547 vs Enhanced 0.3063 (Diff: -0.1484)\n",
      "\n",
      "Example 5: \"Binding of the ligand causes a conformational change that leads to activation of the protein's catalytic domain.\"\n",
      "  ORIGINAL MODEL:\n",
      "    No classes above threshold\n",
      "  ENHANCED MODEL:\n",
      "    Predicted classes:\n",
      "      - autoinhibitory: 0.4589\n",
      "      - autoinduction: 0.4219\n",
      "      - non-autoregulatory: 0.4207\n",
      "      - autophosphorylation: 0.4190\n",
      "      - autoinducer: 0.3655\n",
      "      - autoregulatory: 0.3600\n",
      "      - autofeedback: 0.3497\n",
      "    Relation cues (attended tokens):\n",
      "      - ##igan: 0.9831\n",
      "  COMPARISON:\n",
      "    Enhanced model detected 7 additional classes: autoinduction, autophosphorylation, non-autoregulatory, autoinhibitory, autoregulatory, autofeedback, autoinducer\n",
      "\n",
      "Example 6: \"The dimeric protein complex enables cross-phosphorylation between the two identical subunits.\"\n",
      "  ORIGINAL MODEL:\n",
      "    Predicted classes:\n",
      "      - autophosphorylation: 0.3387\n",
      "  ENHANCED MODEL:\n",
      "    Predicted classes:\n",
      "      - autoinduction: 0.5603\n",
      "      - non-autoregulatory: 0.4777\n",
      "      - autophosphorylation: 0.4570\n",
      "      - autofeedback: 0.4292\n",
      "      - autoinhibitory: 0.4256\n",
      "      - autoinducer: 0.3518\n",
      "    Relation cues (attended tokens):\n",
      "      - -: 0.6081\n",
      "      - ##eric: 0.3808\n",
      "  COMPARISON:\n",
      "    Enhanced model detected 5 additional classes: autoinhibitory, autoinduction, non-autoregulatory, autofeedback, autoinducer\n",
      "    autophosphorylation: Original 0.3387 vs Enhanced 0.4570 (Diff: +0.1183)\n",
      "\n",
      "=== Summary ===\n",
      "The enhanced model should show better performance on examples that describe\n",
      "autoregulatory mechanisms without using explicit 'auto' terminology.\n",
      "It should also provide explanatory information through its attention mechanism.\n"
     ]
    }
   ],
   "source": [
    "# Cell: Compare Original and Enhanced Models\n",
    "def compare_models(test_examples):\n",
    "    \"\"\"\n",
    "    Compare predictions from original and enhanced models on the same examples\n",
    "    \"\"\"\n",
    "    print(\"=== Model Comparison on Test Examples ===\\n\")\n",
    "    \n",
    "    # Load original model\n",
    "    try:\n",
    "        original_inference = BioBERTInference(\n",
    "            model_path=\"model/batch_1/best_model.pt\",\n",
    "            thresholds_path=\"model/batch_1/best_thresholds.json\",\n",
    "            label_columns_path=\"label_columns.json\"\n",
    "        )\n",
    "        print(\"Original model loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading original model: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Load enhanced model\n",
    "    try:\n",
    "        enhanced_inference = EnhancedBioBERTInference(\n",
    "            model_path=\"model/enhanced_biobert_model.pt\",\n",
    "            thresholds_path=\"model/batch_1/best_thresholds.json\",  # Use same thresholds for fair comparison\n",
    "            label_columns_path=\"label_columns.json\"\n",
    "        )\n",
    "        print(\"Enhanced model loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading enhanced model: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Compare predictions\n",
    "    for i, text in enumerate(test_examples):\n",
    "        print(f\"\\nExample {i+1}: \\\"{text}\\\"\")\n",
    "        \n",
    "        # Original model prediction\n",
    "        original_preds = original_inference.predict(text)\n",
    "        print(\"  ORIGINAL MODEL:\")\n",
    "        if original_preds:\n",
    "            print(\"    Predicted classes:\")\n",
    "            for label, prob in sorted(original_preds.items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"      - {label}: {prob:.4f}\")\n",
    "        else:\n",
    "            print(\"    No classes above threshold\")\n",
    "        \n",
    "        # Enhanced model prediction\n",
    "        enhanced_preds = enhanced_inference.predict(text)\n",
    "        print(\"  ENHANCED MODEL:\")\n",
    "        if enhanced_preds['predictions']:\n",
    "            print(\"    Predicted classes:\")\n",
    "            for label, prob in sorted(enhanced_preds['predictions'].items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"      - {label}: {prob:.4f}\")\n",
    "            \n",
    "            print(\"    Relation cues (attended tokens):\")\n",
    "            for token, weight in enhanced_preds['attended_tokens']:\n",
    "                print(f\"      - {token}: {weight:.4f}\")\n",
    "        else:\n",
    "            print(\"    No classes above threshold\")\n",
    "        \n",
    "        print(\"  COMPARISON:\")\n",
    "        # Check if enhanced model found classes the original didn't\n",
    "        original_classes = set(original_preds.keys())\n",
    "        enhanced_classes = set(enhanced_preds['predictions'].keys())\n",
    "        \n",
    "        new_classes = enhanced_classes - original_classes\n",
    "        if new_classes:\n",
    "            print(f\"    Enhanced model detected {len(new_classes)} additional classes: {', '.join(new_classes)}\")\n",
    "        \n",
    "        # Check if confidence improved for common classes\n",
    "        common_classes = original_classes.intersection(enhanced_classes)\n",
    "        for cls in common_classes:\n",
    "            orig_conf = original_preds[cls]\n",
    "            enh_conf = enhanced_preds['predictions'][cls]\n",
    "            diff = enh_conf - orig_conf\n",
    "            print(f\"    {cls}: Original {orig_conf:.4f} vs Enhanced {enh_conf:.4f} (Diff: {diff:+.4f})\")\n",
    "    \n",
    "    print(\"\\n=== Summary ===\")\n",
    "    print(\"The enhanced model should show better performance on examples that describe\")\n",
    "    print(\"autoregulatory mechanisms without using explicit 'auto' terminology.\")\n",
    "    print(\"It should also provide explanatory information through its attention mechanism.\")\n",
    "\n",
    "# Test examples for comparison\n",
    "test_examples = [\n",
    "    # Obvious examples (with \"auto\" terms)\n",
    "    \"The receptor undergoes autophosphorylation upon ligand binding.\",\n",
    "    \n",
    "    # Less obvious examples (without \"auto\" terms)\n",
    "    \"The transcription factor binds to its own promoter, creating a feedback loop.\",\n",
    "    \"The kinase transfers phosphate groups to residues within the same protein.\",\n",
    "    \"The enzyme can activate other copies of itself, creating a cascade effect.\",\n",
    "    \n",
    "    # Challenging examples\n",
    "    \"Binding of the ligand causes a conformational change that leads to activation of the protein's catalytic domain.\",\n",
    "    \"The dimeric protein complex enables cross-phosphorylation between the two identical subunits.\"\n",
    "]\n",
    "\n",
    "# Add this function call after both models are ready\n",
    "compare_models(test_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030cd800",
   "metadata": {},
   "source": [
    "## Model Comparison Summary\n",
    "\n",
    "| Feature | Original BioBERT Model | Enhanced BioBERT Model |\n",
    "|---------|------------------------|------------------------|\n",
    "| **Explicit Detection** | High confidence (0.92+) | Moderate confidence (0.42+) |\n",
    "| **Implicit Detection** | Limited | Improved detection of implicit descriptions |\n",
    "| **Prediction Style** | Focused (1-2 classes) | Broader (5-9 classes) |\n",
    "| **Confidence Level** | Higher confidence | More distributed confidence |\n",
    "| **Explainability** | None | Provides relation attention visualization |\n",
    "| **Class Thresholds** | Higher, more selective | Lower, more inclusive |\n",
    "| **Strengths** | Precision on explicit terms | Detection of implicit relationships |\n",
    "| **Weaknesses** | Misses implicit descriptions | Over-predicts multiple classes |\n",
    "\n",
    "### Example Predictions\n",
    "\n",
    "| Example Text | Original Model Prediction | Enhanced Model Prediction | Key Attention Words |\n",
    "|--------------|---------------------------|---------------------------|---------------------|\n",
    "| \"The receptor undergoes autophosphorylation...\" | autophosphorylation (0.92) | Multiple classes including autophosphorylation (0.43) | \"##igan\" (ligand) |\n",
    "| \"The transcription factor binds to its own promoter...\" | Limited or no detection | Multiple autoregulatory classes | \"transcription\", \"factor\" |\n",
    "| \"The kinase transfers phosphate groups within the same protein...\" | Limited or no detection | Detected as potentially autoregulatory | \"kinase\", \"phosphate\" |\n",
    "\n",
    "### Performance Analysis\n",
    "\n",
    "| Metric | Original Model | Enhanced Model |\n",
    "|--------|----------------|----------------|\n",
    "| Precision | Higher | Lower |\n",
    "| Recall | Lower | Higher |\n",
    "| Classes per prediction | 1-2 | 5-9 |\n",
    "| Explicit term detection | Excellent | Good |\n",
    "| Implicit term detection | Poor | Good |\n",
    "| False positive rate | Moderate | Higher |\n",
    "| Explainability | None | Provides word-level attention |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb33d46b",
   "metadata": {},
   "source": [
    "# Cell 12: Train All Batches \n",
    "\n",
    "# Get unique batch numbers\n",
    "unique_batches = df_cleaned['batch_number'].unique()\n",
    "\n",
    "# Train models for each batch\n",
    "for batch_num in unique_batches:\n",
    "    try:\n",
    "        print(f\"\\n=== Starting training for batch {batch_num} ===\\n\")\n",
    "        model, f1_score = train_model(\n",
    "            batch_number=batch_num, \n",
    "            n_epochs=n_epochs, \n",
    "            learning_rate=learning_rate, \n",
    "            batch_size=batch_size,\n",
    "            use_optimal_thresholds=True\n",
    "        )\n",
    "        print(f\"Batch {batch_num} training complete. Best F1 score: {f1_score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error training batch {batch_num}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b55b31a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoregulatory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
