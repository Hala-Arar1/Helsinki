{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f46fcac3",
   "metadata": {},
   "source": [
    "# Bio Bert Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19186769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import os, random, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "617f1134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7a706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Device setup\n",
    "device = (\n",
    "    torch.device(\"mps\")\n",
    "    if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cuda\")\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485aa656",
   "metadata": {},
   "source": [
    "## 1. Read in Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ef715f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53130, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AC</th>\n",
       "      <th>PMID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Terms</th>\n",
       "      <th>Text_combined</th>\n",
       "      <th>batch_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q8NML3</td>\n",
       "      <td>17183211</td>\n",
       "      <td>RamA, the transcriptional regulator of acetate...</td>\n",
       "      <td>The RamA protein represents a LuxR-type transc...</td>\n",
       "      <td>autoregulation</td>\n",
       "      <td>RamA, the transcriptional regulator of acetate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q9SCZ4</td>\n",
       "      <td>17673660</td>\n",
       "      <td>The FERONIA receptor-like kinase mediates male...</td>\n",
       "      <td>In flowering plants, signaling between the mal...</td>\n",
       "      <td>autophosphorylation</td>\n",
       "      <td>The FERONIA receptor-like kinase mediates male...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q81WX1</td>\n",
       "      <td>12721629</td>\n",
       "      <td>The genome sequence of Bacillus anthracis Ames...</td>\n",
       "      <td>Bacillus anthracis is an endospore-forming bac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The genome sequence of Bacillus anthracis Ames...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P14410</td>\n",
       "      <td>8521865</td>\n",
       "      <td>Phosphorylation of the N-terminal intracellula...</td>\n",
       "      <td>This paper reports the phosphorylation of the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Phosphorylation of the N-terminal intracellula...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P36898</td>\n",
       "      <td>14523231</td>\n",
       "      <td>Mutations in bone morphogenetic protein recept...</td>\n",
       "      <td>Brachydactyly (BD) type A2 is an autosomal dom...</td>\n",
       "      <td>autophosphorylation</td>\n",
       "      <td>Mutations in bone morphogenetic protein recept...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AC      PMID                                              Title  \\\n",
       "0  Q8NML3  17183211  RamA, the transcriptional regulator of acetate...   \n",
       "1  Q9SCZ4  17673660  The FERONIA receptor-like kinase mediates male...   \n",
       "2  Q81WX1  12721629  The genome sequence of Bacillus anthracis Ames...   \n",
       "3  P14410   8521865  Phosphorylation of the N-terminal intracellula...   \n",
       "4  P36898  14523231  Mutations in bone morphogenetic protein recept...   \n",
       "\n",
       "                                            Abstract                Terms  \\\n",
       "0  The RamA protein represents a LuxR-type transc...       autoregulation   \n",
       "1  In flowering plants, signaling between the mal...  autophosphorylation   \n",
       "2  Bacillus anthracis is an endospore-forming bac...                  NaN   \n",
       "3  This paper reports the phosphorylation of the ...                  NaN   \n",
       "4  Brachydactyly (BD) type A2 is an autosomal dom...  autophosphorylation   \n",
       "\n",
       "                                       Text_combined  batch_number  \n",
       "0  RamA, the transcriptional regulator of acetate...             1  \n",
       "1  The FERONIA receptor-like kinase mediates male...             1  \n",
       "2  The genome sequence of Bacillus anthracis Ames...             1  \n",
       "3  Phosphorylation of the N-terminal intracellula...             1  \n",
       "4  Mutations in bone morphogenetic protein recept...             1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../data/processed/shuffled_10_data.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea4f48e",
   "metadata": {},
   "source": [
    "## 2. Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c11d8441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Clean text\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    # Handle NaN values\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    # Keep hyphens as they may be important in biomedical terms\n",
    "    text = re.sub(r'[^\\w\\s-]', '', text)\n",
    "    # Keep numbers that might be part of important terms\n",
    "    # text = re.sub(r'\\d+', '', text)\n",
    "    text = \" \".join([word.strip() for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "df['Text_Cleaned'] = df['Text_combined'].apply(clean_text)\n",
    "\n",
    "# Fill nan with 'non-autoregulatory'\n",
    "df['Terms'] = df['Terms'].fillna('non-autoregulatory')\n",
    "\n",
    "# Keep only selected columns\n",
    "columns_to_keep = ['batch_number', 'Text_Cleaned', 'Terms']\n",
    "df_cleaned = df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ac85503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53130, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_number</th>\n",
       "      <th>Text_Cleaned</th>\n",
       "      <th>Terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>rama transcriptional regulator acetate metabol...</td>\n",
       "      <td>autoregulation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>feronia receptor-like kinase mediates male-fem...</td>\n",
       "      <td>autophosphorylation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>genome sequence bacillus anthracis ames compar...</td>\n",
       "      <td>non-autoregulatory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>phosphorylation n-terminal intracellular tail ...</td>\n",
       "      <td>non-autoregulatory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mutations bone morphogenetic protein receptor ...</td>\n",
       "      <td>autophosphorylation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_number                                       Text_Cleaned  \\\n",
       "0             1  rama transcriptional regulator acetate metabol...   \n",
       "1             1  feronia receptor-like kinase mediates male-fem...   \n",
       "2             1  genome sequence bacillus anthracis ames compar...   \n",
       "3             1  phosphorylation n-terminal intracellular tail ...   \n",
       "4             1  mutations bone morphogenetic protein receptor ...   \n",
       "\n",
       "                 Terms  \n",
       "0       autoregulation  \n",
       "1  autophosphorylation  \n",
       "2   non-autoregulatory  \n",
       "3   non-autoregulatory  \n",
       "4  autophosphorylation  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_cleaned.shape)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0e3083",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7e07168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n9/9tsj9gw15ll9ycvpvm3m3pq80000gn/T/ipykernel_76761/649158024.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['Terms_List'] = df_cleaned['Terms'].apply(\n",
      "/var/folders/n9/9tsj9gw15ll9ycvpvm3m3pq80000gn/T/ipykernel_76761/649158024.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['Terms_List'] = df_cleaned['Terms_List'].apply(lambda x: list(set(x)))\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Prepare labels\n",
    "# Convert terms to list\n",
    "df_cleaned['Terms_List'] = df_cleaned['Terms'].apply(\n",
    "    lambda x: [term.strip() for term in x.split(',')]\n",
    ")\n",
    "df_cleaned['Terms_List'] = df_cleaned['Terms_List'].apply(lambda x: list(set(x)))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(df_cleaned['Terms_List'])\n",
    "label_columns = mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa499d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_df = pd.DataFrame(labels, columns=label_columns)\n",
    "existing_columns = [col for col in label_columns if col in df_cleaned.columns]\n",
    "df_cleaned = df_cleaned.drop(columns=existing_columns, errors='ignore')\n",
    "df_cleaned = pd.concat([df_cleaned, labels_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de3ced07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53130, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_number</th>\n",
       "      <th>Text_Cleaned</th>\n",
       "      <th>Terms</th>\n",
       "      <th>Terms_List</th>\n",
       "      <th>autoactivation</th>\n",
       "      <th>autocatalysis</th>\n",
       "      <th>autocatalytic</th>\n",
       "      <th>autofeedback</th>\n",
       "      <th>autoinducer</th>\n",
       "      <th>autoinduction</th>\n",
       "      <th>autoinhibition</th>\n",
       "      <th>autoinhibitory</th>\n",
       "      <th>autokinase</th>\n",
       "      <th>autolysis</th>\n",
       "      <th>autophosphorylation</th>\n",
       "      <th>autoregulation</th>\n",
       "      <th>autoregulatory</th>\n",
       "      <th>autoubiquitination</th>\n",
       "      <th>non-autoregulatory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>rama transcriptional regulator acetate metabol...</td>\n",
       "      <td>autoregulation</td>\n",
       "      <td>[autoregulation]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>feronia receptor-like kinase mediates male-fem...</td>\n",
       "      <td>autophosphorylation</td>\n",
       "      <td>[autophosphorylation]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>genome sequence bacillus anthracis ames compar...</td>\n",
       "      <td>non-autoregulatory</td>\n",
       "      <td>[non-autoregulatory]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>phosphorylation n-terminal intracellular tail ...</td>\n",
       "      <td>non-autoregulatory</td>\n",
       "      <td>[non-autoregulatory]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mutations bone morphogenetic protein receptor ...</td>\n",
       "      <td>autophosphorylation</td>\n",
       "      <td>[autophosphorylation]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_number                                       Text_Cleaned  \\\n",
       "0             1  rama transcriptional regulator acetate metabol...   \n",
       "1             1  feronia receptor-like kinase mediates male-fem...   \n",
       "2             1  genome sequence bacillus anthracis ames compar...   \n",
       "3             1  phosphorylation n-terminal intracellular tail ...   \n",
       "4             1  mutations bone morphogenetic protein receptor ...   \n",
       "\n",
       "                 Terms             Terms_List  autoactivation  autocatalysis  \\\n",
       "0       autoregulation       [autoregulation]               0              0   \n",
       "1  autophosphorylation  [autophosphorylation]               0              0   \n",
       "2   non-autoregulatory   [non-autoregulatory]               0              0   \n",
       "3   non-autoregulatory   [non-autoregulatory]               0              0   \n",
       "4  autophosphorylation  [autophosphorylation]               0              0   \n",
       "\n",
       "   autocatalytic  autofeedback  autoinducer  autoinduction  autoinhibition  \\\n",
       "0              0             0            0              0               0   \n",
       "1              0             0            0              0               0   \n",
       "2              0             0            0              0               0   \n",
       "3              0             0            0              0               0   \n",
       "4              0             0            0              0               0   \n",
       "\n",
       "   autoinhibitory  autokinase  autolysis  autophosphorylation  autoregulation  \\\n",
       "0               0           0          0                    0               1   \n",
       "1               0           0          0                    1               0   \n",
       "2               0           0          0                    0               0   \n",
       "3               0           0          0                    0               0   \n",
       "4               0           0          0                    1               0   \n",
       "\n",
       "   autoregulatory  autoubiquitination  non-autoregulatory  \n",
       "0               0                   0                   0  \n",
       "1               0                   0                   0  \n",
       "2               0                   0                   1  \n",
       "3               0                   0                   1  \n",
       "4               0                   0                   0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_cleaned.shape)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc5a6b4",
   "metadata": {},
   "source": [
    "## 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6db7b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Instantiate the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80ebbd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Splitting\n",
    "def split_single_batch_data(batch_number, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data from a single batch into train and test sets using stratified sampling.\n",
    "    \"\"\"\n",
    "    batch_df = df_cleaned[df_cleaned['batch_number'] == batch_number].copy()\n",
    "    \n",
    "    if len(batch_df) == 0:\n",
    "        print(f\"Warning: No data found for batch {batch_number}\")\n",
    "        return None, None, None, None\n",
    "        \n",
    "    X = batch_df['Text_Cleaned']\n",
    "    y = labels_df.loc[batch_df.index].values  # Ensure indexing alignment\n",
    "    \n",
    "    # Calculate label distribution\n",
    "    non_auto_count = len(batch_df[batch_df['Terms'] == 'non-autoregulatory'])\n",
    "    auto_count = len(batch_df[batch_df['Terms'] != 'non-autoregulatory'])\n",
    "    \n",
    "    try:\n",
    "        # Split data\n",
    "        msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "        \n",
    "        for train_idx, test_idx in msss.split(X, y):\n",
    "            X_train = X.iloc[train_idx]\n",
    "            X_test = X.iloc[test_idx]\n",
    "            y_train = y[train_idx]\n",
    "            y_test = y[test_idx]\n",
    "        \n",
    "        print(f\"Batch {batch_number} | Train: {len(X_train)}, Test: {len(X_test)} | Non-auto: {non_auto_count}, Auto: {auto_count}\")\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    except Exception as e:\n",
    "        print(f\"Error in splitting batch {batch_number}: {e}\")\n",
    "        # Fallback to regular train_test_split if stratified splitting fails\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "        print(f\"Fallback split for batch {batch_number} | Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee507758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "def get_data_and_weights(batch_number):\n",
    "    \"\"\"\n",
    "    Get data and calculate class weights for a specific batch.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = split_single_batch_data(batch_number)\n",
    "    \n",
    "    if X_train is None:\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    # Calculate class weights\n",
    "    pos_weights = []\n",
    "    for i in range(y_train.shape[1]):\n",
    "        neg_count = len(y_train) - np.sum(y_train[:, i])\n",
    "        pos_count = np.sum(y_train[:, i])\n",
    "        # Handle the case where a class might have no positive examples\n",
    "        if pos_count == 0:\n",
    "            pos_weights.append(1.0)  # Default weight\n",
    "        else:\n",
    "            # Calculate weight and clip to prevent extreme values\n",
    "            weight = neg_count / pos_count\n",
    "            weight = min(max(weight, 0.1), 10.0)  # Clip between 0.1 and 10\n",
    "            pos_weights.append(weight)\n",
    "    \n",
    "    pos_weights = torch.FloatTensor(pos_weights).to(device)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, pos_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8d8eab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Dataset and DataLoader classes\n",
    "# Create dataset class\n",
    "class BioBERTDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.FloatTensor(label)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05fd9cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader\n",
    "def create_dataset_and_loader(X_train, y_train, X_test, y_test, batch_size):\n",
    "    train_dataset = BioBERTDataset(X_train, y_train, tokenizer)\n",
    "    test_dataset = BioBERTDataset(X_test, y_test, tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e29c9d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Model Architecture\n",
    "# Improved BioBERT Classifier with intermediate layers\n",
    "class ImprovedBioBERTClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, dropout1=0.1, dropout2=0.2):\n",
    "        super(ImprovedBioBERTClassifier, self).__init__()\n",
    "        # Create a fresh copy of the model\n",
    "        self.bert = AutoModel.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "        \n",
    "        # Add an intermediate layer\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.intermediate = nn.Linear(self.bert.config.hidden_size, 512)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        self.classifier = nn.Linear(512, n_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.xavier_normal_(self.intermediate.weight)\n",
    "        nn.init.zeros_(self.intermediate.bias)\n",
    "        nn.init.xavier_normal_(self.classifier.weight)\n",
    "        nn.init.zeros_(self.classifier.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        pooled_output = self.dropout1(pooled_output)\n",
    "        intermediate = self.intermediate(pooled_output)\n",
    "        intermediate = self.activation(intermediate)\n",
    "        intermediate = self.dropout2(intermediate)\n",
    "        logits = self.classifier(intermediate)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "331fc11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Training and Evaluation Functions\n",
    "# Training function with improvements\n",
    "def train_epoch(model, data_loader, optimizer, criterion, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        try:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Check for NaN loss\n",
    "            if torch.isnan(loss):\n",
    "                print(\"WARNING: NaN loss detected, skipping batch\")\n",
    "                continue\n",
    "                \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_batches += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch processing: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Protect against division by zero\n",
    "    if total_batches == 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    return total_loss / total_batches\n",
    "\n",
    "# Optimize thresholds for each label\n",
    "def optimize_thresholds(model, val_loader, n_labels):\n",
    "    model.eval()\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Optimizing thresholds\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels']\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "            \n",
    "            all_outputs.append(probs)\n",
    "            all_labels.append(labels.numpy())\n",
    "    \n",
    "    all_outputs = np.vstack(all_outputs)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "    optimal_thresholds = []\n",
    "    \n",
    "    for i in range(n_labels):\n",
    "        best_f1 = 0\n",
    "        best_threshold = 0.5\n",
    "        \n",
    "        for threshold in np.arange(0.3, 0.7, 0.05):\n",
    "            preds = (all_outputs[:, i] >= threshold).astype(int)\n",
    "            f1 = f1_score(all_labels[:, i], preds, zero_division=0)\n",
    "            \n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "        \n",
    "        optimal_thresholds.append(best_threshold)\n",
    "        \n",
    "    return optimal_thresholds\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, data_loader, criterion, thresholds):\n",
    "    \"\"\"\n",
    "    Evaluate the model with focused metrics output.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probabilities = torch.sigmoid(outputs).cpu().numpy()\n",
    "            \n",
    "            # Apply thresholds per label\n",
    "            predictions = np.array([\n",
    "                (probabilities[:, i] >= thresholds[i]).astype(int) for i in range(len(thresholds))\n",
    "            ]).T\n",
    "\n",
    "            all_predictions.extend(predictions)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Samples metrics\n",
    "    samples_precision = precision_score(all_labels, all_predictions, average='samples', zero_division=0)\n",
    "    samples_recall = recall_score(all_labels, all_predictions, average='samples', zero_division=0)\n",
    "    samples_f1 = f1_score(all_labels, all_predictions, average='samples', zero_division=0)\n",
    "\n",
    "    # F1 metrics\n",
    "    micro_f1 = f1_score(all_labels, all_predictions, average='micro', zero_division=0)\n",
    "    macro_f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "    weighted_f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "\n",
    "    # Average loss\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    metrics = {\n",
    "        'loss': avg_loss,\n",
    "        'micro_f1': micro_f1,\n",
    "        'macro_f1': macro_f1,\n",
    "        'weighted_f1': weighted_f1,\n",
    "        'samples_f1': samples_f1,\n",
    "        'samples_precision': samples_precision,\n",
    "        'samples_recall': samples_recall\n",
    "    }\n",
    "    \n",
    "    # Output results\n",
    "    print(f\"Loss: {avg_loss:.4f} | Micro F1: {micro_f1:.4f} | Macro F1: {macro_f1:.4f} | Weighted F1: {weighted_f1:.4f} | Samples F1: {samples_f1:.4f}\")\n",
    "    print(f\"Samples Precision: {samples_precision:.4f} | Samples Recall: {samples_recall:.4f}\")\n",
    "    \n",
    "    # Return metrics for further analysis\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a479e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Main Training Function\n",
    "def train_model(batch_number, n_epochs, learning_rate, batch_size, use_optimal_thresholds=True):\n",
    "    \"\"\"\n",
    "    Training loop for a single batch with dynamic threshold settings.\n",
    "    \"\"\"\n",
    "    # Get data\n",
    "    data_result = get_data_and_weights(batch_number)\n",
    "    \n",
    "    if data_result[0] is None:\n",
    "        print(f\"Skipping batch {batch_number} due to data issues.\")\n",
    "        return\n",
    "        \n",
    "    X_train, X_test, y_train, y_test, pos_weights = data_result\n",
    "    print(f\"\\nProcessing Batch {batch_number} ...\")\n",
    "    \n",
    "    # Set initial thresholds\n",
    "    initial_thresholds = [0.5] * y_train.shape[1]\n",
    "    print(f\"\\nInitial Thresholds: {[f'{t:.2f}' for t in initial_thresholds]}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader, test_loader = create_dataset_and_loader(X_train, y_train, X_test, y_test, batch_size)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ImprovedBioBERTClassifier(n_classes=y_train.shape[1]).to(device)\n",
    "    \n",
    "    # Set up loss and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    # Set up scheduler with warmup\n",
    "    total_steps = len(train_loader) * n_epochs\n",
    "    warmup_steps = int(total_steps * 0.1)  # 10% warmup\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_samples_f1 = 0.0\n",
    "    \n",
    "    # Dictionary to track all metrics\n",
    "    all_metrics = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'micro_f1': [],\n",
    "        'macro_f1': [],\n",
    "        'weighted_f1': [],\n",
    "        'samples_f1': [],\n",
    "        'samples_precision': [],\n",
    "        'samples_recall': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs} | Batch {batch_number}\")\n",
    "        \n",
    "        # Train\n",
    "        epoch_loss = train_epoch(model, train_loader, optimizer, criterion, scheduler)\n",
    "        print(f\"Training Loss: {epoch_loss:.4f}\")\n",
    "        all_metrics['train_loss'].append(epoch_loss)\n",
    "        \n",
    "        # Optimize thresholds if requested (only after first epoch)\n",
    "        thresholds = initial_thresholds\n",
    "        if use_optimal_thresholds and epoch >= 1:\n",
    "            try:\n",
    "                print(\"Optimizing thresholds...\")\n",
    "                thresholds = optimize_thresholds(model, test_loader, len(initial_thresholds))\n",
    "                print(f\"Optimized Thresholds: {[f'{t:.2f}' for t in thresholds]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error optimizing thresholds: {e}\")\n",
    "                print(\"Using default thresholds\")\n",
    "                thresholds = initial_thresholds\n",
    "        \n",
    "        # Evaluate\n",
    "        metrics = evaluate(model, test_loader, criterion, thresholds)\n",
    "        \n",
    "        # Store metrics\n",
    "        for k, v in metrics.items():\n",
    "            if k in all_metrics:\n",
    "                all_metrics[k].append(v)\n",
    "        \n",
    "        current_samples_f1 = metrics['samples_f1']\n",
    "        \n",
    "        if current_samples_f1 > best_samples_f1:\n",
    "            best_samples_f1 = current_samples_f1\n",
    "            print(f\"New best F1 score: {best_samples_f1:.4f}\")\n",
    "    \n",
    "    return model, best_samples_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7418e7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Inference Class\n",
    "class BioBERTInference:\n",
    "    def __init__(self, model_path, thresholds_path, label_columns_path):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Load label columns\n",
    "        with open(label_columns_path, 'r') as f:\n",
    "            self.label_columns = json.load(f)\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = ImprovedBioBERTClassifier(n_classes=len(self.label_columns))\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Load thresholds\n",
    "        with open(thresholds_path, 'r') as f:\n",
    "            self.thresholds = json.load(f)\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        return clean_text(text)\n",
    "    \n",
    "    def predict(self, text):\n",
    "        processed_text = self.preprocess(text)\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            processed_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].to(self.device)\n",
    "        attention_mask = encoding['attention_mask'].to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, attention_mask)\n",
    "            probabilities = torch.sigmoid(outputs).cpu().numpy()[0]\n",
    "        \n",
    "        # Apply thresholds and get predicted labels\n",
    "        predictions = {}\n",
    "        for i, label in enumerate(self.label_columns):\n",
    "            if probabilities[i] >= self.thresholds[i]:\n",
    "                predictions[label] = float(probabilities[i])\n",
    "        \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60c4c1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 | Train: 4248, Test: 1065 | Non-auto: 3542, Auto: 1771\n",
      "\n",
      "Processing Batch 1 ...\n",
      "\n",
      "Initial Thresholds: ['0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50']\n",
      "Epoch 1/7 | Batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 531/531 [15:32<00:00,  1.76s/it, loss=0.2189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 134/134 [01:26<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2680 | Micro F1: 0.7490 | Macro F1: 0.2659 | Weighted F1: 0.7411 | Samples F1: 0.7641\n",
      "Samples Precision: 0.7523 | Samples Recall: 0.7930\n",
      "  New best model saved to model/batch_1/best_model.pt\n",
      "  Thresholds saved to model/batch_1/best_thresholds.json\n",
      "Epoch 2/7 | Batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 531/531 [16:47<00:00,  1.90s/it, loss=0.0878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1764\n",
      "Optimizing thresholds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing thresholds: 100%|██████████| 134/134 [01:22<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Thresholds: ['0.50', '0.50', '0.50', '0.30', '0.60', '0.50', '0.65', '0.65', '0.50', '0.65', '0.55', '0.40', '0.65', '0.55', '0.40']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 134/134 [01:23<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1008 | Micro F1: 0.9031 | Macro F1: 0.5927 | Weighted F1: 0.9058 | Samples F1: 0.9139\n",
      "Samples Precision: 0.9011 | Samples Recall: 0.9399\n",
      "  New best model saved to model/batch_1/best_model.pt\n",
      "  Thresholds saved to model/batch_1/best_thresholds.json\n",
      "Epoch 3/7 | Batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 531/531 [16:11<00:00,  1.83s/it, loss=0.0207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1036\n",
      "Optimizing thresholds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing thresholds: 100%|██████████| 134/134 [01:34<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Thresholds: ['0.30', '0.50', '0.35', '0.30', '0.30', '0.30', '0.60', '0.40', '0.50', '0.60', '0.60', '0.55', '0.35', '0.45', '0.65']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 134/134 [01:20<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0848 | Micro F1: 0.9315 | Macro F1: 0.7845 | Weighted F1: 0.9311 | Samples F1: 0.9322\n",
      "Samples Precision: 0.9291 | Samples Recall: 0.9394\n",
      "  New best model saved to model/batch_1/best_model.pt\n",
      "  Thresholds saved to model/batch_1/best_thresholds.json\n",
      "Epoch 4/7 | Batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 531/531 [15:59<00:00,  1.81s/it, loss=0.1242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0790\n",
      "Optimizing thresholds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing thresholds: 100%|██████████| 134/134 [01:20<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Thresholds: ['0.35', '0.50', '0.40', '0.30', '0.30', '0.30', '0.40', '0.35', '0.30', '0.60', '0.60', '0.55', '0.60', '0.60', '0.30']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 134/134 [01:23<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0750 | Micro F1: 0.9367 | Macro F1: 0.8414 | Weighted F1: 0.9368 | Samples F1: 0.9394\n",
      "Samples Precision: 0.9365 | Samples Recall: 0.9465\n",
      "  New best model saved to model/batch_1/best_model.pt\n",
      "  Thresholds saved to model/batch_1/best_thresholds.json\n",
      "Epoch 5/7 | Batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 531/531 [16:58<00:00,  1.92s/it, loss=0.0606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0655\n",
      "Optimizing thresholds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing thresholds: 100%|██████████| 134/134 [01:22<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Thresholds: ['0.30', '0.50', '0.35', '0.30', '0.30', '0.30', '0.65', '0.35', '0.30', '0.50', '0.60', '0.45', '0.30', '0.50', '0.30']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 134/134 [01:21<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0792 | Micro F1: 0.9437 | Macro F1: 0.8703 | Weighted F1: 0.9430 | Samples F1: 0.9471\n",
      "Samples Precision: 0.9441 | Samples Recall: 0.9549\n",
      "  New best model saved to model/batch_1/best_model.pt\n",
      "  Thresholds saved to model/batch_1/best_thresholds.json\n",
      "Epoch 6/7 | Batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 531/531 [17:09<00:00,  1.94s/it, loss=0.6476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0614\n",
      "Optimizing thresholds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing thresholds: 100%|██████████| 134/134 [01:22<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Thresholds: ['0.30', '0.50', '0.55', '0.30', '0.30', '0.30', '0.60', '0.40', '0.30', '0.40', '0.65', '0.30', '0.30', '0.30', '0.30']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 134/134 [01:23<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0723 | Micro F1: 0.9404 | Macro F1: 0.8678 | Weighted F1: 0.9400 | Samples F1: 0.9430\n",
      "Samples Precision: 0.9404 | Samples Recall: 0.9502\n",
      "Epoch 7/7 | Batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 531/531 [16:30<00:00,  1.87s/it, loss=0.0091]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0544\n",
      "Optimizing thresholds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing thresholds: 100%|██████████| 134/134 [01:22<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Thresholds: ['0.30', '0.50', '0.50', '0.30', '0.30', '0.30', '0.60', '0.30', '0.30', '0.45', '0.55', '0.50', '0.30', '0.35', '0.30']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 134/134 [01:21<00:00,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0698 | Micro F1: 0.9398 | Macro F1: 0.8687 | Weighted F1: 0.9397 | Samples F1: 0.9415\n",
      "Samples Precision: 0.9394 | Samples Recall: 0.9474\n",
      "Metrics saved to results/metrics_batch_1.json\n",
      "Batch 1 training complete. Best F1 score: 0.9471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Run Training\n",
    "# Change these parameters as needed\n",
    "n_epochs = 7\n",
    "learning_rate = 5e-6\n",
    "batch_size = 8\n",
    "\n",
    "# To train a specific batch\n",
    "batch_num = 1  # Change this to the batch you want to train\n",
    "model, f1_score = train_model(\n",
    "    batch_number=batch_num,\n",
    "    n_epochs=n_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,\n",
    "    use_optimal_thresholds=True\n",
    ")\n",
    "print(f\"Batch {batch_num} training complete. Best F1 score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb33d46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting training for batch 1 ===\n",
      "\n",
      "Batch 1 | Train: 4248, Test: 1065 | Non-auto: 3542, Auto: 1771\n",
      "\n",
      "Processing Batch 1 ...\n",
      "\n",
      "Initial Thresholds: ['0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50']\n",
      "Epoch 1/7 | Batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 14/531 [00:28<17:38,  2.05s/it, loss=0.7747]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Starting training for batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     model, f1_score \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_optimal_thresholds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m training complete. Best F1 score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[18], line 59\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(batch_number, n_epochs, learning_rate, batch_size, use_optimal_thresholds)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m all_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(epoch_loss)\n",
      "Cell \u001b[0;32mIn[17], line 31\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, optimizer, criterion, scheduler)\u001b[0m\n\u001b[1;32m     28\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Clip gradients\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/autoregulatory/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:30\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/autoregulatory/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:122\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m    120\u001b[0m         clip_coef_clamped_device \u001b[38;5;241m=\u001b[39m clip_coef_clamped\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m device_grads:\n\u001b[0;32m--> 122\u001b[0m             \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_coef_clamped_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell 12: Train All Batches (Optional)\n",
    "\n",
    "# Get unique batch numbers\n",
    "unique_batches = df_cleaned['batch_number'].unique()\n",
    "\n",
    "# Train models for each batch\n",
    "for batch_num in unique_batches:\n",
    "    try:\n",
    "        print(f\"\\n=== Starting training for batch {batch_num} ===\\n\")\n",
    "        model, f1_score = train_model(\n",
    "            batch_number=batch_num, \n",
    "            n_epochs=n_epochs, \n",
    "            learning_rate=learning_rate, \n",
    "            batch_size=batch_size,\n",
    "            use_optimal_thresholds=True\n",
    "        )\n",
    "        print(f\"Batch {batch_num} training complete. Best F1 score: {f1_score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error training batch {batch_num}: {e}\")\n",
    "        continue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoregulatory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
